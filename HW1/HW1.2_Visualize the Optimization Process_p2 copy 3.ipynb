{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import matplotlib\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x293ffe558f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 60000 \n",
      "test_dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset),\"\\ntest_dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader func\n",
    "def train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def test_loader(batch_size):\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.fc2 = nn.Linear(500, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten as one dimension\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = DNN()\n",
    "# p1=torch.nn.utils.parameters_to_vector(m.fc1.weight.data).detach().numpy()\n",
    "\n",
    "# print(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testDict = {}\n",
    "\n",
    "# for name, parameter in m.named_parameters():\n",
    "#     if'weight' in name:\n",
    "#         testDict['b'] = (torch.nn.utils.parameters_to_vector(parameter).detach().numpy())\n",
    "#         #testDict['b'].append(weights2)\n",
    "# len(testDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = pd.DataFrame()\n",
    "# for name, parameter in m.named_parameters():\n",
    "#             if 'weight' in name:\n",
    "#                 weights = torch.nn.utils.parameters_to_vector(parameter).detach().numpy() \n",
    "#                 temp_df = pd.concat([temp_df, pd.DataFrame(weights).T], axis = 1)\n",
    "                \n",
    "# temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = pd.DataFrame()\n",
    "# for name, parameter in m.named_parameters():\n",
    "#             if 'weight' in name and 'fc3'in name:\n",
    "#                 weights = torch.nn.utils.parameters_to_vector(parameter).detach().numpy() \n",
    "#                 temp_df = pd.concat([temp_df, pd.DataFrame(weights).T], axis = 1)\n",
    "                \n",
    "# temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(p1 == p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def trainFunc(model,num_epochs,train_batch_size,status_interval):\n",
    "    model.train()\n",
    "    print('strated')\n",
    "    train_load = train_loader(train_batch_size)\n",
    "    n_total_steps = len(train_load)\n",
    "    train_losses = []\n",
    "    train_epoch = []\n",
    "    train_acc = []\n",
    "    epoch = 0\n",
    "    modelParamWgt = pd.DataFrame()\n",
    "    trainAvgLossArr = []\n",
    "    trainAvgAccArr = []\n",
    "    firstParaWgt = {}\n",
    "\n",
    "    for epoch in range (num_epochs):\n",
    "        epoch += 1\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        lossSum =0\n",
    "        totalacc=0\n",
    "        epoch_df = pd.DataFrame()\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_load):  \n",
    "            #if (i+1)% 60 == 0 : print(i+1)\n",
    "            images, labels = Variable(images),Variable(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            prediction = model(images)\n",
    "\n",
    "            images.requires_grad = True\n",
    "\n",
    "            loss = loss_func(prediction, labels)\n",
    "            lossSum += loss.detach().numpy()\n",
    "\n",
    "            # Backward and optimize\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "            totalacc += acc\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_acc.append(acc)\n",
    "            train_epoch.append(epoch)\n",
    "\n",
    "            #Print Status\n",
    "            if (i+1) % status_interval == 0:\n",
    "                print (f'Train O/P: Epoch [{epoch}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}',end= '\\r',flush = True)\n",
    "        \n",
    "        #Weight Collection\n",
    "        for name, parameter in model.named_parameters():\n",
    "            #print(name)\n",
    "            if'weight' in name:\n",
    "                weights = torch.nn.utils.parameters_to_vector(parameter).detach().numpy() \n",
    "                epoch_df = pd.concat([epoch_df, pd.DataFrame(weights).T], axis = 1) \n",
    "\n",
    "        modelParamWgt = pd.concat([modelParamWgt, epoch_df], axis = 0)     \n",
    "\n",
    "       \n",
    "        epochLoss = lossSum/n_total_steps\n",
    "        epochAcc = totalacc/n_total_steps\n",
    "        #print(\"Train Avg loss:\",trainAvgLoss)\n",
    "        trainAvgLossArr.append(epochLoss)    \n",
    "        trainAvgAccArr.append(epochAcc)\n",
    "                       \n",
    "    return train_epoch,train_losses,train_acc,trainAvgLossArr,trainAvgAccArr, modelParamWgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunction(model,loss_func,test_batch_size): \n",
    "    test_load = test_loader(test_batch_size)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        testLoss = 0\n",
    "        count = 0\n",
    "        for images, labels in test_load:\n",
    "            images, labels = Variable(images),Variable(labels)\n",
    "            \n",
    "            prediction = model(images)\n",
    "            testLoss += loss_func(prediction,labels).item()\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            count += 1\n",
    "    netTest_loss = testLoss/count\n",
    "    netTest_acc1 = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the test images: {netTest_acc1}% & Test Loss: {netTest_loss}',end= '\\r',flush = True)\n",
    "    return netTest_acc1, netTest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaOps(paramDF,itr):\n",
    "    pcaOperation =  PCA(n_components=2)\n",
    "\n",
    "    pcaVal = pcaOperation.fit_transform(paramDF)\n",
    "\n",
    "    itrData = np.full((pcaVal.shape[0],1),itr)\n",
    "\n",
    "    pcaDf = pd.DataFrame(np.append(pcaVal,itrData,axis=1),columns=['x','y','Itr No.'])\n",
    "\n",
    "    return pcaDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters:418060\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "tempModel = DNN()\n",
    "for i in tempModel.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0\n",
      "strated\n",
      "Time: 1y of the network on the test images: 98.17% & Test Loss: 0.06214663684368134\n",
      "strated\n",
      "Time: 2y of the network on the test images: 98.17% & Test Loss: 0.06214663684368134\n",
      "strated\n",
      "Time: 3y of the network on the test images: 98.17% & Test Loss: 0.06214663684368134\n",
      "strated\n",
      "Time: 4y of the network on the test images: 98.17% & Test Loss: 0.06214663684368134\n",
      "strated\n",
      "Time: 5y of the network on the test images: 98.17% & Test Loss: 0.06214663684368134\n",
      "strated\n",
      "Time: 6y of the network on the test images: 98.17% & Test Loss: 0.06214663684368134\n",
      "strated\n",
      "Time: 7y of the network on the test images: 98.17% & Test Loss: 0.06214663684368134\n",
      "strated\n",
      "Accuracy of the network on the test images: 98.17% & Test Loss: 0.06214663684368134\r"
     ]
    }
   ],
   "source": [
    "#2nd Approach\n",
    "\n",
    "max_epochs = 45\n",
    "all_df = pd.DataFrame()\n",
    "columns=[\"x\",\"y\",\"Times\"]\n",
    "trainAllacc={}\n",
    "testAllacc={}\n",
    "trainAllloss={}\n",
    "testAllloss={}\n",
    "train_batch_size = 1000\n",
    "test_batch_size = 1000\n",
    "status_interval = 60\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "firstLayer = pd.DataFrame()\n",
    "\n",
    "for count in range(8):\n",
    "\n",
    "    j = copy.deepcopy(count)\n",
    "\n",
    "    j = DNN()  \n",
    "\n",
    "    print(\"Time: \"+str(count))\n",
    "    optimizer = torch.optim.Adam(j.parameters(),lr = 0.0004,weight_decay=1e-4)\n",
    "    model_name1 = \"Times:\"+str(count)  \n",
    "    \n",
    "    train_epoch,train_losses,train_acc,trainAvgLoss,trainAvgAccArr, modelParamWgt,firstParamWgt = trainFunc(j,max_epochs,train_batch_size,status_interval)\n",
    "    testAcc, testLoss = testFunction(j,loss_func,test_batch_size)\n",
    "\n",
    "    all_df = all_df.append(modelParamWgt)\n",
    "    \n",
    "    testAllacc[count] = testAcc\n",
    "    trainAllloss[count] = trainAvgLoss\n",
    "    testAllloss[count] = testLoss\n",
    "    trainAllacc[count] = trainAvgAccArr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAccArr = []\n",
    "for key,values in enumerate(trainAllacc):\n",
    "    trainAccArr.append(trainAllacc[key])\n",
    "trainLossArr = []\n",
    "for key,values in enumerate(trainAllloss):\n",
    "    trainLossArr.append(trainAllloss[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.108452e-03</td>\n",
       "      <td>3.076525e-05</td>\n",
       "      <td>-2.651312e-04</td>\n",
       "      <td>3.421936e-04</td>\n",
       "      <td>-1.268616e-02</td>\n",
       "      <td>2.904374e-03</td>\n",
       "      <td>-2.349899e-04</td>\n",
       "      <td>9.891548e-04</td>\n",
       "      <td>-2.102836e-05</td>\n",
       "      <td>1.311653e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119286</td>\n",
       "      <td>-0.009101</td>\n",
       "      <td>-0.065067</td>\n",
       "      <td>0.118004</td>\n",
       "      <td>-0.106717</td>\n",
       "      <td>-0.164172</td>\n",
       "      <td>-0.053234</td>\n",
       "      <td>-0.081339</td>\n",
       "      <td>-0.080208</td>\n",
       "      <td>-0.036831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.739522e-05</td>\n",
       "      <td>-3.164027e-05</td>\n",
       "      <td>9.815199e-08</td>\n",
       "      <td>8.226781e-06</td>\n",
       "      <td>-2.607091e-03</td>\n",
       "      <td>-9.914978e-05</td>\n",
       "      <td>-8.893148e-06</td>\n",
       "      <td>-3.921824e-05</td>\n",
       "      <td>7.500358e-06</td>\n",
       "      <td>-2.689069e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130349</td>\n",
       "      <td>-0.006834</td>\n",
       "      <td>-0.080467</td>\n",
       "      <td>0.130852</td>\n",
       "      <td>-0.129759</td>\n",
       "      <td>-0.172083</td>\n",
       "      <td>-0.062098</td>\n",
       "      <td>-0.095710</td>\n",
       "      <td>-0.090356</td>\n",
       "      <td>-0.043638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.269553e-07</td>\n",
       "      <td>1.569342e-06</td>\n",
       "      <td>-2.580924e-08</td>\n",
       "      <td>-2.326338e-06</td>\n",
       "      <td>-2.287546e-04</td>\n",
       "      <td>4.477086e-06</td>\n",
       "      <td>5.372725e-07</td>\n",
       "      <td>-6.516146e-07</td>\n",
       "      <td>1.268538e-07</td>\n",
       "      <td>-3.920284e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137011</td>\n",
       "      <td>-0.005980</td>\n",
       "      <td>-0.091931</td>\n",
       "      <td>0.138971</td>\n",
       "      <td>-0.157257</td>\n",
       "      <td>-0.176509</td>\n",
       "      <td>-0.069453</td>\n",
       "      <td>-0.107274</td>\n",
       "      <td>-0.095383</td>\n",
       "      <td>-0.046607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.300539e-07</td>\n",
       "      <td>8.491727e-08</td>\n",
       "      <td>2.124261e-08</td>\n",
       "      <td>-5.194166e-09</td>\n",
       "      <td>-1.711311e-06</td>\n",
       "      <td>-2.246634e-07</td>\n",
       "      <td>-1.347103e-08</td>\n",
       "      <td>1.272403e-07</td>\n",
       "      <td>1.081522e-08</td>\n",
       "      <td>1.379866e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141709</td>\n",
       "      <td>-0.005171</td>\n",
       "      <td>-0.101017</td>\n",
       "      <td>0.145978</td>\n",
       "      <td>-0.178843</td>\n",
       "      <td>-0.180453</td>\n",
       "      <td>-0.075584</td>\n",
       "      <td>-0.119624</td>\n",
       "      <td>-0.098053</td>\n",
       "      <td>-0.047095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.778485e-10</td>\n",
       "      <td>2.599349e-09</td>\n",
       "      <td>-4.413521e-10</td>\n",
       "      <td>3.205889e-09</td>\n",
       "      <td>3.577440e-07</td>\n",
       "      <td>4.251778e-09</td>\n",
       "      <td>-8.010753e-10</td>\n",
       "      <td>1.935141e-09</td>\n",
       "      <td>6.821802e-10</td>\n",
       "      <td>-2.647242e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145790</td>\n",
       "      <td>-0.004706</td>\n",
       "      <td>-0.109504</td>\n",
       "      <td>0.151184</td>\n",
       "      <td>-0.195017</td>\n",
       "      <td>-0.183493</td>\n",
       "      <td>-0.079563</td>\n",
       "      <td>-0.131426</td>\n",
       "      <td>-0.099415</td>\n",
       "      <td>-0.046909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.008348e-39</td>\n",
       "      <td>-9.403872e-39</td>\n",
       "      <td>-1.102205e-38</td>\n",
       "      <td>-6.176625e-39</td>\n",
       "      <td>-1.082504e-38</td>\n",
       "      <td>4.926252e-39</td>\n",
       "      <td>4.125347e-39</td>\n",
       "      <td>-2.497178e-39</td>\n",
       "      <td>1.532827e-38</td>\n",
       "      <td>3.156683e-38</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229161</td>\n",
       "      <td>-0.012143</td>\n",
       "      <td>-0.274434</td>\n",
       "      <td>0.268113</td>\n",
       "      <td>-0.418617</td>\n",
       "      <td>-0.248419</td>\n",
       "      <td>-0.151194</td>\n",
       "      <td>-0.377344</td>\n",
       "      <td>-0.180246</td>\n",
       "      <td>-0.055528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.008348e-39</td>\n",
       "      <td>-9.403872e-39</td>\n",
       "      <td>-1.102205e-38</td>\n",
       "      <td>-6.176625e-39</td>\n",
       "      <td>-1.082504e-38</td>\n",
       "      <td>4.926252e-39</td>\n",
       "      <td>4.125347e-39</td>\n",
       "      <td>-2.497178e-39</td>\n",
       "      <td>1.532827e-38</td>\n",
       "      <td>-4.085126e-38</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231176</td>\n",
       "      <td>-0.012140</td>\n",
       "      <td>-0.276585</td>\n",
       "      <td>0.270454</td>\n",
       "      <td>-0.422120</td>\n",
       "      <td>-0.249731</td>\n",
       "      <td>-0.152306</td>\n",
       "      <td>-0.381729</td>\n",
       "      <td>-0.181646</td>\n",
       "      <td>-0.055522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.008348e-39</td>\n",
       "      <td>-9.403872e-39</td>\n",
       "      <td>-1.102205e-38</td>\n",
       "      <td>-6.176625e-39</td>\n",
       "      <td>-1.082504e-38</td>\n",
       "      <td>4.926252e-39</td>\n",
       "      <td>4.125347e-39</td>\n",
       "      <td>-2.497178e-39</td>\n",
       "      <td>1.532827e-38</td>\n",
       "      <td>-4.153385e-38</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232919</td>\n",
       "      <td>-0.012213</td>\n",
       "      <td>-0.279647</td>\n",
       "      <td>0.272979</td>\n",
       "      <td>-0.425895</td>\n",
       "      <td>-0.251312</td>\n",
       "      <td>-0.153784</td>\n",
       "      <td>-0.385938</td>\n",
       "      <td>-0.183550</td>\n",
       "      <td>-0.055799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.008348e-39</td>\n",
       "      <td>-9.403872e-39</td>\n",
       "      <td>-1.102205e-38</td>\n",
       "      <td>-6.176625e-39</td>\n",
       "      <td>-1.082504e-38</td>\n",
       "      <td>4.926252e-39</td>\n",
       "      <td>4.125347e-39</td>\n",
       "      <td>-2.497178e-39</td>\n",
       "      <td>1.532827e-38</td>\n",
       "      <td>3.293197e-38</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235038</td>\n",
       "      <td>-0.012295</td>\n",
       "      <td>-0.282234</td>\n",
       "      <td>0.275224</td>\n",
       "      <td>-0.429751</td>\n",
       "      <td>-0.252842</td>\n",
       "      <td>-0.154945</td>\n",
       "      <td>-0.390308</td>\n",
       "      <td>-0.185046</td>\n",
       "      <td>-0.056044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.008348e-39</td>\n",
       "      <td>-9.403872e-39</td>\n",
       "      <td>-1.102205e-38</td>\n",
       "      <td>-6.176625e-39</td>\n",
       "      <td>-1.082504e-38</td>\n",
       "      <td>4.926252e-39</td>\n",
       "      <td>4.125347e-39</td>\n",
       "      <td>-2.497178e-39</td>\n",
       "      <td>1.532827e-38</td>\n",
       "      <td>3.343721e-38</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237335</td>\n",
       "      <td>-0.012506</td>\n",
       "      <td>-0.284855</td>\n",
       "      <td>0.277507</td>\n",
       "      <td>-0.433445</td>\n",
       "      <td>-0.254390</td>\n",
       "      <td>-0.156235</td>\n",
       "      <td>-0.394852</td>\n",
       "      <td>-0.186996</td>\n",
       "      <td>-0.056179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 417500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1             2             3             4    \\\n",
       "0   1.108452e-03  3.076525e-05 -2.651312e-04  3.421936e-04 -1.268616e-02   \n",
       "0  -4.739522e-05 -3.164027e-05  9.815199e-08  8.226781e-06 -2.607091e-03   \n",
       "0  -1.269553e-07  1.569342e-06 -2.580924e-08 -2.326338e-06 -2.287546e-04   \n",
       "0   1.300539e-07  8.491727e-08  2.124261e-08 -5.194166e-09 -1.711311e-06   \n",
       "0   2.778485e-10  2.599349e-09 -4.413521e-10  3.205889e-09  3.577440e-07   \n",
       "..           ...           ...           ...           ...           ...   \n",
       "0   9.008348e-39 -9.403872e-39 -1.102205e-38 -6.176625e-39 -1.082504e-38   \n",
       "0   9.008348e-39 -9.403872e-39 -1.102205e-38 -6.176625e-39 -1.082504e-38   \n",
       "0   9.008348e-39 -9.403872e-39 -1.102205e-38 -6.176625e-39 -1.082504e-38   \n",
       "0   9.008348e-39 -9.403872e-39 -1.102205e-38 -6.176625e-39 -1.082504e-38   \n",
       "0   9.008348e-39 -9.403872e-39 -1.102205e-38 -6.176625e-39 -1.082504e-38   \n",
       "\n",
       "             5             6             7             8             9    ...  \\\n",
       "0   2.904374e-03 -2.349899e-04  9.891548e-04 -2.102836e-05  1.311653e-04  ...   \n",
       "0  -9.914978e-05 -8.893148e-06 -3.921824e-05  7.500358e-06 -2.689069e-06  ...   \n",
       "0   4.477086e-06  5.372725e-07 -6.516146e-07  1.268538e-07 -3.920284e-08  ...   \n",
       "0  -2.246634e-07 -1.347103e-08  1.272403e-07  1.081522e-08  1.379866e-08  ...   \n",
       "0   4.251778e-09 -8.010753e-10  1.935141e-09  6.821802e-10 -2.647242e-10  ...   \n",
       "..           ...           ...           ...           ...           ...  ...   \n",
       "0   4.926252e-39  4.125347e-39 -2.497178e-39  1.532827e-38  3.156683e-38  ...   \n",
       "0   4.926252e-39  4.125347e-39 -2.497178e-39  1.532827e-38 -4.085126e-38  ...   \n",
       "0   4.926252e-39  4.125347e-39 -2.497178e-39  1.532827e-38 -4.153385e-38  ...   \n",
       "0   4.926252e-39  4.125347e-39 -2.497178e-39  1.532827e-38  3.293197e-38  ...   \n",
       "0   4.926252e-39  4.125347e-39 -2.497178e-39  1.532827e-38  3.343721e-38  ...   \n",
       "\n",
       "         490       491       492       493       494       495       496  \\\n",
       "0  -0.119286 -0.009101 -0.065067  0.118004 -0.106717 -0.164172 -0.053234   \n",
       "0  -0.130349 -0.006834 -0.080467  0.130852 -0.129759 -0.172083 -0.062098   \n",
       "0  -0.137011 -0.005980 -0.091931  0.138971 -0.157257 -0.176509 -0.069453   \n",
       "0  -0.141709 -0.005171 -0.101017  0.145978 -0.178843 -0.180453 -0.075584   \n",
       "0  -0.145790 -0.004706 -0.109504  0.151184 -0.195017 -0.183493 -0.079563   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "0  -0.229161 -0.012143 -0.274434  0.268113 -0.418617 -0.248419 -0.151194   \n",
       "0  -0.231176 -0.012140 -0.276585  0.270454 -0.422120 -0.249731 -0.152306   \n",
       "0  -0.232919 -0.012213 -0.279647  0.272979 -0.425895 -0.251312 -0.153784   \n",
       "0  -0.235038 -0.012295 -0.282234  0.275224 -0.429751 -0.252842 -0.154945   \n",
       "0  -0.237335 -0.012506 -0.284855  0.277507 -0.433445 -0.254390 -0.156235   \n",
       "\n",
       "         497       498       499  \n",
       "0  -0.081339 -0.080208 -0.036831  \n",
       "0  -0.095710 -0.090356 -0.043638  \n",
       "0  -0.107274 -0.095383 -0.046607  \n",
       "0  -0.119624 -0.098053 -0.047095  \n",
       "0  -0.131426 -0.099415 -0.046909  \n",
       "..       ...       ...       ...  \n",
       "0  -0.377344 -0.180246 -0.055528  \n",
       "0  -0.381729 -0.181646 -0.055522  \n",
       "0  -0.385938 -0.183550 -0.055799  \n",
       "0  -0.390308 -0.185046 -0.056044  \n",
       "0  -0.394852 -0.186996 -0.056179  \n",
       "\n",
       "[360 rows x 417500 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Loss:(360,) & Shape of Acc: (360,)\n"
     ]
    }
   ],
   "source": [
    "train_acc_df = pd.DataFrame(trainAccArr)\n",
    "train_acc_data = np.array(train_acc_df).flatten()\n",
    "\n",
    "train_loss_df = pd.DataFrame(trainLossArr)\n",
    "train_loss_data = np.array(train_loss_df).flatten()\n",
    "\n",
    "print(f'Shape of Loss:{np.shape(train_loss_data)} & Shape of Acc: {np.shape(train_acc_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0             1             2             3             4    \\\n",
      "0   1.108452e-03  3.076525e-05 -2.651312e-04  3.421936e-04 -1.268616e-02   \n",
      "0  -4.739522e-05 -3.164027e-05  9.815199e-08  8.226781e-06 -2.607091e-03   \n",
      "0  -1.269553e-07  1.569342e-06 -2.580924e-08 -2.326338e-06 -2.287546e-04   \n",
      "0   1.300539e-07  8.491727e-08  2.124261e-08 -5.194166e-09 -1.711311e-06   \n",
      "0   2.778485e-10  2.599349e-09 -4.413521e-10  3.205889e-09  3.577440e-07   \n",
      "..           ...           ...           ...           ...           ...   \n",
      "0   9.008348e-39 -9.403872e-39 -1.102205e-38 -6.176625e-39 -1.082504e-38   \n",
      "0   9.008348e-39 -9.403872e-39 -1.102205e-38 -6.176625e-39 -1.082504e-38   \n",
      "0   9.008348e-39 -9.403872e-39 -1.102205e-38 -6.176625e-39 -1.082504e-38   \n",
      "0   9.008348e-39 -9.403872e-39 -1.102205e-38 -6.176625e-39 -1.082504e-38   \n",
      "0   9.008348e-39 -9.403872e-39 -1.102205e-38 -6.176625e-39 -1.082504e-38   \n",
      "\n",
      "             5             6             7             8             9    ...  \\\n",
      "0   2.904374e-03 -2.349899e-04  9.891548e-04 -2.102836e-05  1.311653e-04  ...   \n",
      "0  -9.914978e-05 -8.893148e-06 -3.921824e-05  7.500358e-06 -2.689069e-06  ...   \n",
      "0   4.477086e-06  5.372725e-07 -6.516146e-07  1.268538e-07 -3.920284e-08  ...   \n",
      "0  -2.246634e-07 -1.347103e-08  1.272403e-07  1.081522e-08  1.379866e-08  ...   \n",
      "0   4.251778e-09 -8.010753e-10  1.935141e-09  6.821802e-10 -2.647242e-10  ...   \n",
      "..           ...           ...           ...           ...           ...  ...   \n",
      "0   4.926252e-39  4.125347e-39 -2.497178e-39  1.532827e-38  3.156683e-38  ...   \n",
      "0   4.926252e-39  4.125347e-39 -2.497178e-39  1.532827e-38 -4.085126e-38  ...   \n",
      "0   4.926252e-39  4.125347e-39 -2.497178e-39  1.532827e-38 -4.153385e-38  ...   \n",
      "0   4.926252e-39  4.125347e-39 -2.497178e-39  1.532827e-38  3.293197e-38  ...   \n",
      "0   4.926252e-39  4.125347e-39 -2.497178e-39  1.532827e-38  3.343721e-38  ...   \n",
      "\n",
      "         490       491       492       493       494       495       496  \\\n",
      "0  -0.119286 -0.009101 -0.065067  0.118004 -0.106717 -0.164172 -0.053234   \n",
      "0  -0.130349 -0.006834 -0.080467  0.130852 -0.129759 -0.172083 -0.062098   \n",
      "0  -0.137011 -0.005980 -0.091931  0.138971 -0.157257 -0.176509 -0.069453   \n",
      "0  -0.141709 -0.005171 -0.101017  0.145978 -0.178843 -0.180453 -0.075584   \n",
      "0  -0.145790 -0.004706 -0.109504  0.151184 -0.195017 -0.183493 -0.079563   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "0  -0.229161 -0.012143 -0.274434  0.268113 -0.418617 -0.248419 -0.151194   \n",
      "0  -0.231176 -0.012140 -0.276585  0.270454 -0.422120 -0.249731 -0.152306   \n",
      "0  -0.232919 -0.012213 -0.279647  0.272979 -0.425895 -0.251312 -0.153784   \n",
      "0  -0.235038 -0.012295 -0.282234  0.275224 -0.429751 -0.252842 -0.154945   \n",
      "0  -0.237335 -0.012506 -0.284855  0.277507 -0.433445 -0.254390 -0.156235   \n",
      "\n",
      "         497       498       499  \n",
      "0  -0.081339 -0.080208 -0.036831  \n",
      "0  -0.095710 -0.090356 -0.043638  \n",
      "0  -0.107274 -0.095383 -0.046607  \n",
      "0  -0.119624 -0.098053 -0.047095  \n",
      "0  -0.131426 -0.099415 -0.046909  \n",
      "..       ...       ...       ...  \n",
      "0  -0.377344 -0.180246 -0.055528  \n",
      "0  -0.381729 -0.181646 -0.055522  \n",
      "0  -0.385938 -0.183550 -0.055799  \n",
      "0  -0.390308 -0.185046 -0.056044  \n",
      "0  -0.394852 -0.186996 -0.056179  \n",
      "\n",
      "[360 rows x 417500 columns]\n"
     ]
    }
   ],
   "source": [
    "t1 = all_df\n",
    "print(pd.DataFrame(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.484435</td>\n",
       "      <td>7.364438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.131791</td>\n",
       "      <td>1.286802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.699397</td>\n",
       "      <td>5.562063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88.437144</td>\n",
       "      <td>0.394324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.393364</td>\n",
       "      <td>4.006983</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>91.463650</td>\n",
       "      <td>0.294259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.835290</td>\n",
       "      <td>2.569232</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>92.772298</td>\n",
       "      <td>0.251976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.134159</td>\n",
       "      <td>1.304998</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>93.789547</td>\n",
       "      <td>0.220068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>-6.218816</td>\n",
       "      <td>2.322319</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>99.621042</td>\n",
       "      <td>0.019140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>-6.321358</td>\n",
       "      <td>2.493414</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>99.767292</td>\n",
       "      <td>0.017648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>-6.406707</td>\n",
       "      <td>2.646224</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>99.753405</td>\n",
       "      <td>0.016912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>-6.495262</td>\n",
       "      <td>2.797075</td>\n",
       "      <td>43</td>\n",
       "      <td>7</td>\n",
       "      <td>99.824129</td>\n",
       "      <td>0.016311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>-6.570872</td>\n",
       "      <td>2.943630</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>99.790570</td>\n",
       "      <td>0.015348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x         y  Epoch  Iteration        Acc      Loss\n",
       "0    11.484435  7.364438      0          0  47.131791  1.286802\n",
       "1    11.699397  5.562063      1          0  88.437144  0.394324\n",
       "2    11.393364  4.006983      2          0  91.463650  0.294259\n",
       "3    10.835290  2.569232      3          0  92.772298  0.251976\n",
       "4    10.134159  1.304998      4          0  93.789547  0.220068\n",
       "..         ...       ...    ...        ...        ...       ...\n",
       "355  -6.218816  2.322319     40          7  99.621042  0.019140\n",
       "356  -6.321358  2.493414     41          7  99.767292  0.017648\n",
       "357  -6.406707  2.646224     42          7  99.753405  0.016912\n",
       "358  -6.495262  2.797075     43          7  99.824129  0.016311\n",
       "359  -6.570872  2.943630     44          7  99.790570  0.015348\n",
       "\n",
       "[360 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = np.array(t1)\n",
    "pca = PCA(n_components=2)\n",
    "new_data = pca.fit_transform(t1)\n",
    "# scaling=StandardScaler()\n",
    "# scaled_data = scaling.fit_transform(new_data)\n",
    "\n",
    "allEpochDf = pd.DataFrame(new_data,columns=['x','y'])\n",
    "\n",
    "eps_each_time = [i for i in range(max_epochs)] * 8\n",
    "times = np.repeat([i for i in range(8)],max_epochs)\n",
    "\n",
    "allEpochDf['Epoch']=eps_each_time\n",
    "allEpochDf['Iteration']=(times)\n",
    "allEpochDf[\"Acc\"] = train_acc_data\n",
    "allEpochDf[\"Loss\"] = train_loss_data\n",
    "\n",
    "allEpochDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.393364</td>\n",
       "      <td>4.006983</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>91.463650</td>\n",
       "      <td>0.294259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.356701</td>\n",
       "      <td>0.230622</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>94.410890</td>\n",
       "      <td>0.194354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.851554</td>\n",
       "      <td>-1.993984</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>95.896526</td>\n",
       "      <td>0.140965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.415884</td>\n",
       "      <td>-2.990983</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>96.933228</td>\n",
       "      <td>0.107872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.273011</td>\n",
       "      <td>-3.161357</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>97.682687</td>\n",
       "      <td>0.085037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-4.990288</td>\n",
       "      <td>0.738444</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>99.387775</td>\n",
       "      <td>0.027764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-5.546024</td>\n",
       "      <td>1.369022</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>99.541669</td>\n",
       "      <td>0.024140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>-5.987475</td>\n",
       "      <td>1.956722</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>99.680361</td>\n",
       "      <td>0.020665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-6.321358</td>\n",
       "      <td>2.493414</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>99.767292</td>\n",
       "      <td>0.017648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>-6.570872</td>\n",
       "      <td>2.943630</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>99.790570</td>\n",
       "      <td>0.015348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x         y  Epoch  Iteration        Acc      Loss\n",
       "0    11.393364  4.006983      2          0  91.463650  0.294259\n",
       "1     9.356701  0.230622      5          0  94.410890  0.194354\n",
       "2     6.851554 -1.993984      8          0  95.896526  0.140965\n",
       "3     4.415884 -2.990983     11          0  96.933228  0.107872\n",
       "4     2.273011 -3.161357     14          0  97.682687  0.085037\n",
       "..         ...       ...    ...        ...        ...       ...\n",
       "115  -4.990288  0.738444     32          7  99.387775  0.027764\n",
       "116  -5.546024  1.369022     35          7  99.541669  0.024140\n",
       "117  -5.987475  1.956722     38          7  99.680361  0.020665\n",
       "118  -6.321358  2.493414     41          7  99.767292  0.017648\n",
       "119  -6.570872  2.943630     44          7  99.790570  0.015348\n",
       "\n",
       "[120 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch3Df = allEpochDf.loc[(allEpochDf['Epoch']+1)%3 == 0]\n",
    "epoch3Df = epoch3Df.reset_index(drop=True)\n",
    "epoch3Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array(epoch3Df.Acc)\n",
    "mv = []\n",
    "for i in range(len(test)):\n",
    "    mv.append(str(int(test[i])))\n",
    "len(mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = ['red','blue','purple','green','yellow','brown','black','grey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 5000x2500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for i in range(len(mv)):\n",
    "    m = mv[i]\n",
    "    c = epoch3Df['Iteration'][i]\n",
    "    plt.scatter(epoch3Df['x'][i],epoch3Df['y'][i],marker=f'${m}$',color = cmap[c])\n",
    "    plt.title(\"PCA for model\",color=\"g\")\n",
    "    \n",
    "plt.figure(figsize=[10,5],dpi=500)\n",
    "\n",
    "plt.savefig('D:/Clemson/COURSE/SEM-2/CPSC-8430 Deep Learning - 001/Homework/CPSC-8430-Deep-Learning-001/HW1/plots/PCA_1Copy.jpg',\n",
    "            format='jpeg',\n",
    "            dpi=700,\n",
    "            bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_Df = all_df.iloc[:,0:7840]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = layer1_Df \n",
    "t2 = np.array(t2)\n",
    "pca = PCA(n_components=2)\n",
    "new_data2 = pca.fit_transform(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.214312</td>\n",
       "      <td>1.007225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.131791</td>\n",
       "      <td>1.286802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.332997</td>\n",
       "      <td>0.756948</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88.437144</td>\n",
       "      <td>0.394324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.340704</td>\n",
       "      <td>0.539317</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>91.463650</td>\n",
       "      <td>0.294259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.305381</td>\n",
       "      <td>0.346261</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>92.772298</td>\n",
       "      <td>0.251976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.248238</td>\n",
       "      <td>0.183395</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>93.789547</td>\n",
       "      <td>0.220068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>-0.784708</td>\n",
       "      <td>0.275449</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>99.621042</td>\n",
       "      <td>0.019140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>-0.796055</td>\n",
       "      <td>0.302956</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>99.767292</td>\n",
       "      <td>0.017648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>-0.807146</td>\n",
       "      <td>0.319752</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>99.753405</td>\n",
       "      <td>0.016912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>-0.818237</td>\n",
       "      <td>0.335618</td>\n",
       "      <td>43</td>\n",
       "      <td>7</td>\n",
       "      <td>99.824129</td>\n",
       "      <td>0.016311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>-0.828752</td>\n",
       "      <td>0.353254</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>99.790570</td>\n",
       "      <td>0.015348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x         y  Epoch  Iteration        Acc      Loss\n",
       "0    1.214312  1.007225      0          0  47.131791  1.286802\n",
       "1    1.332997  0.756948      1          0  88.437144  0.394324\n",
       "2    1.340704  0.539317      2          0  91.463650  0.294259\n",
       "3    1.305381  0.346261      3          0  92.772298  0.251976\n",
       "4    1.248238  0.183395      4          0  93.789547  0.220068\n",
       "..        ...       ...    ...        ...        ...       ...\n",
       "355 -0.784708  0.275449     40          7  99.621042  0.019140\n",
       "356 -0.796055  0.302956     41          7  99.767292  0.017648\n",
       "357 -0.807146  0.319752     42          7  99.753405  0.016912\n",
       "358 -0.818237  0.335618     43          7  99.824129  0.016311\n",
       "359 -0.828752  0.353254     44          7  99.790570  0.015348\n",
       "\n",
       "[360 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1Df = pd.DataFrame(new_data2,columns=['x','y'])\n",
    "\n",
    "eps_each_time = [i for i in range(max_epochs)] * 8\n",
    "times = np.repeat([i for i in range(8)],max_epochs)\n",
    "\n",
    "layer1Df['Epoch']=eps_each_time\n",
    "layer1Df['Iteration']=(times)\n",
    "layer1Df[\"Acc\"] = train_acc_data\n",
    "layer1Df[\"Loss\"] = train_loss_data\n",
    "\n",
    "layer1Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(layer1Df.Acc)\n",
    "mv = []\n",
    "for i in range(len(test)):\n",
    "    mv.append(str(int(test[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(mv)):\n",
    "    m = mv[i]\n",
    "    c = layer1Df['Iteration'][i]\n",
    "    plt.scatter(layer1Df['x'][i],layer1Df['y'][i],marker=f'${m}$',color = cmap[c])\n",
    "    plt.title(\"PCA for Layer1\",color=\"g\")\n",
    "\n",
    "plt.savefig('D:/Clemson/COURSE/SEM-2/CPSC-8430 Deep Learning - 001/Homework/CPSC-8430-Deep-Learning-001/HW1/plots/PCA_Layer1.jpg',\n",
    "            format='jpeg',\n",
    "            dpi=700,\n",
    "            bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1d86b2f3ed665d691ce24c615a98bbc398f66743afc4d4e970e6f8b36fab2b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CPSC-8430-DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
