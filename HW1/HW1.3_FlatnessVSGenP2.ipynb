{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2494a45b870>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 60000 \n",
      "test_dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset),\"\\ntest_dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader func\n",
    "def train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def test_loader(batch_size):\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def trainFunc(model,num_epochs,train_batch_size):\n",
    "    model.train()\n",
    "    print('strated')\n",
    "    train_load = train_loader(train_batch_size)\n",
    "    n_total_steps = len(train_load)\n",
    "    train_losses = []\n",
    "    train_epoch = []\n",
    "    train_acc = []\n",
    "    not_converged =True\n",
    "    epoch = 0\n",
    "    while not_converged:\n",
    "        epoch += 1\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i, (images, labels) in enumerate(train_load):  \n",
    "            #if (i+1)% 60 == 0 : print(i+1)\n",
    "            \n",
    "            # Forward pass\n",
    "            prediction = model(images.view(-1,784))\n",
    "            loss = loss_func(prediction, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_acc.append(acc)\n",
    "            train_epoch.append(epoch)\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print (f'Train O/P: Epoch [{epoch}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "                # train_epoch.append(epoch)\n",
    "                # print(f'Epoch [{epoch}/{num_epochs}], Accuracy : {acc} %')\n",
    "                # train_acc.append(acc)\n",
    "                if epoch == num_epochs:\n",
    "                        print(\"Max Epoch Reached\")\n",
    "                        not_converged = False\n",
    "                elif (epoch > 5) and  (train_losses[-1] < 0.001):\n",
    "                    if abs(train_losses[-3] - train_losses[-2]) < 1.0e-05 and abs(train_losses[-2] - train_losses[-1]) < 1.0e-05:\n",
    "                        print(\"Convergeance reached for loss:\",train_losses[-1])\n",
    "                        not_converged = False\n",
    "                        \n",
    "    return train_epoch,train_losses,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  10.  385.  760. 1135. 1510.]\n"
     ]
    }
   ],
   "source": [
    "batchArr = np.linspace (10,1510,5)\n",
    "print(batchArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunction(model,loss_func,test_batch_size): \n",
    "    test_load = test_loader(test_batch_size)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_load:\n",
    "            prediction = model(images.view(-1,784))\n",
    "            testLoss = loss_func(prediction,labels)\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    netTest_acc1 = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the test images: {netTest_acc1} %')\n",
    "    return netTest_acc1, testLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation for sensitivity\n",
    "def sensitivityFunc(model):\n",
    "    gradArr = []\n",
    "    froGrad=0\n",
    "    count =0 \n",
    "    sensitivity=[]\n",
    "    grad_all = 0.0\n",
    "    for p in model.parameters():\n",
    "        grad = 0.0\n",
    "        if p.grad is not None:\n",
    "            grad = (p.grad.cpu().data.numpy()**2).sum()\n",
    "            froGrad_norm = torch.norm(grad, p='fro')\n",
    "            froGrad += froGrad_norm\n",
    "            count += 1\n",
    "        grad_all += grad\n",
    "    grad_norm = grad_all ** 0.5\n",
    "    gradArr.append(grad_norm)\n",
    "    sensitivity.append(froGrad/count)\n",
    "    return gradArr,sensitivity\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model :84060 for with batch size:10\n",
      "strated\n",
      "Train O/P: Epoch [1/15], Step [10/6000], Loss: 2.3100\n",
      "Train O/P: Epoch [1/15], Step [20/6000], Loss: 2.2461\n",
      "Train O/P: Epoch [1/15], Step [30/6000], Loss: 2.0772\n",
      "Train O/P: Epoch [1/15], Step [40/6000], Loss: 1.8255\n",
      "Train O/P: Epoch [1/15], Step [50/6000], Loss: 1.5338\n",
      "Train O/P: Epoch [1/15], Step [60/6000], Loss: 1.6724\n",
      "Train O/P: Epoch [1/15], Step [70/6000], Loss: 1.2940\n",
      "Train O/P: Epoch [1/15], Step [80/6000], Loss: 1.2695\n",
      "Train O/P: Epoch [1/15], Step [90/6000], Loss: 1.0237\n",
      "Train O/P: Epoch [1/15], Step [100/6000], Loss: 0.5741\n",
      "Train O/P: Epoch [1/15], Step [110/6000], Loss: 0.7218\n",
      "Train O/P: Epoch [1/15], Step [120/6000], Loss: 1.1984\n",
      "Train O/P: Epoch [1/15], Step [130/6000], Loss: 0.6655\n",
      "Train O/P: Epoch [1/15], Step [140/6000], Loss: 0.3398\n",
      "Train O/P: Epoch [1/15], Step [150/6000], Loss: 0.9652\n",
      "Train O/P: Epoch [1/15], Step [160/6000], Loss: 0.6454\n",
      "Train O/P: Epoch [1/15], Step [170/6000], Loss: 0.6610\n",
      "Train O/P: Epoch [1/15], Step [180/6000], Loss: 1.5982\n",
      "Train O/P: Epoch [1/15], Step [190/6000], Loss: 0.4855\n",
      "Train O/P: Epoch [1/15], Step [200/6000], Loss: 1.2465\n",
      "Train O/P: Epoch [1/15], Step [210/6000], Loss: 0.6786\n",
      "Train O/P: Epoch [1/15], Step [220/6000], Loss: 0.6215\n",
      "Train O/P: Epoch [1/15], Step [230/6000], Loss: 0.7567\n",
      "Train O/P: Epoch [1/15], Step [240/6000], Loss: 0.7523\n",
      "Train O/P: Epoch [1/15], Step [250/6000], Loss: 0.4760\n",
      "Train O/P: Epoch [1/15], Step [260/6000], Loss: 0.4162\n",
      "Train O/P: Epoch [1/15], Step [270/6000], Loss: 0.2883\n",
      "Train O/P: Epoch [1/15], Step [280/6000], Loss: 0.2305\n",
      "Train O/P: Epoch [1/15], Step [290/6000], Loss: 0.2299\n",
      "Train O/P: Epoch [1/15], Step [300/6000], Loss: 0.3240\n",
      "Train O/P: Epoch [1/15], Step [310/6000], Loss: 0.3009\n",
      "Train O/P: Epoch [1/15], Step [320/6000], Loss: 0.4164\n",
      "Train O/P: Epoch [1/15], Step [330/6000], Loss: 0.1526\n",
      "Train O/P: Epoch [1/15], Step [340/6000], Loss: 0.3012\n",
      "Train O/P: Epoch [1/15], Step [350/6000], Loss: 0.5581\n",
      "Train O/P: Epoch [1/15], Step [360/6000], Loss: 0.1466\n",
      "Train O/P: Epoch [1/15], Step [370/6000], Loss: 0.2701\n",
      "Train O/P: Epoch [1/15], Step [380/6000], Loss: 0.1158\n",
      "Train O/P: Epoch [1/15], Step [390/6000], Loss: 0.2952\n",
      "Train O/P: Epoch [1/15], Step [400/6000], Loss: 0.3361\n",
      "Train O/P: Epoch [1/15], Step [410/6000], Loss: 0.6750\n",
      "Train O/P: Epoch [1/15], Step [420/6000], Loss: 0.2964\n",
      "Train O/P: Epoch [1/15], Step [430/6000], Loss: 0.4223\n",
      "Train O/P: Epoch [1/15], Step [440/6000], Loss: 0.3204\n",
      "Train O/P: Epoch [1/15], Step [450/6000], Loss: 0.3022\n",
      "Train O/P: Epoch [1/15], Step [460/6000], Loss: 0.7805\n",
      "Train O/P: Epoch [1/15], Step [470/6000], Loss: 0.1767\n",
      "Train O/P: Epoch [1/15], Step [480/6000], Loss: 0.1114\n",
      "Train O/P: Epoch [1/15], Step [490/6000], Loss: 0.6278\n",
      "Train O/P: Epoch [1/15], Step [500/6000], Loss: 0.7322\n",
      "Train O/P: Epoch [1/15], Step [510/6000], Loss: 0.3469\n",
      "Train O/P: Epoch [1/15], Step [520/6000], Loss: 1.5249\n",
      "Train O/P: Epoch [1/15], Step [530/6000], Loss: 0.4097\n",
      "Train O/P: Epoch [1/15], Step [540/6000], Loss: 0.4810\n",
      "Train O/P: Epoch [1/15], Step [550/6000], Loss: 0.1337\n",
      "Train O/P: Epoch [1/15], Step [560/6000], Loss: 0.2274\n",
      "Train O/P: Epoch [1/15], Step [570/6000], Loss: 0.3466\n",
      "Train O/P: Epoch [1/15], Step [580/6000], Loss: 0.7217\n",
      "Train O/P: Epoch [1/15], Step [590/6000], Loss: 0.1337\n",
      "Train O/P: Epoch [1/15], Step [600/6000], Loss: 0.4541\n",
      "Train O/P: Epoch [1/15], Step [610/6000], Loss: 0.3534\n",
      "Train O/P: Epoch [1/15], Step [620/6000], Loss: 0.4591\n",
      "Train O/P: Epoch [1/15], Step [630/6000], Loss: 0.7108\n",
      "Train O/P: Epoch [1/15], Step [640/6000], Loss: 0.7274\n",
      "Train O/P: Epoch [1/15], Step [650/6000], Loss: 0.2410\n",
      "Train O/P: Epoch [1/15], Step [660/6000], Loss: 0.1749\n",
      "Train O/P: Epoch [1/15], Step [670/6000], Loss: 0.3889\n",
      "Train O/P: Epoch [1/15], Step [680/6000], Loss: 0.0558\n",
      "Train O/P: Epoch [1/15], Step [690/6000], Loss: 0.5212\n",
      "Train O/P: Epoch [1/15], Step [700/6000], Loss: 0.3547\n",
      "Train O/P: Epoch [1/15], Step [710/6000], Loss: 0.3722\n",
      "Train O/P: Epoch [1/15], Step [720/6000], Loss: 0.4779\n",
      "Train O/P: Epoch [1/15], Step [730/6000], Loss: 0.2318\n",
      "Train O/P: Epoch [1/15], Step [740/6000], Loss: 0.0616\n",
      "Train O/P: Epoch [1/15], Step [750/6000], Loss: 0.3824\n",
      "Train O/P: Epoch [1/15], Step [760/6000], Loss: 0.1267\n",
      "Train O/P: Epoch [1/15], Step [770/6000], Loss: 0.1254\n",
      "Train O/P: Epoch [1/15], Step [780/6000], Loss: 0.1464\n",
      "Train O/P: Epoch [1/15], Step [790/6000], Loss: 0.0365\n",
      "Train O/P: Epoch [1/15], Step [800/6000], Loss: 0.1758\n",
      "Train O/P: Epoch [1/15], Step [810/6000], Loss: 0.3086\n",
      "Train O/P: Epoch [1/15], Step [820/6000], Loss: 0.3037\n",
      "Train O/P: Epoch [1/15], Step [830/6000], Loss: 0.2114\n",
      "Train O/P: Epoch [1/15], Step [840/6000], Loss: 0.0913\n",
      "Train O/P: Epoch [1/15], Step [850/6000], Loss: 0.8373\n",
      "Train O/P: Epoch [1/15], Step [860/6000], Loss: 0.0965\n",
      "Train O/P: Epoch [1/15], Step [870/6000], Loss: 0.1581\n",
      "Train O/P: Epoch [1/15], Step [880/6000], Loss: 0.5660\n",
      "Train O/P: Epoch [1/15], Step [890/6000], Loss: 0.3551\n",
      "Train O/P: Epoch [1/15], Step [900/6000], Loss: 0.6448\n",
      "Train O/P: Epoch [1/15], Step [910/6000], Loss: 0.1105\n",
      "Train O/P: Epoch [1/15], Step [920/6000], Loss: 0.6168\n",
      "Train O/P: Epoch [1/15], Step [930/6000], Loss: 0.4043\n",
      "Train O/P: Epoch [1/15], Step [940/6000], Loss: 0.4174\n",
      "Train O/P: Epoch [1/15], Step [950/6000], Loss: 0.1201\n",
      "Train O/P: Epoch [1/15], Step [960/6000], Loss: 0.4519\n",
      "Train O/P: Epoch [1/15], Step [970/6000], Loss: 0.0403\n",
      "Train O/P: Epoch [1/15], Step [980/6000], Loss: 0.8457\n",
      "Train O/P: Epoch [1/15], Step [990/6000], Loss: 0.8097\n",
      "Train O/P: Epoch [1/15], Step [1000/6000], Loss: 0.5094\n",
      "Train O/P: Epoch [1/15], Step [1010/6000], Loss: 0.6102\n",
      "Train O/P: Epoch [1/15], Step [1020/6000], Loss: 0.1761\n",
      "Train O/P: Epoch [1/15], Step [1030/6000], Loss: 0.2694\n",
      "Train O/P: Epoch [1/15], Step [1040/6000], Loss: 0.0436\n",
      "Train O/P: Epoch [1/15], Step [1050/6000], Loss: 0.1133\n",
      "Train O/P: Epoch [1/15], Step [1060/6000], Loss: 0.1534\n",
      "Train O/P: Epoch [1/15], Step [1070/6000], Loss: 0.1127\n",
      "Train O/P: Epoch [1/15], Step [1080/6000], Loss: 0.1182\n",
      "Train O/P: Epoch [1/15], Step [1090/6000], Loss: 0.1767\n",
      "Train O/P: Epoch [1/15], Step [1100/6000], Loss: 0.4488\n",
      "Train O/P: Epoch [1/15], Step [1110/6000], Loss: 0.0501\n",
      "Train O/P: Epoch [1/15], Step [1120/6000], Loss: 0.1664\n",
      "Train O/P: Epoch [1/15], Step [1130/6000], Loss: 0.1220\n",
      "Train O/P: Epoch [1/15], Step [1140/6000], Loss: 0.9100\n",
      "Train O/P: Epoch [1/15], Step [1150/6000], Loss: 0.3271\n",
      "Train O/P: Epoch [1/15], Step [1160/6000], Loss: 0.2909\n",
      "Train O/P: Epoch [1/15], Step [1170/6000], Loss: 0.1777\n",
      "Train O/P: Epoch [1/15], Step [1180/6000], Loss: 0.0940\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19596/4045091088.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Total no of parameters in Model :{np.sum(a)} for with batch size:{train_batch_size}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtest_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchArr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19596/3975309800.py\u001b[0m in \u001b[0;36mtrainFunc\u001b[1;34m(model, num_epochs, train_batch_size)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelsTrainEpochArr = []\n",
    "modelsTrainLossArr = []\n",
    "modelsTrainAccArr = []\n",
    "modelsTestLossArr = []\n",
    "modelsTestAccArr = []\n",
    "modelsSensitivityArr=[]\n",
    "\n",
    "for i in range (len(batchArr)):\n",
    "    j=copy.deepcopy(i) \n",
    "    j = Model()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(j.parameters(), lr=1e-3) #weight_decay = weight_decay_val)\n",
    "\n",
    "    max_epochs = 15\n",
    "    train_batch_size = int(batchArr[i])\n",
    "\n",
    "    a=[]\n",
    "    for k in j.parameters():\n",
    "        a.append(torch.numel(k))\n",
    "    print(f'Total no of parameters in Model :{np.sum(a)} for with batch size:{train_batch_size}')\n",
    "\n",
    "    train_epoch,train_losses,train_acc = trainFunc(j,max_epochs,train_batch_size)\n",
    "\n",
    "    test_batch_size = int(batchArr[i])\n",
    "\n",
    "    testLoss, testAcc = testFunction(j,loss_func,test_batch_size)\n",
    "\n",
    "    gradArr,sensitivity = sensitivityFunc(j)\n",
    "    \n",
    "    modelsTrainEpochArr.append(train_epoch)\n",
    "    modelsTrainLossArr.append(train_losses)\n",
    "    modelsTrainAccArr.append(train_acc)\n",
    "    modelsTestAccArr.append(testAcc)\n",
    "    modelsTestLossArr.append(testLoss)\n",
    "    modelsSensitivityArr.append(sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(modelsTrainAccArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 7.31 %\n",
      "Accuracy of the network on the test images: 10.46 %\n",
      "Accuracy of the network on the test images: 10.65 %\n",
      "Accuracy of the network on the test images: 11.31 %\n",
      "Accuracy of the network on the test images: 8.99 %\n"
     ]
    }
   ],
   "source": [
    "# modelsTestLossArr = []\n",
    "# modelsTestAccArr = []\n",
    "\n",
    "# for i in range (len(batchArr)):\n",
    "#     j=copy.deepcopy(i) \n",
    "#     j = Model()\n",
    "#     loss_func = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(j.parameters(), lr=1e-3) #weight_decay = weight_decay_val)\n",
    "\n",
    "#     test_batch_size = int(batchArr[i])\n",
    "\n",
    "#     testLoss, testAcc = testFunction(j,loss_func,test_batch_size)\n",
    "#     modelsTestAccArr.append(testAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsSensitivityArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alpha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19596/1664604903.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelsTrainLossArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Blue\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dashed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"o\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelsTestLossArr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Blue\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"v\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Train Loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Test Loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"center left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Alpha\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Green\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'alpha' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(batchArr,minScore(modelsTrainLossArr),color=\"Blue\",linestyle='dashed', marker=\"o\")\n",
    "ax.plot(batchArr,modelsTestLossArr,color=\"Blue\", marker=\"v\")\n",
    "ax.legend(['Train Loss','Test Loss'],loc=\"upper right\")\n",
    "ax.set_xlabel(\"Batch\",color=\"Green\")\n",
    "ax.set_ylabel(\"CrossEntropy Loss\",color = \"blue\")\n",
    "\n",
    "\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(batchArr,modelsSensitivityArr,color=\"red\")\n",
    "ax2.set_xlabel(\"Batch\",color=\"Green\")\n",
    "ax2.set_ylabel(\"Sensitivity\",color = \"red\")\n",
    "ax2.legend(['Sensitivity'],loc=\"center left\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('D:/Clemson/COURSE/SEM-2/CPSC-8430 Deep Learning - 001/Homework/CPSC-8430-Deep-Learning-001/HW1/Diff Batch Graph HW1_3.2.jpg',\n",
    "            format='jpeg',\n",
    "            dpi=100,\n",
    "            bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1d86b2f3ed665d691ce24c615a98bbc398f66743afc4d4e970e6f8b36fab2b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CPSC-8430-DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
