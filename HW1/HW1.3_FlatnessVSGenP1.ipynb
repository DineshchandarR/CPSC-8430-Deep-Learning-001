{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26023ea7870>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 60000 \n",
      "test_dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset),\"\\ntest_dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader func\n",
    "def train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def test_loader(batch_size):\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M1(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(M1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.conv2 = nn.Conv2d(3, 13, 5)\n",
    "        self.fc1 = nn.Linear(208, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # flatten as one dimension\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def trainFunc(model,num_epochs,train_batch_size):\n",
    "    model.train()\n",
    "    print('strated')\n",
    "    train_load = train_loader(train_batch_size)\n",
    "    n_total_steps = len(train_load)\n",
    "    train_losses = []\n",
    "    train_epoch = []\n",
    "    train_acc = []\n",
    "    not_converged =True\n",
    "    epoch = 0\n",
    "    while not_converged:\n",
    "        epoch += 1\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i, (images, labels) in enumerate(train_load):  \n",
    "            \n",
    "            images, labels = Variable(images),Variable(labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            prediction = model(images)\n",
    "            loss = loss_func(prediction, labels)\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_acc.append(acc)\n",
    "            train_epoch.append(epoch)\n",
    "\n",
    "            if (i+1) % 60 == 0:\n",
    "                print (f'Train O/P: Epoch [{epoch}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "   \n",
    "                if epoch == num_epochs:\n",
    "                        print(\"Max Epoch Reached\")\n",
    "                        not_converged = False\n",
    "                elif (epoch > 5) and  (train_losses[-1] < 0.001):\n",
    "                    if abs(train_losses[-3] - train_losses[-2]) < 1.0e-05 and abs(train_losses[-2] - train_losses[-1]) < 1.0e-05:\n",
    "                        print(\"Convergeance reached for loss:\",train_losses[-1])\n",
    "                        not_converged = False\n",
    "                        \n",
    "    return train_epoch,train_losses,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model with batch_size=64 is:37160\n"
     ]
    }
   ],
   "source": [
    "# Training Model with batch size=64\n",
    "torch.manual_seed(1)\n",
    "\n",
    "learning_rate = 0.0004\n",
    "mBatch1 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "weight_decay_val = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(mBatch1.parameters(), lr=learning_rate, weight_decay = weight_decay_val)\n",
    "\n",
    "a=[]\n",
    "for i in mBatch1.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model with batch_size={64} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "Train O/P: Epoch [1/8], Step [60/938], Loss: 1.8126\n",
      "Train O/P: Epoch [1/8], Step [120/938], Loss: 0.7484\n",
      "Train O/P: Epoch [1/8], Step [180/938], Loss: 0.6761\n",
      "Train O/P: Epoch [1/8], Step [240/938], Loss: 0.7137\n",
      "Train O/P: Epoch [1/8], Step [300/938], Loss: 0.4875\n",
      "Train O/P: Epoch [1/8], Step [360/938], Loss: 0.2338\n",
      "Train O/P: Epoch [1/8], Step [420/938], Loss: 0.3865\n",
      "Train O/P: Epoch [1/8], Step [480/938], Loss: 0.2349\n",
      "Train O/P: Epoch [1/8], Step [540/938], Loss: 0.2753\n",
      "Train O/P: Epoch [1/8], Step [600/938], Loss: 0.2953\n",
      "Train O/P: Epoch [1/8], Step [660/938], Loss: 0.3244\n",
      "Train O/P: Epoch [1/8], Step [720/938], Loss: 0.2153\n",
      "Train O/P: Epoch [1/8], Step [780/938], Loss: 0.0978\n",
      "Train O/P: Epoch [1/8], Step [840/938], Loss: 0.2839\n",
      "Train O/P: Epoch [1/8], Step [900/938], Loss: 0.4217\n",
      "Train O/P: Epoch [2/8], Step [60/938], Loss: 0.3644\n",
      "Train O/P: Epoch [2/8], Step [120/938], Loss: 0.1992\n",
      "Train O/P: Epoch [2/8], Step [180/938], Loss: 0.3326\n",
      "Train O/P: Epoch [2/8], Step [240/938], Loss: 0.0912\n",
      "Train O/P: Epoch [2/8], Step [300/938], Loss: 0.1916\n",
      "Train O/P: Epoch [2/8], Step [360/938], Loss: 0.0746\n",
      "Train O/P: Epoch [2/8], Step [420/938], Loss: 0.2588\n",
      "Train O/P: Epoch [2/8], Step [480/938], Loss: 0.1698\n",
      "Train O/P: Epoch [2/8], Step [540/938], Loss: 0.1551\n",
      "Train O/P: Epoch [2/8], Step [600/938], Loss: 0.2095\n",
      "Train O/P: Epoch [2/8], Step [660/938], Loss: 0.1565\n",
      "Train O/P: Epoch [2/8], Step [720/938], Loss: 0.0816\n",
      "Train O/P: Epoch [2/8], Step [780/938], Loss: 0.0645\n",
      "Train O/P: Epoch [2/8], Step [840/938], Loss: 0.1248\n",
      "Train O/P: Epoch [2/8], Step [900/938], Loss: 0.1076\n",
      "Train O/P: Epoch [3/8], Step [60/938], Loss: 0.1069\n",
      "Train O/P: Epoch [3/8], Step [120/938], Loss: 0.0844\n",
      "Train O/P: Epoch [3/8], Step [180/938], Loss: 0.0301\n",
      "Train O/P: Epoch [3/8], Step [240/938], Loss: 0.1129\n",
      "Train O/P: Epoch [3/8], Step [300/938], Loss: 0.1343\n",
      "Train O/P: Epoch [3/8], Step [360/938], Loss: 0.1177\n",
      "Train O/P: Epoch [3/8], Step [420/938], Loss: 0.1637\n",
      "Train O/P: Epoch [3/8], Step [480/938], Loss: 0.2310\n",
      "Train O/P: Epoch [3/8], Step [540/938], Loss: 0.0482\n",
      "Train O/P: Epoch [3/8], Step [600/938], Loss: 0.0815\n",
      "Train O/P: Epoch [3/8], Step [660/938], Loss: 0.0543\n",
      "Train O/P: Epoch [3/8], Step [720/938], Loss: 0.1073\n",
      "Train O/P: Epoch [3/8], Step [780/938], Loss: 0.1281\n",
      "Train O/P: Epoch [3/8], Step [840/938], Loss: 0.2006\n",
      "Train O/P: Epoch [3/8], Step [900/938], Loss: 0.1518\n",
      "Train O/P: Epoch [4/8], Step [60/938], Loss: 0.0196\n",
      "Train O/P: Epoch [4/8], Step [120/938], Loss: 0.0511\n",
      "Train O/P: Epoch [4/8], Step [180/938], Loss: 0.1164\n",
      "Train O/P: Epoch [4/8], Step [240/938], Loss: 0.0790\n",
      "Train O/P: Epoch [4/8], Step [300/938], Loss: 0.0275\n",
      "Train O/P: Epoch [4/8], Step [360/938], Loss: 0.1467\n",
      "Train O/P: Epoch [4/8], Step [420/938], Loss: 0.0828\n",
      "Train O/P: Epoch [4/8], Step [480/938], Loss: 0.1252\n",
      "Train O/P: Epoch [4/8], Step [540/938], Loss: 0.0728\n",
      "Train O/P: Epoch [4/8], Step [600/938], Loss: 0.0583\n",
      "Train O/P: Epoch [4/8], Step [660/938], Loss: 0.0173\n",
      "Train O/P: Epoch [4/8], Step [720/938], Loss: 0.1772\n",
      "Train O/P: Epoch [4/8], Step [780/938], Loss: 0.0674\n",
      "Train O/P: Epoch [4/8], Step [840/938], Loss: 0.1363\n",
      "Train O/P: Epoch [4/8], Step [900/938], Loss: 0.0652\n",
      "Train O/P: Epoch [5/8], Step [60/938], Loss: 0.1438\n",
      "Train O/P: Epoch [5/8], Step [120/938], Loss: 0.0599\n",
      "Train O/P: Epoch [5/8], Step [180/938], Loss: 0.0448\n",
      "Train O/P: Epoch [5/8], Step [240/938], Loss: 0.0575\n",
      "Train O/P: Epoch [5/8], Step [300/938], Loss: 0.1724\n",
      "Train O/P: Epoch [5/8], Step [360/938], Loss: 0.0660\n",
      "Train O/P: Epoch [5/8], Step [420/938], Loss: 0.0450\n",
      "Train O/P: Epoch [5/8], Step [480/938], Loss: 0.0810\n",
      "Train O/P: Epoch [5/8], Step [540/938], Loss: 0.1094\n",
      "Train O/P: Epoch [5/8], Step [600/938], Loss: 0.1211\n",
      "Train O/P: Epoch [5/8], Step [660/938], Loss: 0.1590\n",
      "Train O/P: Epoch [5/8], Step [720/938], Loss: 0.0656\n",
      "Train O/P: Epoch [5/8], Step [780/938], Loss: 0.1333\n",
      "Train O/P: Epoch [5/8], Step [840/938], Loss: 0.0466\n",
      "Train O/P: Epoch [5/8], Step [900/938], Loss: 0.0537\n",
      "Train O/P: Epoch [6/8], Step [60/938], Loss: 0.0487\n",
      "Train O/P: Epoch [6/8], Step [120/938], Loss: 0.1566\n",
      "Train O/P: Epoch [6/8], Step [180/938], Loss: 0.0276\n",
      "Train O/P: Epoch [6/8], Step [240/938], Loss: 0.0150\n",
      "Train O/P: Epoch [6/8], Step [300/938], Loss: 0.0145\n",
      "Train O/P: Epoch [6/8], Step [360/938], Loss: 0.0799\n",
      "Train O/P: Epoch [6/8], Step [420/938], Loss: 0.0764\n",
      "Train O/P: Epoch [6/8], Step [480/938], Loss: 0.2413\n",
      "Train O/P: Epoch [6/8], Step [540/938], Loss: 0.1146\n",
      "Train O/P: Epoch [6/8], Step [600/938], Loss: 0.0469\n",
      "Train O/P: Epoch [6/8], Step [660/938], Loss: 0.0654\n",
      "Train O/P: Epoch [6/8], Step [720/938], Loss: 0.0649\n",
      "Train O/P: Epoch [6/8], Step [780/938], Loss: 0.1667\n",
      "Train O/P: Epoch [6/8], Step [840/938], Loss: 0.0125\n",
      "Train O/P: Epoch [6/8], Step [900/938], Loss: 0.0438\n",
      "Train O/P: Epoch [7/8], Step [60/938], Loss: 0.0811\n",
      "Train O/P: Epoch [7/8], Step [120/938], Loss: 0.0342\n",
      "Train O/P: Epoch [7/8], Step [180/938], Loss: 0.0339\n",
      "Train O/P: Epoch [7/8], Step [240/938], Loss: 0.0203\n",
      "Train O/P: Epoch [7/8], Step [300/938], Loss: 0.0829\n",
      "Train O/P: Epoch [7/8], Step [360/938], Loss: 0.0677\n",
      "Train O/P: Epoch [7/8], Step [420/938], Loss: 0.1171\n",
      "Train O/P: Epoch [7/8], Step [480/938], Loss: 0.0196\n",
      "Train O/P: Epoch [7/8], Step [540/938], Loss: 0.0864\n",
      "Train O/P: Epoch [7/8], Step [600/938], Loss: 0.0184\n",
      "Train O/P: Epoch [7/8], Step [660/938], Loss: 0.0365\n",
      "Train O/P: Epoch [7/8], Step [720/938], Loss: 0.0609\n",
      "Train O/P: Epoch [7/8], Step [780/938], Loss: 0.0337\n",
      "Train O/P: Epoch [7/8], Step [840/938], Loss: 0.0489\n",
      "Train O/P: Epoch [7/8], Step [900/938], Loss: 0.1366\n",
      "Train O/P: Epoch [8/8], Step [60/938], Loss: 0.0287\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [120/938], Loss: 0.1008\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [180/938], Loss: 0.0144\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [240/938], Loss: 0.0583\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [300/938], Loss: 0.0964\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [360/938], Loss: 0.0169\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [420/938], Loss: 0.0595\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [480/938], Loss: 0.0180\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [540/938], Loss: 0.0840\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [600/938], Loss: 0.0727\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [660/938], Loss: 0.0768\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [720/938], Loss: 0.0113\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [780/938], Loss: 0.0525\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [840/938], Loss: 0.0128\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [8/8], Step [900/938], Loss: 0.0096\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 8\n",
    "train_batch_size = 64\n",
    "B1_train_epoch,B1_train_losses,B1_train_acc = trainFunc(mBatch1,max_epochs,train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model with batch_size=1000 is:37160\n"
     ]
    }
   ],
   "source": [
    "# Training Model with batch size=1000\n",
    "torch.manual_seed(1)\n",
    "\n",
    "learning_rate = 0.0004\n",
    "mBatch2 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "weight_decay_val = 1e-4\n",
    "optimizer = torch.optim.Adam(mBatch2.parameters(), lr=learning_rate, weight_decay=weight_decay_val) \n",
    "\n",
    "a=[]\n",
    "for i in mBatch2.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model with batch_size={1000} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "Train O/P: Epoch [1/8], Step [60/60], Loss: 1.3874\n",
      "Train O/P: Epoch [2/8], Step [60/60], Loss: 0.4865\n",
      "Train O/P: Epoch [3/8], Step [60/60], Loss: 0.3587\n",
      "Train O/P: Epoch [4/8], Step [60/60], Loss: 0.2949\n",
      "Train O/P: Epoch [5/8], Step [60/60], Loss: 0.2689\n",
      "Train O/P: Epoch [6/8], Step [60/60], Loss: 0.2514\n",
      "Train O/P: Epoch [7/8], Step [60/60], Loss: 0.2530\n",
      "Train O/P: Epoch [8/8], Step [60/60], Loss: 0.2263\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 8\n",
    "train_batch_size = 1000\n",
    "B2_train_epoch,B2_train_losses,B2_train_acc = trainFunc(mBatch2,max_epochs,train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7504"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(B1_train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting Loss plot for both Batch 1 and Batch 2 models\n",
    "#B2_train_epoch,B2_train_losses,B2_train_acc\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(B1_train_epoch,B1_train_losses,color=\"blue\")\n",
    "plt.plot(B2_train_epoch,B2_train_losses,color=\"orange\")\n",
    "plt.title('Batch1 & Bathc2 Loss VS Epoch')\n",
    "plt.legend(['Batch 1','Batch 2'])\n",
    "plt.xlabel ('Epoch')\n",
    "plt.ylabel ('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001013577333651483 0.16086338460445404\n"
     ]
    }
   ],
   "source": [
    "print(np.min(B1_train_losses),np.min(B2_train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1525, -0.0533, -0.0366,  ...,  0.0089,  0.0374, -0.0364],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 37160\n"
     ]
    }
   ],
   "source": [
    "batch1_param = torch.nn.utils.parameters_to_vector(mBatch1.parameters())\n",
    "print(batch1_param,'\\nlen:',len(batch1_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1597, -0.0194,  0.0378,  ...,  0.0244,  0.0006, -0.0516],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 37160\n"
     ]
    }
   ],
   "source": [
    "batch2_param = torch.nn.utils.parameters_to_vector(mBatch2.parameters())\n",
    "print(batch2_param,'\\nlen:',len(batch2_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.         -1.86666667 -1.73333333 -1.6        -1.46666667 -1.33333333\n",
      " -1.2        -1.06666667 -0.93333333 -0.8        -0.66666667 -0.53333333\n",
      " -0.4        -0.26666667 -0.13333333  0.          0.13333333  0.26666667\n",
      "  0.4         0.53333333  0.66666667  0.8         0.93333333  1.06666667\n",
      "  1.2         1.33333333  1.46666667  1.6         1.73333333  1.86666667\n",
      "  2.        ]\n"
     ]
    }
   ],
   "source": [
    "alpha = np.linspace(-2.0, 2.0, num=31)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaArr =[]\n",
    "for i in range (len(alpha)):\n",
    "    theta = (1-alpha[i])*batch1_param + alpha[i]*batch2_param\n",
    "    thetaArr.append(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThetaModel(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(ThetaModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.conv2 = nn.Conv2d(3, 13, 5)\n",
    "        self.fc1 = nn.Linear(208, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # flatten as one dimension\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunction(model,loss_func,testLoader,test_batch_size): \n",
    "    test_loader = testLoader\n",
    "    test_load = test_loader(test_batch_size)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        testLoss = 0\n",
    "        count =0\n",
    "        for images, labels in test_load:\n",
    "            images, labels = Variable(images),Variable(labels)\n",
    "            \n",
    "            prediction = model(images)\n",
    "            testLoss += loss_func(prediction,labels).item()\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            count +=1\n",
    "    netTest_loss = testLoss/n_samples\n",
    "    netTest_acc1 = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the test images: {netTest_acc1} & Test Loss: {netTest_loss} %')\n",
    "    return netTest_acc1, netTest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model Theta 0 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 43.42 & Test Loss: 0.02575144319534302 %\n",
      "Accuracy of the network on the test images: 43.42 & Test Loss: 0.15130163221359252 %\n",
      "Total no of parameters in Model Theta 1 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 46.0 & Test Loss: 0.02074001007080078 %\n",
      "Accuracy of the network on the test images: 46.0 & Test Loss: 0.12183141679763794 %\n",
      "Total no of parameters in Model Theta 2 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 49.25 & Test Loss: 0.016485475158691407 %\n",
      "Accuracy of the network on the test images: 49.25 & Test Loss: 0.09681345415115357 %\n",
      "Total no of parameters in Model Theta 3 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 52.4 & Test Loss: 0.012902762174606324 %\n",
      "Accuracy of the network on the test images: 52.4 & Test Loss: 0.07575185809135437 %\n",
      "Total no of parameters in Model Theta 4 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 56.02 & Test Loss: 0.009906974792480469 %\n",
      "Accuracy of the network on the test images: 56.02 & Test Loss: 0.058144139981269834 %\n",
      "Total no of parameters in Model Theta 5 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 59.91 & Test Loss: 0.007425611066818237 %\n",
      "Accuracy of the network on the test images: 59.91 & Test Loss: 0.0435631281375885 %\n",
      "Total no of parameters in Model Theta 6 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 64.1 & Test Loss: 0.005399047899246216 %\n",
      "Accuracy of the network on the test images: 64.1 & Test Loss: 0.03165827225446701 %\n",
      "Total no of parameters in Model Theta 7 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 69.2 & Test Loss: 0.0037742439031600954 %\n",
      "Accuracy of the network on the test images: 69.2 & Test Loss: 0.02211525217294693 %\n",
      "Total no of parameters in Model Theta 8 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 74.29 & Test Loss: 0.0025039722084999085 %\n",
      "Accuracy of the network on the test images: 74.29 & Test Loss: 0.014655370777845383 %\n",
      "Total no of parameters in Model Theta 9 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 80.41 & Test Loss: 0.001562614107131958 %\n",
      "Accuracy of the network on the test images: 80.41 & Test Loss: 0.009128875041007995 %\n",
      "Total no of parameters in Model Theta 10 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 86.28 & Test Loss: 0.0009151305049657822 %\n",
      "Accuracy of the network on the test images: 86.28 & Test Loss: 0.005336089619994163 %\n",
      "Total no of parameters in Model Theta 11 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 91.39 & Test Loss: 0.0005090231463313103 %\n",
      "Accuracy of the network on the test images: 91.39 & Test Loss: 0.00296345152258873 %\n",
      "Total no of parameters in Model Theta 12 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 94.9 & Test Loss: 0.0002799354426562786 %\n",
      "Accuracy of the network on the test images: 94.9 & Test Loss: 0.0016290520193055272 %\n",
      "Total no of parameters in Model Theta 13 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 96.92 & Test Loss: 0.00016126415636390448 %\n",
      "Accuracy of the network on the test images: 96.92 & Test Loss: 0.0009397040236159228 %\n",
      "Total no of parameters in Model Theta 14 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 97.97 & Test Loss: 0.00010720267528668046 %\n",
      "Accuracy of the network on the test images: 97.97 & Test Loss: 0.0006266419643739937 %\n",
      "Total no of parameters in Model Theta 15 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 98.31 & Test Loss: 9.187543671578169e-05 %\n",
      "Accuracy of the network on the test images: 98.31 & Test Loss: 0.000537878477817867 %\n",
      "Total no of parameters in Model Theta 16 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 98.05 & Test Loss: 0.00010294156805612147 %\n",
      "Accuracy of the network on the test images: 98.05 & Test Loss: 0.0006000011769618141 %\n",
      "Total no of parameters in Model Theta 17 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 97.47 & Test Loss: 0.0001331430461257696 %\n",
      "Accuracy of the network on the test images: 97.47 & Test Loss: 0.000772641665226547 %\n",
      "Total no of parameters in Model Theta 18 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 96.77 & Test Loss: 0.00017442811569198967 %\n",
      "Accuracy of the network on the test images: 96.77 & Test Loss: 0.0010103641299298034 %\n",
      "Total no of parameters in Model Theta 19 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 95.92 & Test Loss: 0.00021648808643221856 %\n",
      "Accuracy of the network on the test images: 95.92 & Test Loss: 0.0012534942978061736 %\n",
      "Total no of parameters in Model Theta 20 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 95.38 & Test Loss: 0.0002498841088265181 %\n",
      "Accuracy of the network on the test images: 95.38 & Test Loss: 0.0014469099675305187 %\n",
      "Total no of parameters in Model Theta 21 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 94.91 & Test Loss: 0.00027357564643025397 %\n",
      "Accuracy of the network on the test images: 94.91 & Test Loss: 0.0015847922985441983 %\n",
      "Total no of parameters in Model Theta 22 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "Accuracy of the network on the test images: 94.66 & Test Loss: 0.0002946469521149993 %\n",
      "Accuracy of the network on the test images: 94.66 & Test Loss: 0.0017068315661977976 %\n",
      "Total no of parameters in Model Theta 23 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33844/272875475.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mtrain_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#T_train_epoch,T_train_losses,T_train_acc = testFunction(j,loss_func,train_batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mT_train_acc\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mT_train_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33844/3907955008.py\u001b[0m in \u001b[0;36mtestFunction\u001b[1;34m(model, loss_func, test_batch_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mtestLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_load\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelsTrainEpochArr = []\n",
    "modelsTrainLossArr = []\n",
    "modelsTrainAccArr = []\n",
    "modelsTestLossArr = []\n",
    "modelsTestAccArr = []\n",
    "\n",
    "for i in range (len(thetaArr)):\n",
    "    #torch.manual_seed(1)\n",
    "    \n",
    "    j=copy.deepcopy(i) \n",
    "    theta = (1-alpha[i])*batch1_param + alpha[i]*batch2_param\n",
    "    j = ThetaModel()\n",
    "    torch.nn.utils.vector_to_parameters(theta,j.parameters())\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(j.parameters(), lr=0.0004, weight_decay = 1e-4)\n",
    "\n",
    "    a=[]\n",
    "    for k in j.parameters():\n",
    "        a.append(torch.numel(k))\n",
    "    print(f'Total no of parameters in Model Theta {i} is:{np.sum(a)}')\n",
    "\n",
    "    print(j.parameters)\n",
    "\n",
    "    max_epochs = 1\n",
    "    train_batch_size = 600\n",
    "    T_train_epoch,T_train_losses,T_train_acc = testFunction(j,loss_func,train_batch_size)\n",
    "    #T_train_acc ,T_train_losses = testFunction(j,loss_func,train_batch_size)\n",
    "    \n",
    "    \n",
    "    #modelsTrainEpochArr.append(T_train_epoch)\n",
    "    modelsTrainLossArr.append(T_train_losses)\n",
    "    modelsTrainAccArr.append(T_train_acc)\n",
    "    \n",
    "    test_batch_size=100\n",
    "    T_acc, T_testLoss = testFunction(j,loss_func,test_batch_size)\n",
    "    modelsTestAccArr.append(T_acc)\n",
    "    modelsTestLossArr.append(T_testLoss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(modelsTrainAccArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanScore(dataArr):\n",
    "    meanModelData = []\n",
    "    for i in range (len(dataArr)):\n",
    "        meanScore = np.mean(dataArr[i])\n",
    "        meanModelData.append(meanScore)\n",
    "    return meanModelData\n",
    "\n",
    "def minScore(dataArr):\n",
    "    minModelScore = []\n",
    "    for i in range (len(dataArr)):\n",
    "        minScore = np.mean(dataArr[i])\n",
    "        minModelScore.append(minScore)\n",
    "    return minModelScore\n",
    "\n",
    "def maxScore(dataArr):\n",
    "    maxModelScore = []\n",
    "    for i in range (len(dataArr)):\n",
    "        maxScore = np.max(dataArr[i])\n",
    "        maxModelScore.append(maxScore)\n",
    "    return maxModelScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(alpha,minScore(modelsTrainLossArr),color=\"Blue\",linestyle='dashed', marker=\"o\")\n",
    "ax.plot(alpha,modelsTestLossArr,color=\"Blue\", marker=\"v\")\n",
    "ax.legend(['Train Loss','Test Loss'],loc=\"center left\")\n",
    "ax.set_xlabel(\"Alpha\",color=\"Green\")\n",
    "ax.set_ylabel(\"CrossEntropy Loss\",color = \"blue\")\n",
    "\n",
    "\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(alpha,meanScore(modelsTrainAccArr),color=\"red\",linestyle='dashed', marker=\"o\")\n",
    "ax2.plot(alpha,modelsTestAccArr,color=\"red\", marker=\"v\")\n",
    "ax2.set_xlabel(\"Alpha\",color=\"Green\")\n",
    "ax2.set_ylabel(\"Accuracy\",color = \"red\")\n",
    "ax2.legend(['Train Acc','Test Acc'],loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('D:/Clemson/COURSE/SEM-2/CPSC-8430 Deep Learning - 001/Homework/CPSC-8430-Deep-Learning-001/HW1/Diff Batch Graph HW1_3.1-2.jpg',\n",
    "            format='jpeg',\n",
    "            dpi=100,\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model with batch_size=600 is:37160\n"
     ]
    }
   ],
   "source": [
    "# Training Model with batch size=600 and Lr 1e-3\n",
    "torch.manual_seed(1)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "mLr1 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mLr1.parameters(), lr=learning_rate) \n",
    "\n",
    "a=[]\n",
    "for i in mLr1.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model with batch_size={600} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "Train O/P: Epoch [1/15], Step [60/100], Loss: 0.6050\n",
      "Train O/P: Epoch [2/15], Step [60/100], Loss: 0.3066\n",
      "Train O/P: Epoch [3/15], Step [60/100], Loss: 0.1868\n",
      "Train O/P: Epoch [4/15], Step [60/100], Loss: 0.1115\n",
      "Train O/P: Epoch [5/15], Step [60/100], Loss: 0.1018\n",
      "Train O/P: Epoch [6/15], Step [60/100], Loss: 0.1589\n",
      "Train O/P: Epoch [7/15], Step [60/100], Loss: 0.0668\n",
      "Train O/P: Epoch [8/15], Step [60/100], Loss: 0.0962\n",
      "Train O/P: Epoch [9/15], Step [60/100], Loss: 0.0827\n",
      "Train O/P: Epoch [10/15], Step [60/100], Loss: 0.0940\n",
      "Train O/P: Epoch [11/15], Step [60/100], Loss: 0.0860\n",
      "Train O/P: Epoch [12/15], Step [60/100], Loss: 0.0474\n",
      "Train O/P: Epoch [13/15], Step [60/100], Loss: 0.0756\n",
      "Train O/P: Epoch [14/15], Step [60/100], Loss: 0.0663\n",
      "Train O/P: Epoch [15/15], Step [60/100], Loss: 0.0371\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 15\n",
    "train_batch_size = 600\n",
    "L1_train_epoch,L1_train_losses,L1_train_acc = trainFunc(mLr1,max_epochs,train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1359, -0.0415,  0.0009,  ...,  0.0156,  0.0185, -0.0429],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 37160\n"
     ]
    }
   ],
   "source": [
    "Lr1_param = torch.nn.utils.parameters_to_vector(mLr1.parameters())\n",
    "print(Lr1_param,'\\nlen:',len(Lr1_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model with batch_size=600 is:37160\n"
     ]
    }
   ],
   "source": [
    "# Training Model with batch size=600 and Lr 1e-2\n",
    "torch.manual_seed(1)\n",
    "learning_rate = 1e-2\n",
    "mLr2 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mLr2.parameters(), lr=learning_rate) \n",
    "\n",
    "a=[]\n",
    "for i in mLr2.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model with batch_size={600} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "Train O/P: Epoch [1/15], Step [60/100], Loss: 0.1845\n",
      "Train O/P: Epoch [2/15], Step [60/100], Loss: 0.0812\n",
      "Train O/P: Epoch [3/15], Step [60/100], Loss: 0.0607\n",
      "Train O/P: Epoch [4/15], Step [60/100], Loss: 0.0380\n",
      "Train O/P: Epoch [5/15], Step [60/100], Loss: 0.0390\n",
      "Train O/P: Epoch [6/15], Step [60/100], Loss: 0.0608\n",
      "Train O/P: Epoch [7/15], Step [60/100], Loss: 0.0173\n",
      "Train O/P: Epoch [8/15], Step [60/100], Loss: 0.0344\n",
      "Train O/P: Epoch [9/15], Step [60/100], Loss: 0.0255\n",
      "Train O/P: Epoch [10/15], Step [60/100], Loss: 0.0150\n",
      "Train O/P: Epoch [11/15], Step [60/100], Loss: 0.0230\n",
      "Train O/P: Epoch [12/15], Step [60/100], Loss: 0.0162\n",
      "Train O/P: Epoch [13/15], Step [60/100], Loss: 0.0242\n",
      "Train O/P: Epoch [14/15], Step [60/100], Loss: 0.0359\n",
      "Train O/P: Epoch [15/15], Step [60/100], Loss: 0.0235\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 15\n",
    "train_batch_size = 600\n",
    "L2_train_epoch,L2_train_losses,L2_train_acc = trainFunc(mLr2,max_epochs,train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI8AAAHwCAYAAAAvuU+xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABAfElEQVR4nO3de5xdZX0v/s8zkzsQwi1cEiBYowgciRARvLW2UhEttIpaqrXeDlLvHm1rL8cYT+39oLVW/WnxiJbSSqUHWgHrpT29iFCEgCjQAoJEQBAJEELuz++PtYdMZmZN5pKdPZO836/Xeq211/W7d/ZM9nz28zyr1FoDAAAAACPp63UBAAAAAExdwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAOiqsrJ8tqwsv9vrOgCAiZnR6wIAgOmrrCx3JnlTXVG/OoZ9ZyX5qyTLkxyZ5AV1Rf3nHRzzmiS/m2T/JDcneXldUVePsv8Hkjy5rqivGUM9s5N8PMkLO+e/Lclv1RX1ipb9X5fmuT53R+feWcrKMifJfUleVlfUrw/Z9uEkh9cV9ayysjw3yR8lOTbJljSv1bvqivofI5zzA0l+O8mGQas31xV1QVeeBAAw7Wl5BAB0XVlZBr6w+rckr0kTiOzomL2T/J8k5yRZkORtSdbvxHpmJLk7yU8m2TfJ/0zyhbKyLNkZ19gZ6oq6PsnfJHnt4PVlZelPcnaSC8rKMj/JPyT5szQh2KIkK7N9ODTU39QVde9B04Ju1A8A7B60PAIAdrqysvxUkr9ME2i8O8lX6or6y0k+0tm+ZQynqUk2J/leXVG3JhnWimacNd2Z5BNJXp3kqUn2qivqBwbt8g9lZflekhOT3DnOcz87yZ8meUqS/0zyzrqifqOz7XVJ3p/koCQ/SvI7dUW9sKwsT05yfpJlSTYl+VpdUV81wukvSPLlsrK8pa6o6zrrXpTmS8ArOsenrqgXdbY9nuQfx1P/kOdSk7wzybuSzE8T4P1GXVG3lpWlL8lvJfnvSeYmuTLJ2+uK+nDn2IEWUMckeTTJ/6wr6mc7p96vrCxfSvL8JN9N8kt1Rb19onUCALuOlkcAQLcckqYlzJFpWg+N18Ykq9K0BtpvJ9V0dpKXJFlQV9TNgzeUleXgNOHPd8ZzwrKy7J/kS0k+muSAJOcl+VJZWQ4oK8tenfUvrivqPkmeneY5Jcn/ShPy7JdkcZqgbZhOCHVvkpcNWv3LSf6q8xz+M8mWsrJcUFaWF++k1+oX0nQvPCHJmUne0Fn/us70giRPSrJ3ko8lSVlZjkgTZv1ZmqBsWbY916R57Vemeb63JfnQTqgTANgFtDwCALpla5IVdUUdrfvUaP4syQ1J7kjy1bKyvLCuqA+VleVDSebUFfU9EzjnR+uKevfQlWVlmZnkwiQX1BX1lnGe8yVJ/quuqJ/vPL6orCzvSPJzSS5O8zocV1aW79cV9d40QVDStDY6MslhnXGc/m2Ua3wuTde1v+x0UzszyXOSpK6oj3Ra/PxGkk8nOaSsLJcn+e91Rf1hy/leWVaWlw56fH1dUV8w6PEf1hX1x0l+XFaWj6QJfv4iTaut8+qKekeSlJXlN5PcVFaW13e2fXVQC6gHO9OAS+qKek3nuAvThGwAwDSg5REA0C0PdMbsGbdOi503JvmjuqL+UZKvpAmQ9kvTemeHA3S3GCk46kvy+TQtnd42gXMeluSuIevuSrKorqiPJXlVknOT3FtWli+VleXozj6/nqQkuaasLN8pK8sb0u5zSV5QVpZFSc5KcltdUa8f2FhX1Jvrivq6uqIuTnJcp6aPjHK+L9QVdcGg6QVDtg9+ne7qnG+k53pXmi8jD05yeJLRuqENHudqXZpWSwDANCA8AgC6pU7i2L4k/WnGPEpdUd+X5Nok30wyL81YO5OuqawsJc24QwenuZPbpgmc8540LYgGOyLJD5KkrqhfrivqqUkOTXJLmtZBqSvqfXVF/e91RT0syZuTfLwzDtLwolfU7yf51zSte345TZg0ok7Lqc+mCZEm6vAhz+WezvLQ53pEmn+jH6YJnH5iEtcEAKYo3dYAgMma2bml/IDNbTuWlWV2mtY2STKrc9yGuqJuF+rUFfXRsrJcmSZQeWOSh5J8PU1rpKuSzEzTUmgkfUPqqaN0nftEkqcleWFdUR9vq3vwUxhy7iS5PMmflZXll5J8IcnL0wwY/Q+dcZSeleRraQayXptkS5KUleUVSa7qdFl7KE2wNdpA4hekGSfpkCS/9ERBTUuml6S5g9rqsrIcnqab2TfH8Hza/FpZWa5O0zrondnWxeyiJL9RVpYrkjyQ5Pc6193c6Yr2W2VleWWSS9Lcwe7wuqKumkQdAMAUoOURADBZl6cJRgamD4yy762dfRYl+XJneWirnQGvSdOi5YY0rVpeneZOaCXJZ0a5xtlD6hmxK1VZWY5M0+JnWZL7ysqytjO9epRzP3vIuR9P8nCSlyZ5T5oxfn49yUvrivqjNJ+13pOmxc6Pk/xkkrd0zvXMJFeXlWVtksvS3KHte6Nc+2/TDDb9tc7YSQMeTRNQXV1WlsfShEY3da7b5lWDnu/AtHDQ9kuTfCvNgNdfStM6K2le988n+Zck30uyPsnbkydaR53eue6PO8ceP0oNAMA0UWqdTItyAAB2J2VlqUmW1hX1tl7XAgBMDVoeAQAAANBKeAQAAABAK93WAAAAAGil5REAAAAArYRHAAAAALSa0esCxuvAAw+sS5Ys6XUZAAAAALuNb33rWz+qtR400rZpFx4tWbIk1157ba/LAAAAANhtlFLuatum2xoAAAAArYRHAAAAALQSHgEAAADQatqNeQQAAADsnjZt2pTVq1dn/fr1vS5ltzVnzpwsXrw4M2fOHPMxwiMAAABgSli9enX22WefLFmyJKWUXpez26m15sEHH8zq1atz1FFHjfk43dYAAACAKWH9+vU54IADBEddUkrJAQccMO6WXcIjAAAAYMoQHHXXRF5f4REAAABAx9577z1s3Qc+8IEsWrQoy5YtyzHHHJOLLrroiW0XX3xxjj322PT19eXaa69tPe8b3vCGLFy4MMcdd9yE6jrttNNy/PHH59hjj825556bLVu2DNvnlltuySmnnJLZs2fnT/7kTyZ0nZEIjwAAAAB24N3vfndWrVqVSy+9NG9+85uzadOmJMlxxx2XSy65JM9//vNHPf51r3tdrrzyyglf/wtf+EJuuOGG3HTTTXnggQdy8cUXD9tn//33z0c/+tG8973vnfB1RiI8AgAAABijpUuXZt68eXnooYeSJE972tPy1Kc+dYfHPf/5z8/+++8/bP3tt9+e0047LSeeeGKe97zn5ZZbbhnx+Pnz5ydJNm/enI0bN47Y/WzhwoV55jOfOa47qY2Fu60BAAAAU8673pWsWrVzz7lsWfKRj0zuHNddd12WLl2ahQsXjrrfPffckze96U25/PLLR93vnHPOySc/+cksXbo0V199dd7ylrfk61//+oj7vuhFL8o111yTF7/4xTnrrLMm/BzGS3gEAAAAsAMf/vCH8+lPfzp33HHHmLqfHXbYYTsMjtauXZtvfOMbecUrXvHEug0bNrTu/+Uvfznr16/Pq1/96nz961/PqaeeOvYnMAnCIwAAAGDKmWwLoZ3t3e9+d9773vfmkksuyWtf+9rcfvvtmTNnzqTOuXXr1ixYsCCrhjSx2rJlS0488cQkyRlnnJEPfvCDT2ybM2dOzjjjjFx66aW7LDwy5hEAAADAGL3sZS/L8uXLc8EFF0z6XPPnz89RRx31xODXtdbccMMN6e/vz6pVq7Jq1ap88IMfzNq1a3PvvfcmacY8uvzyy3P00UdP+vpjJTwCAAAA6Fi3bl0WL178xHTeeecN2+f9739/zjvvvGzdujV/93d/l8WLF+eqq67KS17ykrzoRS9K0ox5dPrppz9xzNlnn51TTjklt956axYvXpzzzz8/SXLhhRfm/PPPz/HHH59jjz02l1566bDrPfbYYznjjDPy9Kc/Pccff3wWLlyYc889d9h+99133xM1/+7v/m4WL16cRx55ZNKvSam1Tvoku9Ly5cvrtdde2+syAAAAgJ3s5ptvztOe9rRel7HbG+l1LqV8q9a6fKT9tTzqkUcefCSPPfxYr8sAAAAAGJXwqEd++Pnn5Nufem2vywAAAAAYlfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACgY++99x627gMf+EAWLVqUZcuW5ZhjjslFF130xLZf+7Vfy9FHH52nP/3p+YVf+IWsWbNmxPO+4Q1vyMKFC3PccceNu6Z169blJS95SY4++ugce+yxed/73jfuc0yG8AgAAABgB9797ndn1apVufTSS/PmN785mzZtSpKceuqpuemmm3LjjTfmKU95Sn7/939/xONf97rX5corr5zw9d/73vfmlltuyfXXX59///d/zxVXXDHhc42X8AgAAABgjJYuXZp58+bloYceSpL87M/+bGbMmJEkOfnkk7N69eoRj3v+85+f/ffff9j622+/PaeddlpOPPHEPO95z8stt9wybJ958+blBS94QZJk1qxZOeGEE1qv0w0zdtmVAAAAAMbqW+9KHlq1c8+537LkxI9M6hTXXXddli5dmoULFw7b9pnPfCavetWrkiT33HNP3vSmN+Xyyy8f9XznnHNOPvnJT2bp0qW5+uqr85a3vCVf//rXW/dfs2ZN/v7v/z7vfOc7J/U8xkN4BAAAALADH/7wh/PpT386d9xxx4jdzz70oQ9lxowZefWrX50kOeyww3YYHK1duzbf+MY38opXvOKJdRs2bGjdf/PmzTn77LPzjne8I0960pMm+EzGT3gEAAAATD2TbCG0s7373e/Oe9/73lxyySV57Wtfm9tvvz1z5sxJklxwwQX5h3/4h3zta19LKWXM59y6dWsWLFiQVatWbbd+y5YtOfHEE5MkZ5xxRj74wQ8maVopLV26NO9617t2ynMaK2MeAQAAAIzRy172sixfvjwXXHBBkuTKK6/MH/7hH+ayyy7LvHnzxnWu+fPn56ijjsrFF1+cJKm15oYbbkh/f39WrVqVVatWPREc/c7v/E4efvjhfOQjH9mpz2cshEcAAAAAHevWrcvixYufmM4777xh+7z//e/Peeedl61bt+Ztb3tbHn300Zx66qlZtmxZzj333CTNmEenn376E8ecffbZOeWUU3Lrrbdm8eLFOf/885MkF154Yc4///wcf/zxOfbYY3PppZcOu97q1avzoQ99KN/97ndzwgknZNmyZfmLv/iLLr0Cw+m2BgAAANCxdevWHe5z4okn5tZbb02S3HbbbSPuM3TMo4suumjE/Y466qgRx1AabPHixam17rCubtHyCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACmjF6O7bMnmMjrKzwCAAAApoQ5c+bkwQcfFCB1Sa01Dz74YObMmTOu49xtDQAAAJgSFi9enNWrV+eBBx7odSm7rTlz5mTx4sXjOkZ4BAAAAEwJM2fOzFFHHdXrMhhCtzUAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACAVsIjAAAAAFoJjwAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACAVsIjAAAAAFoJjwAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFZdC49KKYeXUv6plHJzKeU7pZR3jrBPKaV8tJRyWynlxlLKCd2qBwAAAIDxm9HFc29O8p5a63WllH2SfKuU8pVa63cH7fPiJEs707OSfKIzBwAAAGAK6FrLo1rrvbXW6zrLjya5OcmiIbudmeRztfHNJAtKKYd2qyYAAAAAxmeXjHlUSlmS5BlJrh6yaVGSuwc9Xp3hARMAAAAAPdL18KiUsneSLyZ5V631kaGbRzikjnCOc0op15ZSrn3ggQe6USYAAAAAI+hqeFRKmZkmOLqw1nrJCLusTnL4oMeLk9wzdKda66dqrctrrcsPOuig7hQLAAAAwDDdvNtaSXJ+kptrree17HZZktd27rp2cpKHa633dqsmAAAAAManm3dbe06SX07y7VLKqs6630pyRJLUWj+Z5PIkpye5Lcm6JK/vYj0AAAAAjFPXwqNa679l5DGNBu9Tk7y1WzUAAAAAMDm75G5rAAAAAExPwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACAVsIjAAAAAFoJjwAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACAVsIjAAAAAFoJjwAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACAVsIjAAAAAFoJjwAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACAVsIjAAAAAFoJjwAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACAVsIjAAAAAFoJjwAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACAVsIjAAAAAFoJjwAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFZdC49KKZ8ppdxfSrmpZftPlVIeLqWs6kzv71YtAAAAAEzMjC6e+7NJPpbkc6Ps86+11pd2sQYAAAAAJqFrLY9qrf+S5MfdOj8AAAAA3dfrMY9OKaXcUEq5opRybI9rAQAAAGCIbnZb25HrkhxZa11bSjk9yf9NsnSkHUsp5yQ5J0mOOOKIXVYgAAAAwJ6uZy2Paq2P1FrXdpYvTzKzlHJgy76fqrUur7UuP+igg3ZpnQAAAAB7sp6FR6WUQ0oppbN8UqeWB3tVDwAAAADDda3bWinloiQ/leTAUsrqJCuSzEySWusnk5yV5FdLKZuTPJ7kF2uttVv1AAAAADB+XQuPaq1n72D7x5J8rFvXBwAAAGDyen23NQAAAACmMOERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBIeAQAAANBKeAQAAABAK+ERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBIeAQAAANBKeAQAAABAK+ERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBIeAQAAANBKeAQAAABAK+ERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBpTeFRKeWcpZX5pnF9Kua6U8rPdLg4AAACA3hpry6M31FofSfKzSQ5K8vokf9C1qgAAAACYEsYaHpXO/PQk/6fWesOgdQAAAADspsYaHn2rlPKPacKjL5dS9kmytXtlAQAAADAVzBjjfm9MsizJHbXWdaWU/dN0XQMAAABgNzbWlkenJLm11rqmlPKaJL+T5OHulQUAAADAVDDW8OgTSdaVUo5P8utJ7kryua5VBQAAAMCUMNbwaHOttSY5M8mf1lr/NMk+3SsLAAAAgKlgrGMePVpK+c0kv5zkeaWU/iQzu1cWAAAAAFPBWFsevSrJhiRvqLXel2RRkj/uWlUAAAAATAljCo86gdGFSfYtpbw0yfpaqzGPAAAAAHZzYwqPSimvTHJNklckeWWSq0spZ3WzMAAAAAB6b6xjHv12kmfWWu9PklLKQUm+muRvu1UYAAAAAL031jGP+gaCo44Hx3EsAAAAANPUWFseXVlK+XKSizqPX5Xk8u6UBAAAAMBUMabwqNb6a6WUlyd5TpKS5FO11r/ramUAAAAA9NxYWx6l1vrFJF/sYi0AAAAATDGjhkellEeT1JE2Jam11vldqQoAAACAKWHU8KjWus+uKgQAAACAqccd0wAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACAVl0Lj0opnyml3F9KualleymlfLSUclsp5cZSygndqgUAAACAielmy6PPJjltlO0vTrK0M52T5BNdrAUAAACACehaeFRr/ZckPx5llzOTfK42vplkQSnl0G7VAwAAAMD49XLMo0VJ7h70eHVnHQAAAABTRC/DozLCujrijqWcU0q5tpRy7QMPPNDlsgAAAAAY0MvwaHWSwwc9XpzknpF2rLV+qta6vNa6/KCDDtolxQEAAADQ2/DosiSv7dx17eQkD9da7+1hPQAAAAAMMaNbJy6lXJTkp5IcWEpZnWRFkplJUmv9ZJLLk5ye5LYk65K8vlu1AAAAADAxXQuPaq1n72B7TfLWbl0fAAAAgMnrZbc1AAAAAKY44REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBIeAQAAANBKeAQAAABAK+ERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBIeAQAAANBKeAQAAABAK+ERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBIeAQAAANBKeAQAAABAK+ERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBIeAQAAANBKeAQAAABAK+ERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBIeAQAAANBKeAQAAABAK+ERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBIeAQAAANBKeAQAAABAK+ERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQKuuhkellNNKKbeWUm4rpbxvhO0/VUp5uJSyqjO9v5v1AAAAADA+M7p14lJKf5I/T3JqktVJ/qOUclmt9btDdv3XWutLu1UHAAAAABPXzZZHJyW5rdZ6R611Y5K/TnJmF68HAAAAwE7WzfBoUZK7Bz1e3Vk31CmllBtKKVeUUo7tYj0AAAAAjFPXuq0lKSOsq0MeX5fkyFrr2lLK6Un+b5Klw05UyjlJzkmSI444YieXCQAAAECbbrY8Wp3k8EGPFye5Z/AOtdZHaq1rO8uXJ5lZSjlw6IlqrZ+qtS6vtS4/6KCDulgyAAAAAIN1Mzz6jyRLSylHlVJmJfnFJJcN3qGUckgppXSWT+rU82AXawIAAABgHLrWba3WurmU8rYkX07Sn+QztdbvlFLO7Wz/ZJKzkvxqKWVzkseT/GKtdWjXNgAAAAB6pJtjHg10Rbt8yLpPDlr+WJKPdbMGpqfzz0/e9KZk9epk0UjDrAMAAAC7RDe7rcGEffGLzfyGG3pbBwAAAOzphEcAAAAAtBIeAQAAANBKeAQAAABAK+ERAAAAAK2ERwAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQakavC9hTrd+Q/ODe5IQTkhkztp9mzhy+bqpsb9vW35+U0utXFQAAANjZhEc9tnhxsnnz9tPjjw9fNzBt2tS+bcuW3j6X/v6dF1xdcUVvnwsAAADQEB712GWX7bxz1doeLI0lfOr29h0dOzg0G7Bo0c57fQAAAIDxEx7tRkppWu7MnNnrSibvwguT17wmmTev15UAAADAns2A2QAAAAC0Eh4BAAAA0Ep4BAAAAEAr4REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAAAAAtBIeAQAAANBqRq8L2FMdcnCy4PBeVwEAAAAwOi2PeuSgg5LDhUdMwsUXJzfe2OsqAAAA2N1peQTT1Ctf2cxr7W0dAAAA7N60PAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCVu63BNLJ6dXLVVc0EAAAAu4LwCKaoDRuS66/fFhZddVUTHiXJ7Nm9rQ0AAIA9h/AIpojBrYquuiq57rpk48Zm25FHJs99bnLKKcnJJyfLlgmQAAAA2DWER9ADGzY04dBVVyXf/ObwVkXLlyfveEcTFp1ySnLoob2td7rZsiW5+eZk3brkpJN6XQ0AAMD0JjyCXWC8rYpmzeppudNKrc3re801zXT11cm11yaPPZb09SUPPZTMn9/rKgEAAKYv4RHsZFoVddfDDzfh0NVXbwuM7r232TZrVhO+veENyf33J3/zN9tCOgAAACZGeASTpFVR92zcmNx44/atim65Zdv2pz41eeELk2c9q+me9vSnbxsL6mMfa8IjAAAAJkd4BOMwWquiOXOSE0/Uqmiiak1uv337FkXXX9+85kly8MFNSPSa1zRB0TOfmSxY0NOSAQAA9gjCIxjFWFsVnXJKcvzxWhWNxwMPbN+i6JprmvGJkmTevKZ739vfvq1V0eGHJ6X0tmYAAIA9kfAIOga3KhqYfvCDZptWRZOzbl3z2g4Oi+68s9nW15ccd1xy1llNSHTSSckxxyQz/HYCAACYEvx5xh5rR62Knvc8rYomYsuW5Oabt29R9O1vN+uT5rU96aTkbW9r5ieckOy1V29rBgAAoJ3wiD2CVkXdUWsTwg1uUXTttcljjzXbFyxoAqLf/M1trYoOPrinJQMAADBOwiN2S1oVdcfDDzfh0OBBre+9t9k2a1ZzN7nXv37bOEVPfnLTLQ0AAIDpS3jEtLejVkXLlyfvfGdy8slaFY3Hxo3JjTdu3/3sllu2bX/KU5IXvnBbi6Ljj09mz+5dvQAAAHSH8Ihp5+67m4Dom9/UqmhnqTW5/fbtWxRdf30TzCXJwoVNa6JXv7qZL1+e7Ldfb2tm8h5/vPn56O/vdSUAAMBUJjxiStuwYfsWRVoV7RwPPLD9OEXXXJM89FCzbd685nV9+9ubFkXPelZy+OFJKb2tmZ1v3rxkyZLke9/rdSVT0/33J//0T8mrXtXrSgAAoLeER0xpxx+fbN3aLGtVNDHr1jWtswaHRXfe2Wzr60uOOy55+cu3jVN0zDHJDL8Z9hgD7wWGO+mk5K67kpe9LJk5s9fVAABA7/gTkSnplFOSM85InvpUrYrGY8uW5Oabt29R9O1vN+uT5IgjmpDorW9t/jA+8cRkr716WzNMVffd18y3bBEeAQCwZxMeMSU96UnJpZf2uorp4Ytf3BYWXXtt8thjzfp9920Cove9rwmMnvnM5JBDelsrAAAA04/wiKlp7feSf3x28gs/SIp7vY/mrLOaVhHLliWvf/22u58tXdp0SwMAAIDJEB4xNV37jmT9fcm9X0kOe1Gvq5nSrr66Gf9p9uxeVzK1DNwp7rLLkv33b+4oV2szhtbg+WSWp8o5Jno+AACAsRAeMTXVLdvPaXXSSb2uYGq68MJm/sY39raO0fT1NXexK2Xiy5M5B0zW97/fdId18wIAgN2b8AjYLa1b18z/+q+Tpzxl5wUuO+McUyW8GajhkkuSRYuSww5rggCDQzMWmzc3d8E86aSmBSQAALsv4RGwWzv++OToo3tdxdT28pdvWy4lWbhwW5jUNj/ggKkRgNE7mzc38xtu6G0dU9lb35p8/OO6iQIA05/wCGAPd911yT33JD/4wfbzu+9uWpQ88MDwY2bNakKk0QKmRYuSvfba9c9nZxkYN2ugRda++yYLFjTTWJfnzROy7ck+/vFeVzD1veMdyW/8RvP7AgCYuoRHME1df33zBy1M1jOe0UxtNmxI7rtveLg0sHzjjckVVyRr1w4/dv784aHS0IBpqneVO+20ZNOmZM2a5MEHk9tvb5bXrGnWj2bGjPEHToOX589310R2X9/7XvJnf9aE1Lo+juyWW5I77khOP73XlTBd/cqvNP9PX399rysBpjvhEUxTy5b1ugL2FLNnN2PbHHnk6Ps9+ujwgGnw/J//uVke6O40YKCr3I5aMfWqq9ynPpXsvffw9bUm69c3IdLDD28LlHa0fO+925Yfe2z0a5fSBEgTCZ4G5gazZqp6/PFm/uijva1jKnva05q5ro/tfvzj5q6qjOxzn+t1BcDuQngE09UP/znZ68hk76N6XQkkSfbZpxlfarQxprZuTX70o/aAafXq0bvKHXrojsdjGino6YZSkrlzm+nQQyd2jk2bkkceGXvwtGZN053w29/etn5Hf1TOnbstVJpIC6i5c3W9A6amW25pAraVK5P3v7/X1QDs3oRHMF197QXN/Jd8Hcn00dfXtDJauHD01nMbNzYtdEYKme65pwlPvvzlkVsszJ8/eje5ww5rwp6p0FVu5symRdUBB0zs+K1bm+6C42n9NND17uGHk4cemnjXu9mzm+0DY0MB7Gq33dbMr7mmt3UA7AmERwBMObNmjb2rXFsrpnvuSf7f/2tCqKEBSSnJQQe1t1467LDuPbedqa+vCcvmz5/Y8QNd78bT7W7NmmYMrPvu23aeY49tWnzts08zH7w81nV7753090/u9QCA8fr3f0+e85xeV8F0deONzd2d77xzx59bpzvhEbBbOnj+fXntKz+a/v7f63UpU1a9sORfb3lukn/tdSkTts8+yVOf2kxtBneVawua/uM/kvvv33V1TxWDu96NdwD+9eub45LkmGOaFlCPPtq81o8+2jxeuzZZt27s55w7d/yB02jb3e1uerj55mZclhkzmgBxYBr8eLzLY9nPewO44opmQPqPfjR5+9t7XQ3T0Wc/28y/+MXkf/yPnpbSdcIjYLd0+fvOzF7rr0kOOzfJEb0uZ8p63tH/1usSum48XeUG31XurLOa9TP8Tzmq2bOTiy9u375lSzMw+OBAaWB5LOvWrGnGwhq8fUdd7QaUsnNCqMHrBrrrjceTn9y0pps9e/h8pHXd2meqvZfXrNm2/Cu/suuvX8quD6zGe/yA++5rwtV586ZGl1vYHTz6aPLVrzbLt97a21qmsoGg3cD9TLGPEUCrrZuSx+9LHr83efyeXlcz5e016+FkfZLN42j2wB5t1qzkiCOaKUmedMjqnLX8wvTddWiy35JkryXJ3EVJn75V49HfP7mudSPZuHH8IdTg+X33Dd8+1g/FM2eOPXAacPLJzdhQGzc28w0bmjuNrVmzbd3Q+YYNYw/Jxqqvb8cB064MtQa39rv99uZOjFu2NNNYlse63646fuPGnXPNrVuH/9sNHpS/v78JkQbCpIHWg0PX7Yxts2ZpoTUdbdmy7WYM//W/n5wnH3J7kj3vL/+1a5uuRAPT9763/eMf/3jbvjvz/yjYXQmPoNe2bEzW39cEQgPB0HbzzvKGEW4/BXTNL53y+fyvs34r+daglWVGMu/wZO8lTZg0MA08nntY0ue/1m6bNau5NffOuj13rU2YM9HWUWvXNncIHLzu8ceTfeetyaL9fpC//MtjJ1zXxo2jB0xt2yazz+OPN2NcjbbPxo0757Uf8KQn7dzzTWe1bguU5sxp1n38482/y7p128+Hrnv00eSHPxy+7fHHJ1ZLX9/OD6Ta1s2eLagaUOu2lpdr1jQ3NxhYHjy1rX/44UHnuvD2XVv8LrR2bXLXXe0B0YMPbr//nDnJkiXJUUclz3pWs3z//cn//t/Jc5+7i4uHacgn3F5Ze0fyyHeTu/9v8/iJ/y2Hznfitu3+R27bNsI+u2rb4HWP3Zlpb8uGJhRad0+y/t5t86Hh0IYfDT+29CdzDm7+EN3riOTAk5O5hzaPB+ZXnrjrnxPsQfr7tiRJNrzwu5m9ZXXze+mxO5O1nfm9Xx7eCnAPC5d+8LHD8vbPfzLJGb0uZVJKaf54nTcvOfjgnXPOLVuSxy9YnL3nPJaJfuNfyrbWO4NbMk0FtTYtoyYTVK1alXzqUzvvNd9dDHSnG9xt7Vd/dXLnHBgcvy10Gi2Qatv22GPNGGcjbZtI95bBY7CNNYi64YbJvS7dUmvzWow3+BlY9/DDI7dAG2z+/G13v1ywoBmod9my7dctWNCFJ7cLrVu3fTA0NCT60ZCP0LNnN4HQkiXJM5+5bXlgWrhweEB5zTVNeATs2O7zCXa62dLpSvOvv9DbOqa6GXN7XcFwW9Z3uo+1tRTqhEQbHhx+bOlP5hzSCYWOSg58zvBQaO6hyeyDxt415sunJKVv25TBy2X4ugnt09f537Zl+7j2GXTNnVJbyz6P6Lw+oi0bkk2PNBOjGhgMusxfmsx52sg7bVmfPHb3tmBph+FSfxMuDQ6UBgdMcxdNn3Bpy/octt+9+ctzX5Vkgs0admP9/ekER7unUpoWYLNmTfwc3/1uEx7trBZku6OTfuLqHLf4piRvnNR5Bgcz3Vbrtq6ZOyOkGpg/9NDwbWvXNtfsxh2OBrqWjrW1z9D1mzePfv699toW8Oy3X3OXz2OO2T742W+/4WHQggXJvvuO4+6UfzX257yrPf5403JoaHeygWnojSxmz27+rZcsSZ7xjG2tiAaHQ319u/QpwB5lmnxC3Y29+PpBX88MnWf4tu2+ytkF20aqYVfUd+eFyfe/kMxdnF1m8+OdlkH3todCj9+bbPzx8GPLjE74c2iyz5OThc9L5hyazDts+/mcgzpBx040c5+kbk1SO/OtydbNzfLA4+2W6wjrJrLPoGsOu84U6le/eW2vK5i8WpvAeSD0Geu0eYR1W3dyX5Pd2JvfnOT7O/jjuH9OMn9pM41ky4bkse9vHy49dlcnXPpKJ1wa9PMyDcOlUqbQzzvTzqrfOz7vv/Ivk/y3XpcyJV39wZM7S5MLj3alUpruQXPmNOFHN916a3L00SPfZn3jxvG19hk6bdgw+rXnzt0+0DnooGTp0vbAZ/D6fffdMwY+f/zx5Pvfbx9z6Ic/3H7/WbO2hUNnnrktFBoIiA4+WDjUSxdf3LSIq7WZD13e0ePdddu11zavz9BukrujqfUJdE/ySz5sj2rTo014tDNsXjd6IDTQpWzTmuHH9s1sQp+5hyb7PCVZ+JPDWwnNPSyZfcDOD4XG6qf/sTfXHU2t2T5YqiMETEMDqbHsM45g66vPb2rZsr4HL0DH1i3J5kfHGfSMsP/mRzrPbQf65yQz5ycz5jfzmfOTeUdsWx46XfXa7r8G09iCBUm+P8mT9M/ecbi07u7tWywNTFM8XBr41tsYJUzUrPX/mWOOvDF/9POvTnJjr8thGupb973UC5+Ut1/w0fz5n799u0BoR+M8zZy5faCz335NQDFa6DM4/BkYj2pPtn799uHQ0IDovvu233/mzG3h0M/93PZdyo46KjnkkN6EQ2eceGlSz8j2Q20w4MY/+G/p79uSY1/53V1yvVKaqa+vmQYvD328K7fNmDHytgH/+Z+75OXpKeER09fmx3bcSujxe5JNDw8/tm9WE/zMOTSZf3Sy8AXbWgdtFwrt37tQaDorJdu6k01DWza2t9wZT2ufzWPssjJj7+HBztxDhgdBo00z9kn6x9l/ZCA8uvSoJgCddUAzH2159oFNvRKDnaN/dtNacZ8nj7x9CodLA9+aT6bb0h5j7Z2dn529/Oxsp+nXM6NvB/17oMVDd34nSfKip385t6x6ew47bMddvgbWz5njx3FHNmwYHg4NDonuvXf7/WfMaO5YumRJ8pKXDB9z6NBDx9HdbhdZ8NhlufR//Hxu3vDHSd6bZPtB60e7W+LOfrwrrzWex/+18qYkyU037byApm3fgeBoOnn602uevfD/y0N95/a6lK4THjG1/eDvk3tKSyg0wpgtfbO2hT/7HpMc8jMjtxSatf/0+83ExNzzpWaA+vGEQVt30FY9af5IHxrgzD4w2ftJYwt7ngh99u59yHbQc5sxujb8KFl7W7M8Uug6oG/m2IKm7Zb3n3LdraaFbodL847cPmSat3jc/05+k47BZUc1877Z20LY2QcOWT6w8zNzYDJn0LLACXZo4cLkK1/pdRXTz8aNyd13t485dM89248s0d+/LRw67bTtxxtasqQZt2mqhUM7sv7Bponx1y67M8f/YhOYTGTA911pYFD9/v5t0+DHo20bad+ZM0ffd8CxE7tx6G7vQ6/+k/zc4b+eL63ZN8nZvS6nq3ySZ2qauXczv/49zbx/zrbuY/selxxyaksotJ8P2Wzvu3+w/eO+2cMDnHmLRwh1dhD69M/dfd5rz/788HVbNzfje214sJk2Pti+/Oh/Jj/qLG/d1H6dmfuOo4VT57E/nEfXlXBp8ch3iptguLRHe/I5yW2fSp71mSacHZg2dsLaNTdu+1lqGytuWOA0JHSaxoHT/M4d5GbsAWO/0B2LD9uU3J6ctOhLyVWvS2bMS/rnTXA+p/df5nTRZz4zPCT6wQ+Gh0OHH94EQaeeOnxA6sMO2z5M2B089FCSvZrn9Z73jC1sGWswM9lQp23bLv/1PoUHXZ8K9pnZNMHbq+/eHew5/e1mP/7sNg57SXLqN5JZ+zbB0MwF0+KDMFPI3j+RrL09efZFyYEndcKgfZo/ttmxvhnJnIXNNFa1NgOUDw2YRgyfftTcEW/jg6Pf+a1vdtN9dOCP4rG0dpq139jvVri722G4tHFbuDQ0YPrh15J1P0hruDT3sC4XvxuYsXcz/cTrR99v65Zm3L2BFoAbfjRkeXDgdMMYA6ehLZumXuC0sPPr5fDDd+ll2Y0cNv972x788J+am0tsXrftrsbj1T937IHTePadMWi5b1ZPPtO+8Y1Nt6CBcOhnfmb4mEOLFk2zcKjWprX4prXNGJOb1zZjR4403/zoiOt+cq9/T5I89xnfy7lv7fHzgSluOv16YE/SNyM56JReV8F0NtA6Yr9lTVcyuq+UJqCbuU+SJWM/bsvGba2cRmvhtOHB5OHvdpZ/nNS2cVJKMmvBjls1DV2eMW/yr8F00z8r2ecnmmkko4ZLX2/22boh+cLeSQZGjuxrvr1/YnnQuifGQuvMx7ouLefr5jVGu+5Yr3HLeWP7d+jr3/Z+zFPGdkwvA6eBxzspcOrz3RA7w8/ftW251uaGGYPDpInMN69LtjzeBA3r7x++fSzd3IcqfZNoHTXW+dxhrUTvuCNZvLjHd3mrW5vxIFtDntGCn5aAqPWzwBClrxkfcuY+nWB/n209HZIcMvfmLj1p2H0Ij2C6et4XmzvAwXTXP6sZIHzuIWM/ptamxdIOWzg9mDx+X/Lwd5rlzWtHqWPO9qHSD/9p8s9tuhstXNqyPvmbuc3yk88d4Y6Ig+6M+MTdFEe6C+Mo6wbOM9K6Yefe2nS3HO+5t7vGDuof97m7aDoETgMhUxcCpz3OV3+qGW+ub1ZnGmG5zGx+Zgcvl862wcsjnmcc66ZD165Skhlzm2n2Ad27ztYtTbg0oVBqhPmGB5vAfrv1j43tjqtD9c1qwqSOo46ayPPbNLZwZ7SQZ/C2sd5IJGn+T34i5OkEPrP2a8btGxoAzeg8ntkyn7FPp1vi8N89m/7ltZm5+vPZb5+1zeeK0p/mi4L+zhcBvegnNkX9+PrO69HfhJMDy2Oe+nbL13LgxiFz9oDODV0Nj0oppyX50yT9Sf6i1voHQ7aXzvbTk6xL8rpa63XdrAl2G4e/rNcVTG3L/jj5lzOSfVpukc70VkrTrXXWvuNrWbZlwwhjOf1o5PBpQN0c37WMom92csKf9LqKqemvptiH5KkWOA202ti6qQkjt25O6pbmZ65u6TwevDxoW93c1FZH2Wfw49G2jXWf0erZ7vEE6tnu+QzaZ7DNjyVb1yRbN3amTUPmg5bbXv+dofSPL3Da6QHWoOWHVnXveY5FX3/St/d2LVh2ulo7PyNtodMI4dXg5f/8aHOe2z8zcsgzUgA08HjrxjEWWUYObeYuagY3awt0WoOfvZp/411g5qzmOv2bHkgu3neUpzgoTBoWLvXtYNuOjp3o9m6ee/D6jitPmPwLXvqSMo7gqW/oujEcO+yYHRzbuv8Ix46w7ymH/lWyITnpWVN8pPWdoGufhksp/Un+PMmpSVYn+Y9SymW11u8O2u3FSZZ2pmcl+URnDjA5i38u+aXd/5f4pBz/e8mhp/W6il2rf3ZngP1Dd7zvX8+ZWJcE2J10I3AaCJt+fH2z/9rbtrVimwoG/7HwxLfrA384jPZ4yLr+uaMc03K+MqPzx0nn8Xc+1NT0wn8e33PYumV4oFQ3NV1RBy/Xzra25ZGCqXGvWzd6DQPLo91wYU9WShO89c9qumSP10B4dPUbt63rm7V9iDMQ2sw9tLN+HC16Zu7dtHCari1KFjx92/Iz/qTTonRLmtasnXndsm390PnO3L51U8v2nXjtyQTLz7ukc55Rpq0jrd88jn1HObZt/60b248d0zWGTOM08M7vW3PDxF/baaKbX6WelOS2WusdSVJK+eskZyYZHB6dmeRztdaa5JullAWllENrrbv/UOUAvXbsb/a6gqntmZ9Irn5D01KC4QZeF++jdnMWNuOk7GnGGjitvz+55OBm+fjf33HQMjRYGSmAGU/YM1JwM9W6VQyER+PV15/0zU0yhUK5Ham188feOIKpR25Jrn1b8ow/7nX1U9+Zd20bxL9/Vq+rmToGWtYsfWvytPf0tpZdYaCb9njCp0uXNMce/gs9LHwXqgPd4McYNn3jNcmDV4/ti8lprpvh0aIkdw96vDrDWxWNtM+iJMIjAHrrJ16/47tk7clK0bpvR868a/S7Ce7p5ixMzrwzmbs47pDY4mUPNC219gSlpOnWNjPJXmM75pCfSZ7iFlmjOuXzzThKex3R60qmpiNekXzr7clT3tLrSnaNUjqB2Th+5z713ZlUi6XppvSNL2D96a8kVzwjefr/6l5NU0Q3w6ORvrYZ+q4byz4ppZyT5JwkOeIIv/gAgGmgf04z0W6vI3tdwdQ2p3OHO5ioo17T6wqmtrkH+yJkR04c451D91Qz90nOuK3XVewS3bxdwuokhw96vDjJPRPYJ7XWT9Val9dalx900EE7vVAAAAAARtbN8Og/kiwtpRxVSpmV5BeTXDZkn8uSvLY0Tk7ysPGOAAAAAKaOrnVbq7VuLqW8LcmX03Sq/Eyt9TullHM72z+Z5PIkpye5Lcm6JAaXAAAAAJhCujnmUWqtl6cJiAav++Sg5ZrEKHcAAAAAU1Q3u60BAAAAMM0JjwAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACglfAIAAAAgFbCIwAAAABaCY8AAAAAaCU8AgAAAKCV8AgAAACAVsIjAAAAAFoJjwAAAABoVWqtva5hXEopDyS5q9d1sEscmORHvS6Cac17iMnyHmKyvIeYLO8hJst7iMnyHtpzHFlrPWikDdMuPGLPUUq5tta6vNd1MH15DzFZ3kNMlvcQk+U9xGR5DzFZ3kMkuq0BAAAAMArhEQAAAACthEdMZZ/qdQFMe95DTJb3EJPlPcRkeQ8xWd5DTJb3EMY8AgAAAKCdlkcAAAAAtBIeMeWUUg4vpfxTKeXmUsp3Sinv7HVNTE+llP5SyvWllH/odS1MP6WUBaWUvy2l3NL5fXRKr2tieimlvLvz/9hNpZSLSilzel0TU1sp5TOllPtLKTcNWrd/KeUrpZT/6sz362WNTG0t76E/7vxfdmMp5e9KKQt6WCJT3EjvoUHb3ltKqaWUA3tRG70lPGIq2pzkPbXWpyU5OclbSynH9Lgmpqd3Jrm510Uwbf1pkitrrUcnOT7eS4xDKWVRknckWV5rPS5Jf5Jf7G1VTAOfTXLakHXvS/K1WuvSJF/rPIY2n83w99BXkhxXa316kv9M8pu7uiimlc9m+HsopZTDk5ya5Pu7uiCmBuERU06t9d5a63Wd5UfT/MG2qLdVMd2UUhYneUmSv+h1LUw/pZT5SZ6f5PwkqbVurLWu6WlRTEczkswtpcxIMi/JPT2uhymu1vovSX48ZPWZSS7oLF+Q5Od3ZU1MLyO9h2qt/1hr3dx5+M0ki3d5YUwbLb+HkuTDSX49iUGT91DCI6a0UsqSJM9IcnWPS2H6+Uia/+C29rgOpqcnJXkgyf/pdH38i1LKXr0uiumj1vqDJH+S5hvae5M8XGv9x95WxTR1cK313qT5gi3Jwh7Xw/T2hiRX9LoIppdSyhlJflBrvaHXtdA7wiOmrFLK3km+mORdtdZHel0P00cp5aVJ7q+1fqvXtTBtzUhyQpJP1FqfkeSx6CrCOHTGpTkzyVFJDkuyVynlNb2tCtiTlVJ+O83wEBf2uhamj1LKvCS/neT9va6F3hIeMSWVUmamCY4urLVe0ut6mHaek+SMUsqdSf46yU+XUv6ytyUxzaxOsrrWOtDq8W/ThEkwVi9M8r1a6wO11k1JLkny7B7XxPT0w1LKoUnSmd/f43qYhkopv5LkpUleXWvV7Yjx+Ik0X4Tc0PlsvTjJdaWUQ3paFbuc8Igpp5RS0owzcnOt9bxe18P0U2v9zVrr4lrrkjQD1H691uobf8as1npfkrtLKU/trPqZJN/tYUlMP99PcnIpZV7n/7WfiUHXmZjLkvxKZ/lXklzaw1qYhkoppyX5jSRn1FrX9boeppda67drrQtrrUs6n61XJzmh81mJPYjwiKnoOUl+OU1rkVWd6fReFwXscd6e5MJSyo1JliX5vd6Ww3TSabX2t0muS/LtNJ+5PtXTopjySikXJbkqyVNLKatLKW9M8gdJTi2l/FeaOx39QS9rZGpreQ99LMk+Sb7S+Vz9yZ4WyZTW8h6CFK0WAQAAAGij5REAAAAArYRHAAAAALQSHgEAAADQSngEAAAAQCvhEQAAAACthEcAADtQStnSucX1wPS+nXjuJaWUm3bW+QAAdrYZvS4AAGAaeLzWuqzXRQAA9IKWRwAAE1RKubOU8oellGs605M7648spXytlHJjZ35EZ/3BpZS/K6Xc0Jme3TlVfynl06WU75RS/rGUMrdnTwoAYAjhEQDAjs0d0m3tVYO2PVJrPSnJx5J8pLPuY0k+V2t9epILk3y0s/6jSf5frfX4JCck+U5n/dIkf15rPTbJmiQv7+qzAQAYh1Jr7XUNAABTWillba117xHW35nkp2utd5RSZia5r9Z6QCnlR0kOrbVu6qy/t9Z6YCnlgSSLa60bBp1jSZKv1FqXdh7/RpKZtdbf3QVPDQBgh7Q8AgCYnNqy3LbPSDYMWt4S41ICAFOI8AgAYHJeNWh+VWf5G0l+sbP86iT/1ln+WpJfTZJSSn8pZf6uKhIAYKJ8qwUAsGNzSymrBj2+stb6vs7y7FLK1Wm+lDu7s+4dST5TSvm1JA8keX1n/TuTfKqU8sY0LYx+Ncm93S4eAGAyjHkEADBBnTGPltdaf9TrWgAAukW3NQAAAABaaXkEAAAAQCstjwAAAABoJTwCAAAAoJXwCAAAAIBWwiMAAAAAWgmPAAAAAGglPAIAAACg1f8Pibnud9TGAasAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting Loss plot for both Batch 1 and Batch 2 models\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(L1_train_epoch,L1_train_losses,color=\"blue\")\n",
    "plt.plot(L2_train_epoch,L2_train_losses,color=\"orange\")\n",
    "plt.title('Lr1 & Lr2 Loss VS Epoch',color=\"green\")\n",
    "plt.legend(['LR1:1e-3 1','LR2:1e-2'])\n",
    "plt.xlabel ('Epoch')\n",
    "plt.ylabel ('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1326, -0.0430, -0.0602,  ...,  0.0240,  0.1268, -0.0453],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 37160\n"
     ]
    }
   ],
   "source": [
    "Lr2_param = torch.nn.utils.parameters_to_vector(mLr2.parameters())\n",
    "print(Lr2_param,'\\nlen:',len(Lr2_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model Theta 0 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 1089.3296\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 360.0997\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 182.9377\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 166.1089\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 119.5325\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 86.4572\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 69.0773\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 54.4664\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 69.8954\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 60.5991\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 48.8338\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 43.3977\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 45.8358\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 29.0469\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 46.3312\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 22.2333\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 44.8665\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 33.0703\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 28.7660\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 20.9117\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 39.6131\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 31.5458\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 14.1364\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 16.0391\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 28.4555\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 19.8543\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 28.1350\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 24.1907\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 18.2828\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 26.6634\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 17.2491\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 17.0274\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 16.4037\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 28.4373\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 15.0682\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 14.5586\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 27.6834\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 16.5779\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 16.6280\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 4.9800\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 12.9221\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 9.7653\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 11.9477\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 14.9816\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 11.0866\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 13.1428\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 15.5901\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 10.6051\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 7.4695\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 17.4399\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 16.7509\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 5.3685\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 7.2633\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 19.4822\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 4.3033\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 6.8868\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 12.5623\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 6.9692\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 13.8561\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 8.1938\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 5.0491\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 6.0175\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 8.4773\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 5.9848\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 11.2200\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 5.1031\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 4.4292\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 8.3596\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 9.3248\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 5.7597\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 3.4229\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 7.5260\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 3.3633\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 6.4584\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 3.9512\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 4.9078\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 3.6700\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 2.4697\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 3.7755\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 8.7145\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 2.4406\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 1.1974\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 3.8614\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 2.5414\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 6.5651\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 6.2479\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 5.4087\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 3.1437\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.1693\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 4.5420\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 4.3538\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 2.0842\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.8512\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 1.2197\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 1.9980\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 3.4967\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 3.1043\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 1.5395\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 2.4832\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 3.7816\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 93.08 & Test Loss: 0.02929633630964007 %\n",
      "Total no of parameters in Model Theta 1 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 652.3210\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 252.0910\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 131.5886\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 117.8835\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 81.5855\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 55.3851\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 47.3532\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 35.3239\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 43.5276\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 33.9335\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 33.8507\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 29.8721\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 30.8254\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 20.9391\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 31.6675\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 15.7414\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 32.2972\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 23.3363\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 23.4258\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 16.8152\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 25.4677\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 20.5961\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 9.4863\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 11.6183\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 19.6448\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 13.5454\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 18.2049\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 20.3878\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 10.7965\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 15.2561\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 9.8172\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 11.3054\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 12.5251\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 24.3145\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 7.3573\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 10.1028\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 18.2878\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 15.3419\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 8.4465\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 2.3126\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 6.6686\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 4.5090\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 12.7424\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 10.9380\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 5.9147\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 10.4999\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 7.3577\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 9.7839\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 4.5556\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 13.4544\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 9.5705\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 6.6564\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 3.6440\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 5.8983\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 4.7153\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 2.8598\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 6.7161\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 5.9026\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 8.0618\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 4.3313\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 3.6020\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 4.6951\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 7.3618\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 2.5585\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 7.8445\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 2.6718\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 1.9379\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 5.7181\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 6.4419\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 3.1330\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 3.0835\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 4.1731\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 3.1225\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 3.8104\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 3.4439\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 4.4033\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 1.4504\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 1.0375\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 2.5121\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 6.3774\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 1.4523\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 1.6803\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 3.0396\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 1.4513\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 4.4220\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 3.4541\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 3.6729\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 2.0813\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.3914\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 2.1464\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 2.3331\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 1.5298\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.3088\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.5331\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 1.7092\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 1.3005\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 1.8120\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 1.4187\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 1.3014\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 1.6421\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 93.03 & Test Loss: 0.019554220682488632 %\n",
      "Total no of parameters in Model Theta 2 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 387.2997\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 182.1269\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 92.0074\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 82.4998\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 56.5242\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 38.4987\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 35.7675\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 26.6514\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 30.6217\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 24.5212\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 24.0436\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 20.8109\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 18.9193\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 12.5962\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 25.6044\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 10.2787\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 21.9626\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 16.8233\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 12.2166\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 9.7830\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 15.2887\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 10.9248\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 5.9451\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 7.5694\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 12.0303\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 8.2068\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 12.6185\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 12.0099\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 5.4015\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 9.4652\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 6.8857\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 8.1040\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 6.4514\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 15.5725\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 6.6550\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 8.6327\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 12.1188\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 13.7445\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 6.0565\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.8769\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 4.4425\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 2.1788\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 7.3837\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 4.1251\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 3.1999\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 7.4967\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 7.9621\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 6.1792\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 3.4978\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 8.1239\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 7.3049\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 2.0070\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 1.4066\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 4.2259\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 3.1069\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 2.9781\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 5.1039\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 3.8224\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 6.9356\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 2.6703\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 2.5054\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 2.5872\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 4.2612\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 2.1879\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 6.9221\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 1.9920\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 2.2437\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 4.2244\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 4.1102\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 2.9117\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 1.9780\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 4.2045\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 3.0080\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 3.8839\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 1.6680\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 2.8140\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 1.6561\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.4750\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 1.7953\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 3.5394\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 2.0920\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 1.1802\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 2.0788\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.9647\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 4.2672\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 1.6609\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 2.1977\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.9349\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.1849\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 1.0915\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 2.2408\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.9010\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0741\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.4049\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.9868\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.8032\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.9123\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.8124\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.7000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.4789\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 94.01 & Test Loss: 0.011505444888825263 %\n",
      "Total no of parameters in Model Theta 3 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 307.6625\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 118.7286\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 62.8539\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 49.7364\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 37.4870\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 24.6563\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 23.8835\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 17.3964\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 23.3975\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 19.2065\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 17.7382\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 14.4932\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 15.0879\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 8.9654\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 15.5122\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 7.3555\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 12.5743\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 9.8955\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 8.9220\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 4.9561\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 10.9557\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 9.7083\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 4.5445\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 3.5669\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 10.0686\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 5.1431\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 9.0439\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 9.1884\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 3.9702\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 7.2571\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 4.4797\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 3.4600\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 5.4923\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 10.6480\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 3.0269\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 9.4039\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 5.8320\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 5.5971\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 4.1458\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.9356\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 2.8996\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 1.1001\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 5.4018\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 3.6518\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 1.9872\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 4.6739\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 3.5968\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 3.8827\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 2.6302\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 4.7827\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 4.2757\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 3.2213\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 1.2791\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 1.1140\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 1.5573\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 1.0612\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 2.8480\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 2.1977\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 4.1680\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 2.0648\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 2.6007\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 1.6640\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 2.7966\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 2.1605\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 4.6391\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 1.2543\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 2.0220\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 2.8851\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 3.2466\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 2.2466\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 1.9859\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 2.0683\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 2.3668\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 1.3403\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 1.7475\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 2.7000\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.9698\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.3833\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.9752\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 2.7137\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 1.1735\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.8092\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 1.4141\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.3789\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.9512\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 1.0801\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 1.5641\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.9926\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.5209\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.7542\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 1.1089\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.3365\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0633\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.8730\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.4031\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.5012\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.4416\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.5357\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.5177\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.1616\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 94.3 & Test Loss: 0.007572144470051418 %\n",
      "Total no of parameters in Model Theta 4 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 192.8318\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 78.5063\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 42.8359\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 29.9472\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 22.5182\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 15.5053\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 14.0802\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 13.5718\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 14.5213\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 11.7417\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 14.0529\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 9.0728\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 9.8282\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 6.3050\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 9.4666\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 4.3754\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 9.4965\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 7.9086\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 5.8877\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 2.9733\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 7.7115\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 5.9237\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 2.9910\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 2.2932\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 7.0542\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 3.6011\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 5.5813\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 5.0950\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 2.4658\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 4.9166\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 2.9127\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 2.3529\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 2.5040\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 7.9104\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 1.9019\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 5.7176\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 3.7018\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 4.3126\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 2.7333\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.4293\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 2.2771\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.8521\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 2.9878\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 2.8191\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 1.4583\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 2.9717\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 2.0116\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 2.2938\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 1.1398\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 2.6922\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 2.6031\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 2.0790\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 1.3340\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 1.2577\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.9471\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.7621\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 1.9722\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 1.7721\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 3.0685\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 1.3982\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 1.3149\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.4667\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 1.5806\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 1.0625\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 2.4999\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.7416\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.5639\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 1.6376\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 1.6144\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 1.2368\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.5623\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 1.2538\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 1.1449\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 1.1463\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.9028\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 2.2187\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.4706\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.1676\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.8329\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 1.4266\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.8199\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.6990\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.8469\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.2469\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.5529\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.6395\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.7683\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.6959\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.2286\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.2819\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.6554\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.2500\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0141\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.4615\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.1188\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.4127\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.1239\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.3662\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.4415\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.1323\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 94.58 & Test Loss: 0.00476404248545889 %\n",
      "Total no of parameters in Model Theta 5 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 122.6631\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 46.6824\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 28.2738\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 17.9535\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 12.5742\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 9.0895\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 7.6286\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 9.1273\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 9.9264\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 7.5976\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 8.8153\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 5.3379\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 6.0666\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 4.5787\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 5.7850\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 3.6589\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 5.4080\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 5.3123\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 3.9287\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 1.5013\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 4.7205\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 3.1933\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 1.9369\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 1.8053\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 3.6079\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 2.3162\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 3.4272\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 3.2140\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 1.3126\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 3.0144\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 1.4109\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.9904\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 1.8249\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 5.7522\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 1.1714\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 2.3500\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 2.2847\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 2.5075\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 1.8542\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.2350\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 1.0566\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.3110\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 1.7533\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 1.9152\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.9644\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 2.0327\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 1.3189\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 1.8731\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 1.5664\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 1.4690\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 2.3161\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.7586\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.9829\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.4146\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.4781\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.3638\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.9534\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.8398\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 1.8923\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.9270\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.5562\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.3419\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 1.1625\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.9686\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 1.8600\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.3076\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.3673\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 1.0584\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 1.1245\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.6208\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.2135\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.5691\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.4533\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.7889\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.5446\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.7826\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.3132\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0722\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.5816\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.8835\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.6768\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.4095\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.4969\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.1035\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.4628\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.3701\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.4843\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.3143\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.1252\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.1110\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.3855\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.1254\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0124\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.2719\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0630\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.1949\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0420\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.1746\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.3925\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0591\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 95.29 & Test Loss: 0.0028486402567090407 %\n",
      "Total no of parameters in Model Theta 6 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 68.0893\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 26.3998\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 17.1042\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 10.6512\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 7.4326\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 4.5591\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 4.5133\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 6.5311\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 6.4630\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 4.5539\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 6.2726\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 3.9719\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 3.3047\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 2.4994\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 3.6137\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 1.8700\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 4.0087\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 3.1191\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 1.7146\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.8609\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 2.6300\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 2.4337\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 1.3222\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.8759\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 2.6270\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 1.3115\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 1.7069\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 2.2135\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.8216\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 1.9131\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.8007\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.3068\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 1.1614\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 3.4996\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.4558\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.9822\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 1.5610\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 1.5799\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.9741\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.1832\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 1.1016\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.4062\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.5538\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 1.4033\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.3414\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 1.8391\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.4942\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.8591\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.7520\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.8422\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.8440\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.4719\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.4917\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.2681\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.3160\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.5891\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.6711\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.5965\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 1.2629\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.6794\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.2821\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.3179\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.7126\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.6488\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.8420\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.1979\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.2584\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.8361\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.6453\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.2753\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0863\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.2715\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.3043\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.4181\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.2273\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.4593\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.3206\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0594\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.4474\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.5352\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.3238\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.1920\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.3506\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0455\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.3036\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.3052\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.2274\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0914\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0870\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.1103\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.2021\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0553\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0173\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.2186\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.1041\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0867\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0157\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.1233\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.2830\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0259\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 96.27 & Test Loss: 0.001679167950869305 %\n",
      "Total no of parameters in Model Theta 7 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 40.6105\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 14.2755\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 9.9232\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 5.6598\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 4.0063\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 2.5850\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 2.4329\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 3.0878\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 3.5261\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 2.3963\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 3.8424\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 1.5990\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 1.9169\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 1.3102\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 1.9646\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 1.6157\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 2.3327\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 1.4229\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.8094\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.5101\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 1.5000\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 1.8069\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.7303\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.5716\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 1.4526\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.9303\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.7306\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 1.7042\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.5852\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 1.0458\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.4580\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0950\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.5473\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 1.6633\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.3388\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.4210\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.7090\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 1.0149\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.4893\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.4088\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.4608\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.1090\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.2788\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.8821\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.2642\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.7268\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.5606\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.4110\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.4081\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.7194\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.3383\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.4438\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.3858\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.1143\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.2111\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.2085\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.5328\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.3636\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.6843\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.5290\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0713\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.2525\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.4813\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.3105\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.4016\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0898\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.2690\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.4799\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.3901\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.1676\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0810\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.1442\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.1070\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.2476\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.1178\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.2918\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.2459\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0349\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.2309\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.3207\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.1634\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0880\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.2402\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0097\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.1705\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.2076\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.2195\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0515\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0587\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.1144\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.1182\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0198\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0212\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0613\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0985\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0782\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0100\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.1035\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.1844\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0351\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 96.16 & Test Loss: 0.0013073123369860696 %\n",
      "Total no of parameters in Model Theta 8 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 18.8986\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 7.1435\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 5.0365\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 2.7115\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 1.6590\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 1.6177\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 1.4804\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 1.6143\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 2.7707\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.9939\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 2.3299\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 1.0878\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 1.0145\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.6837\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.9781\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.5985\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 1.4253\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.9610\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.3008\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.3980\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.6398\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.8120\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.4998\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.3245\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.8542\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.4491\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.3451\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.8453\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.4658\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.4950\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.2391\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0082\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.3260\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.8056\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.2601\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.2570\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.3224\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.5704\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.3200\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.3035\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.2330\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0418\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.2405\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.3896\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.1193\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.5199\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.3109\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.2266\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.1874\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.3796\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.3105\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.3062\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.1950\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0633\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0720\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0595\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.2621\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.2598\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.4238\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.2754\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0305\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.1377\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.2642\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0733\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.2210\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0986\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.1440\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.2820\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.2650\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0990\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.1023\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.1546\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0306\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.1920\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.1076\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.1730\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.2427\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0341\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.1650\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.2120\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.1400\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0373\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.1517\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0084\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.1339\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.1143\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.1565\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0357\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0362\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.1050\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0763\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0335\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0336\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0436\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0849\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0481\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0085\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0436\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.1432\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0532\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 97.08 & Test Loss: 0.0008618880105379503 %\n",
      "Total no of parameters in Model Theta 9 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 8.0344\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 2.7540\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 2.4613\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 1.4838\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.7931\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.8571\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.8233\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 1.1085\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 1.2373\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.6413\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 1.1695\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.5445\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.6927\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.3221\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.4449\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.3190\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.6516\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.5662\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.3091\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.3413\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.4545\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.4400\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.2486\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.1791\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.5240\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.2786\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.2435\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.5627\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.2933\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.3385\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.2149\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0048\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.2301\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.4307\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.1875\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.1736\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.2202\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.4149\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.1485\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.2152\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.1380\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0220\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.1660\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.2973\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0557\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.3115\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.1937\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.1394\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.1089\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.2129\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.2210\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0911\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.1560\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0480\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0199\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0481\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.1792\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.1864\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.2528\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.2133\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0254\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0921\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.1820\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0421\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.1654\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0764\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0833\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.2409\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.2035\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0799\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0655\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.1166\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0283\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.1945\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0718\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.1437\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.2052\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0254\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.1191\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.1706\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0878\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0384\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0816\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0137\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.1031\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0931\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.1203\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0440\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0421\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0571\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0545\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0319\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0396\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0344\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0705\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0379\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0129\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0112\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0744\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0747\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 97.69 & Test Loss: 0.0006984521743477671 %\n",
      "Total no of parameters in Model Theta 10 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 3.8230\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 1.4275\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 1.4984\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.9438\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.5204\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.5031\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.4344\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.6314\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.6277\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.2271\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.7984\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.3856\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.3039\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.1613\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.1559\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.2284\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.3545\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.2636\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.1408\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.1967\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.2802\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.2813\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.1202\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0608\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.3061\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.1650\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0752\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.4522\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.1657\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.1890\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.1063\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0238\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.2004\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.2953\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.1336\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.1262\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.1852\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.3425\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.1086\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0530\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.1188\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0435\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.1257\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.1979\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0177\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.2101\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.1195\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.1038\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.1081\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.1749\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.1738\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0855\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.1127\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0418\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0108\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0630\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.1263\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.1453\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.1441\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.1172\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0109\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0498\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.1150\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0142\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.1149\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0564\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0497\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.1735\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.1025\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0513\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0407\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0829\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0405\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.1155\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0543\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0812\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.1565\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0246\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.1193\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.1635\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0325\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0399\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0882\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0308\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0831\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0878\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0939\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0211\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0669\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0525\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0389\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0371\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0419\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0307\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0652\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0520\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0139\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0064\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0427\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0546\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.14 & Test Loss: 0.0005531904358729661 %\n",
      "Total no of parameters in Model Theta 11 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 1.4808\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.4137\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.8371\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.4131\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.3518\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.2375\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.2411\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.3272\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.3291\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.1291\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.4306\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.2658\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.1427\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0720\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.1018\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.1797\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.1843\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.1996\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0633\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0720\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.1451\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0821\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0742\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0265\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.1808\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0997\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0397\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.3406\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0767\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0984\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0653\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0303\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.1423\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.2369\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.1626\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0896\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.1337\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.2080\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0638\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0176\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0974\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0873\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0666\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.1428\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0234\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.1410\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0952\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.1086\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.1012\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.1383\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.1525\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0795\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.1230\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0515\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0169\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.1161\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.1036\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.1083\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.1089\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0673\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0115\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0290\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.1150\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0189\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0833\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0446\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0457\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.1496\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0522\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0556\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0255\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0806\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0585\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0857\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0398\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0693\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.1426\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0153\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0808\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.1620\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0264\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0368\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0682\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0496\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0617\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0731\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0667\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0210\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0826\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0339\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0396\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0474\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0360\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0219\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0583\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0429\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0290\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0030\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0507\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0839\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.42 & Test Loss: 0.0004347441651210829 %\n",
      "Total no of parameters in Model Theta 12 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.3821\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.1670\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.4550\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.2057\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.1680\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0955\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0835\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.1492\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.1763\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0513\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.2848\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.1364\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0866\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0625\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0711\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.1080\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0601\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.1589\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0525\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0378\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.1468\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0563\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0414\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0308\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.1430\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0952\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0442\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.2010\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0438\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0776\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0325\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0223\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0743\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.1712\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.1032\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0833\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0876\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.1281\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0478\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0135\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0928\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0615\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0325\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0838\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0113\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0938\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0574\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.1023\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0799\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0916\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.1036\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0698\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0876\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0610\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0111\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.1067\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0820\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0655\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0959\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0386\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0144\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0339\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0784\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0187\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0546\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0481\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0314\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.1233\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0404\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0501\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0157\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0544\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0477\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0548\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0720\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0209\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.1318\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0079\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0693\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.1212\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0168\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0203\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0641\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0406\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0338\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0680\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0545\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0241\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0689\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0217\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0316\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0271\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0238\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0090\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0143\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0668\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0296\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0033\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0202\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.1027\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.45 & Test Loss: 0.0004397354414330039 %\n",
      "Total no of parameters in Model Theta 13 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.1276\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0309\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.2311\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0762\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0920\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0641\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0306\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0888\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0889\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0241\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.1328\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0828\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0825\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0288\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0517\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0336\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0402\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.1042\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0265\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0400\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.1044\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0684\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0323\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0622\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0954\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0566\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0318\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.1525\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0573\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0544\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0160\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0394\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0609\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.1051\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0596\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0705\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0825\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.1188\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0452\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0162\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0844\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0247\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0266\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0420\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0066\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0639\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0788\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.1043\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0614\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0511\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0804\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0348\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0599\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0572\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0124\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0789\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0690\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0558\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0420\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0219\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0150\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0464\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0738\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0277\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0386\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0309\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0181\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.1168\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0494\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0228\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0188\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0228\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0284\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0295\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0562\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0252\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0822\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0119\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0459\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0794\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0100\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0067\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0449\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0290\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0139\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0461\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0463\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0123\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0398\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0083\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0143\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0288\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0165\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0390\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0239\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0700\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0334\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0033\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0036\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0529\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.82 & Test Loss: 0.00035671099330720606 %\n",
      "Total no of parameters in Model Theta 14 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0501\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0179\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.1138\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0510\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0517\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0616\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0325\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0553\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0694\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0422\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0913\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0767\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0544\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0264\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0416\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0165\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0437\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0943\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0161\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0439\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.1014\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0592\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0320\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0595\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0932\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0612\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0355\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.1048\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0411\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0402\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0125\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0227\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0615\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0445\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0709\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0546\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0881\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0839\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0292\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0182\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0839\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0310\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0152\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0224\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0051\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0656\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0594\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0962\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0395\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0463\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0397\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0129\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0429\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0399\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0058\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0654\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0883\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0497\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0412\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0427\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0082\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0238\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0679\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0267\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0247\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0181\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0071\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0943\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0355\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0414\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0080\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0284\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0428\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0499\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0196\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0226\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0566\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0222\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0275\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0615\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0240\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0019\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0267\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0344\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0032\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0588\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0300\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0204\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0213\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0140\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0125\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0069\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0217\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0132\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0264\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0759\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0072\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0018\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0030\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0206\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.84 & Test Loss: 0.00034065885972540857 %\n",
      "Total no of parameters in Model Theta 15 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0212\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0409\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.1196\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0457\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0368\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0818\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0394\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0470\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0546\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0905\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.1163\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0492\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0488\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0292\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0385\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0236\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0326\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.1230\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0190\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0325\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0971\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0643\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0403\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0630\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.1180\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0391\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0321\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0795\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0384\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0612\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0066\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0312\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0378\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0432\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0405\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0615\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0738\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0443\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0320\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0057\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0657\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0245\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0098\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0175\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0055\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0588\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0569\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0751\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0324\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0433\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0283\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0286\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0305\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0189\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0052\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0441\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0821\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0332\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0511\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0136\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0091\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0185\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0470\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0225\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0145\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0240\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0063\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.1030\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0205\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0519\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0116\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0238\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0177\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0149\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0218\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0199\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0394\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0156\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0417\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0582\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0211\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0017\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0328\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0276\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0056\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0325\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0362\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0072\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0058\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0256\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0092\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0030\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0093\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0068\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0290\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0353\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0026\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0018\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0008\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0246\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.84 & Test Loss: 0.0003667798863740245 %\n",
      "Total no of parameters in Model Theta 16 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0532\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0398\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.1022\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0535\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0292\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0986\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0296\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0449\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0382\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0663\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0723\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0416\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0466\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0126\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0402\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0181\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0104\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.1332\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0229\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0243\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.1052\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0290\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0312\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0515\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.1085\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0273\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0362\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0508\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0720\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0660\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0077\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0174\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0323\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0324\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0124\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0615\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0760\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0390\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0300\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0021\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0653\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0157\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0021\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0160\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0050\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0638\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0370\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0731\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0354\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0185\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0282\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0329\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0154\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0171\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0091\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0227\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0589\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0551\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0178\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0050\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0187\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0086\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0332\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0205\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0112\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0097\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0139\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0413\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0362\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0369\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0035\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0111\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0046\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0084\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0074\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0102\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0140\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0025\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0262\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0707\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0181\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0008\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0172\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0056\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0047\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0294\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0474\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0216\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0213\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0383\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0125\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0030\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0069\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0009\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0077\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0506\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0007\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0014\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0015\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0087\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.84 & Test Loss: 0.0003822737357438427 %\n",
      "Total no of parameters in Model Theta 17 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0194\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0239\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0725\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0417\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0189\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0665\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0198\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0370\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0433\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0478\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0505\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0317\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0419\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0101\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0275\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0110\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0090\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0879\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0060\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0304\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0612\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0119\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0224\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0318\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0744\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0100\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0328\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0348\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0191\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0705\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0131\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0149\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0177\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0392\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0236\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0269\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.1124\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0302\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0062\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0013\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0153\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0029\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0026\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0432\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0021\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0213\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0163\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0743\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0090\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0174\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0183\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0034\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0287\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0093\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0007\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0032\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0375\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0131\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0007\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0074\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0097\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0043\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0330\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0007\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0031\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0035\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0030\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0277\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0381\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0130\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0003\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0013\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0016\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0048\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0019\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0009\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0034\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0079\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0511\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0038\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0035\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0031\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0064\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0089\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0039\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0341\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0141\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0211\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0014\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0048\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0052\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0014\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0031\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0274\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0055\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0064\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0206\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0017\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.83 & Test Loss: 0.0004126739696276445 %\n",
      "Total no of parameters in Model Theta 18 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0123\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0184\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0659\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0118\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0192\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0446\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0100\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0189\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0177\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0215\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0170\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0222\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0320\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0090\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0169\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0034\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0087\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0198\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0074\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0399\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0484\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0094\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0169\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0258\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0404\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0041\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0378\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0211\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0232\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0205\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0110\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0044\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0066\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0160\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0103\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0102\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0382\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0029\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0034\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0004\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0049\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0007\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0310\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0027\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0069\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0046\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0227\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0060\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0046\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0020\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0014\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0121\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0005\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0007\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0047\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0041\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0751\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0053\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0011\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0045\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0036\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0032\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0010\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0075\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0019\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0247\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0087\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0152\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0005\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0007\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0012\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0011\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0009\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0008\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0028\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0153\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0043\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0173\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0028\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0189\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0042\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0128\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0126\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0007\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0008\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0032\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0005\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0005\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0037\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0009\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0088\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0106\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.92 & Test Loss: 0.00041174454435837903 %\n",
      "Total no of parameters in Model Theta 19 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0096\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0098\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0355\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0142\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0086\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0263\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0063\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0182\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0057\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0076\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0123\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0183\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0303\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0035\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0116\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0013\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0046\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0171\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0048\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0348\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0119\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0020\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0133\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0041\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0245\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0026\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0353\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0033\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0129\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0110\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0041\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0004\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0046\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0075\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0033\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0040\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0519\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0025\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0011\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0006\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0013\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0005\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0006\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0107\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0009\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0037\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0076\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0043\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0046\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0004\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0006\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0137\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0004\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0005\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0062\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0313\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0007\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0007\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0006\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0048\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0003\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0015\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0012\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0132\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0107\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0066\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0003\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0003\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0003\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0007\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0005\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0004\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0005\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0536\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0043\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0161\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0003\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0101\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0460\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0022\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0072\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0014\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0005\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0009\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0020\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0012\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0111\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.84 & Test Loss: 0.0005097712854464583 %\n",
      "Total no of parameters in Model Theta 20 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0045\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0121\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0254\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0063\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0050\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0131\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0035\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0096\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0037\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0162\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0049\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0073\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0185\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0011\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0047\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0005\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0005\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0139\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0026\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0335\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0023\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0020\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0028\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0035\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0187\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0026\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0238\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0013\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0080\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0104\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0027\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0005\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0047\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0041\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0010\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0527\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0006\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0012\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0009\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0009\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0004\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0040\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0003\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0013\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0030\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0038\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0077\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0008\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0019\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0016\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0128\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0004\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0005\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0018\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0153\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0010\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0024\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0006\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0031\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0017\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0017\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0004\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0005\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0017\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0280\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0007\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0044\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0008\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0013\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0004\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0009\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0014\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0026\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0004\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0003\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0022\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0064\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0004\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0010\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0010\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.92 & Test Loss: 0.0005256244838986121 %\n",
      "Total no of parameters in Model Theta 21 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0014\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0131\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0223\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0024\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0019\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0109\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0016\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0051\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0024\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0145\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0036\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0120\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0183\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0005\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0010\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0005\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0032\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0017\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0379\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0039\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0003\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0007\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0006\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0110\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0017\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0338\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0012\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0102\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0033\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0022\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0003\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0038\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0016\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0037\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0290\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0004\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0014\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0163\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0003\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0007\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0028\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0021\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0009\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0009\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0166\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0047\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0417\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0009\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0013\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0003\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0012\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0220\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0003\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0025\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0027\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0021\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0003\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0025\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0002\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0071\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0066\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0012\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0080\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0028\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0002\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0010\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0189\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0083\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0346\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0014\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0012\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0217\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.59 & Test Loss: 0.0008563340627944698 %\n",
      "Total no of parameters in Model Theta 22 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0004\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0083\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0246\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0083\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0019\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0081\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0007\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0062\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0015\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0392\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0014\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0041\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0062\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0022\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0009\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0013\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0016\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0002\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0258\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0025\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0005\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0004\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0149\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0365\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0056\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0210\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0011\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0271\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0006\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0029\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0142\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0021\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0024\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0352\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0012\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0002\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0027\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0003\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0011\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0006\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0010\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0041\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0003\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0024\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0082\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0425\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0004\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0003\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0022\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0010\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0033\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0003\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0008\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0009\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0004\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0076\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0041\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0006\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0036\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0024\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0176\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0050\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0005\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0083\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0006\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0010\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0004\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.89 & Test Loss: 0.0005922652287992515 %\n",
      "Total no of parameters in Model Theta 23 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0163\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0332\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0333\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0025\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0050\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0001\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0048\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0006\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0078\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0136\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0012\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0026\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0035\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0015\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0010\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0258\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0096\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0006\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0007\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0006\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0570\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0020\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0041\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0017\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0005\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0014\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0085\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0005\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0010\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0008\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0259\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0008\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0004\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0003\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0050\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0010\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0003\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0417\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0003\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0105\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0040\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0019\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0020\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0138\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0003\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0005\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0072\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0007\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0002\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0008\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0004\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0003\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0002\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0002\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0008\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0013\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0055\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0003\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.76 & Test Loss: 0.0007076521805941054 %\n",
      "Total no of parameters in Model Theta 24 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0072\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0400\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0082\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0022\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0029\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0083\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0014\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0027\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0041\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0009\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0012\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0012\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0006\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0016\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0018\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0151\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0005\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0007\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0349\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0039\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0018\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0017\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0264\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0116\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0007\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0091\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0131\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0002\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0559\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0008\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0226\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0073\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0183\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0003\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0012\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0004\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0117\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0030\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0183\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0013\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0010\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0026\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0045\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0013\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0031\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0003\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0007\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0071\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0004\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.64 & Test Loss: 0.0009632429874012747 %\n",
      "Total no of parameters in Model Theta 25 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0004\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0062\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0205\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0002\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0014\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0174\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0003\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0061\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0017\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0018\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0007\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0056\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0025\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0004\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0002\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0008\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0050\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0003\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0016\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0011\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0066\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0300\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0024\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0169\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0021\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0016\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0360\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0003\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0004\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0007\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0010\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0028\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0010\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0003\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0006\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0004\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0012\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0006\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0015\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0004\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0020\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 2.822614305841853e-06\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0009\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0075\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0048\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0003\n",
      "Accuracy of the network on the test images: 98.65 & Test Loss: 0.0009658488633968728 %\n",
      "Total no of parameters in Model Theta 26 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0007\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0597\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0118\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0090\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0017\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0007\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0036\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0010\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0626\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0031\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0016\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0014\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0076\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0013\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0273\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0004\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0012\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0090\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0327\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0092\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0003\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0144\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0003\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0449\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0011\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0010\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0149\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0003\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0002\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0020\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0116\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0006\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0007\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0204\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 1.8332793843001127e-06\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0005\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0024\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0008\n",
      "Accuracy of the network on the test images: 98.81 & Test Loss: 0.0009659589939233178 %\n",
      "Total no of parameters in Model Theta 27 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0035\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0483\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0027\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0206\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0016\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0278\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0015\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0757\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0135\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0052\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0002\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0011\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0748\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0023\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0094\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0447\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0032\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0005\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0005\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0250\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0217\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0015\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0036\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0004\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0012\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0089\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 1.0014719009632245e-05\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0357\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0020\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Accuracy of the network on the test images: 98.74 & Test Loss: 0.0011467837225579832 %\n",
      "Total no of parameters in Model Theta 28 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0337\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0179\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0254\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0029\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0004\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0322\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0016\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0504\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0526\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0273\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0031\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0004\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.1129\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0005\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0101\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0673\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0039\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0016\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0313\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0121\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0003\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0497\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0014\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0007\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0003\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0008\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0122\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0006\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0213\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 8.344646751368145e-09\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0523\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0092\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0002\n",
      "Accuracy of the network on the test images: 98.7 & Test Loss: 0.001314548592046562 %\n",
      "Total no of parameters in Model Theta 29 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.1919\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0340\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0002\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0201\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0108\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0003\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0299\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0562\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0142\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0653\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0082\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0077\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.1438\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0016\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0110\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0046\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0261\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.1167\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0019\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0353\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0007\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0008\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0024\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0002\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0724\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0245\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0005\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0005\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0021\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0009\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 0.0\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0029\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0363\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0003\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0000\n",
      "Accuracy of the network on the test images: 98.85 & Test Loss: 0.0015739783156441278 %\n",
      "Total no of parameters in Model Theta 30 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.5395\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0288\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0333\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.1712\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0032\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0008\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.1512\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.1166\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0004\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.1034\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0194\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0009\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0008\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0427\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0007\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0429\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0526\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0003\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0149\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.2667\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0851\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0111\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0010\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0509\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0010\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0293\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0033\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0006\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0246\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0005\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0009\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0186\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0013\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0602\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0004\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0277\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 2.38418573772492e-09\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0011\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0263\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0002\n",
      "Accuracy of the network on the test images: 98.78 & Test Loss: 0.0019091117901165497 %\n"
     ]
    }
   ],
   "source": [
    "modelsTrainEpochArr2 = []\n",
    "modelsTrainLossArr2 = []\n",
    "modelsTrainAccArr2 = []\n",
    "modelsTestLossArr2 = []\n",
    "modelsTestAccArr2 = []\n",
    "\n",
    "for i in range (len(thetaArr)):\n",
    "    torch.manual_seed(1)\n",
    "    j=copy.deepcopy(i) \n",
    "    theta = (1-alpha[i])*Lr1_param + alpha[i]*Lr2_param\n",
    "    j = ThetaModel()\n",
    "    torch.nn.utils.vector_to_parameters(theta,j.parameters())\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(j.parameters(), lr=1e-3) #,weight_decay = 0.025)\n",
    "\n",
    "    a=[]\n",
    "    for k in j.parameters():\n",
    "        a.append(torch.numel(k))\n",
    "    print(f'Total no of parameters in Model Theta {i} is:{np.sum(a)}')\n",
    "\n",
    "    print(j.parameters)\n",
    "\n",
    "    max_epochs = 10\n",
    "    train_batch_size = 100\n",
    "    T2_train_epoch,T2_train_losses,T2_train_acc = trainFunc(j,max_epochs,train_batch_size)\n",
    "    \n",
    "    \n",
    "    modelsTrainEpochArr2.append(T2_train_epoch)\n",
    "    modelsTrainLossArr2.append(T2_train_losses)\n",
    "    modelsTrainAccArr2.append(T2_train_acc)\n",
    "    \n",
    "    test_batch_size=100\n",
    "    T2_acc,T2_testLoss = testFunction(j,loss_func,test_batch_size)\n",
    "    modelsTestAccArr2.append(T2_acc)\n",
    "    modelsTestLossArr2.append(T2_testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89.62884194614863,\n",
       " 61.79887772740043,\n",
       " 42.08576773314613,\n",
       " 27.694102717806896,\n",
       " 17.76721482405187,\n",
       " 11.014632973575795,\n",
       " 6.572698178871884,\n",
       " 3.7090218148498875,\n",
       " 1.9631822247291373,\n",
       " 1.0177112113306843,\n",
       " 0.5655763817184294,\n",
       " 0.28391484476396966,\n",
       " 0.1413017183856185,\n",
       " 0.06943830830068327,\n",
       " 0.04525784298930861,\n",
       " 0.039741171537641395,\n",
       " 0.03305051712005661,\n",
       " 0.02092504599868395,\n",
       " 0.012272847957333095,\n",
       " 0.0077360808638877645,\n",
       " 0.005641960839310741,\n",
       " 0.004287244827290162,\n",
       " 0.003704088951099872,\n",
       " 0.00332471203706506,\n",
       " 0.003609596868031848,\n",
       " 0.00465361259817729,\n",
       " 0.006279159568208365,\n",
       " 0.010022570470187434,\n",
       " 0.014064729865678182,\n",
       " 0.02115848555913043,\n",
       " 0.030557849257920502]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minScore(modelsTrainLossArr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[81.44456822040547,\n",
       " 82.13261913487203,\n",
       " 82.77886652101176,\n",
       " 83.61030656132422,\n",
       " 84.40509838761669,\n",
       " 85.36985557575807,\n",
       " 86.57398280195477,\n",
       " 88.14167627096502,\n",
       " 89.95553115098623,\n",
       " 91.53130063083748,\n",
       " 92.98458981262884,\n",
       " 94.67069750008962,\n",
       " 96.18584909313564,\n",
       " 97.54180568766613,\n",
       " 98.41494751250983,\n",
       " 98.71199005081381,\n",
       " 98.92528109170576,\n",
       " 99.30129781218571,\n",
       " 99.58960300838024,\n",
       " 99.75046654276956,\n",
       " 99.82538983347176,\n",
       " 99.86365214636047,\n",
       " 99.89509540615995,\n",
       " 99.89469085451289,\n",
       " 99.89073192278288,\n",
       " 99.83679538741447,\n",
       " 99.8037411504239,\n",
       " 99.71720785736764,\n",
       " 99.66369181920395,\n",
       " 99.55447537933436,\n",
       " 99.47716564922071]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanScore(modelsTrainAccArr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(alpha,minScore(modelsTrainLossArr2),color=\"Blue\",linestyle='dashed', marker=\"o\")\n",
    "ax.plot(alpha,modelsTestLossArr2,color=\"Blue\", marker=\"v\")\n",
    "ax.legend(['Train Loss','Test Loss'],loc=\"center left\")\n",
    "ax.set_xlabel(\"Alpha\",color=\"Green\")\n",
    "ax.set_ylabel(\"CrossEntropy Loss\",color = \"blue\")\n",
    "ax.set_title(\"learning rate 1e-3 vs. 1e-2\",color = \"green\")\n",
    "\n",
    "\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(alpha,maxScore(modelsTrainAccArr2),color=\"red\",linestyle='dashed', marker=\"o\")\n",
    "ax2.plot(alpha,modelsTestAccArr2,color=\"red\", marker=\"v\")\n",
    "ax2.set_xlabel(\"Alpha\",color=\"Green\")\n",
    "ax2.set_ylabel(\"Accuracy\",color = \"red\")\n",
    "ax2.legend(['Train Acc','Test Acc'],loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('D:/Clemson/COURSE/SEM-2/CPSC-8430 Deep Learning - 001/Homework/CPSC-8430-Deep-Learning-001/HW1/Diff Batch Graph HW1_3.1Lr.jpg',\n",
    "            format='jpeg',\n",
    "            dpi=100,\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1d86b2f3ed665d691ce24c615a98bbc398f66743afc4d4e970e6f8b36fab2b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CPSC-8430-DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
