{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x218115c99d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 60000 \n",
      "test_dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset),\"\\ntest_dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader 1\n",
    "m1train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "m1test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader 2\n",
    "m2train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=1000, \n",
    "                                           shuffle=True)\n",
    "\n",
    "m2test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M1(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(M1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size) #1st Convolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)   #pool_size=2, strides=2 \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size) #2nd Convolution\n",
    "        self.fc1 = nn.Linear(320, 50) #((I/P - Filter + 2*Pad)/Stride)+1 \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(50, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 320)            #Flattening \n",
    "        x = F.relu(self.fc1(x))        #Fully Connected NN   \n",
    "        x = self.dropout(x)   \n",
    "        x = F.relu(self.fc2(x))        #Fully Connected NN           \n",
    "        x = self.fc3(x)                #O/P Layer       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def trainFunc(model,num_epochs,train_loader):\n",
    "    print('strated')\n",
    "    n_total_steps = len(train_loader)\n",
    "    train_losses = []\n",
    "    train_epoch = []\n",
    "    train_acc = []\n",
    "    not_converged =True\n",
    "    epoch = 0\n",
    "    while not_converged:\n",
    "        epoch += 1\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            if (i+1)% 60 == 0 : print(i+1)\n",
    "            \n",
    "            # Forward pass\n",
    "            prediction = model(images)\n",
    "            loss = loss_func(prediction, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "            if (i+1) % 60 == 0:\n",
    "                print (f'Epoch [{epoch}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "                train_epoch.append(epoch)\n",
    "                train_losses.append(loss.item())\n",
    "                print(f'Epoch [{epoch}/{num_epochs}], Accuracy : {acc} %')\n",
    "                train_acc.append(acc)\n",
    "\n",
    "                if epoch == num_epochs:\n",
    "                        print(\"Max Epoch Reached\")\n",
    "                        not_converged = False\n",
    "                elif (epoch > 5) and  (train_losses[-1] < 0.001):\n",
    "                    if abs(train_losses[-3] - train_losses[-2]) < 1.0e-05 and abs(train_losses[-2] - train_losses[-1]) < 1.0e-05:\n",
    "                        print(\"Convergeance reached for loss:\",train_losses[-1])\n",
    "                        not_converged = False\n",
    "                        \n",
    "    return train_epoch,train_losses,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "max_epochs = 15\n",
    "learning_rate = 0.001\n",
    "kernel_size = 4\n",
    "weight_decay_val = 1e-4\n",
    "dropout = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model 1 with batch_size=64 is:25550\n"
     ]
    }
   ],
   "source": [
    "mBatch1 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mBatch1.parameters(), lr=learning_rate, weight_decay = weight_decay_val)\n",
    "\n",
    "a=[]\n",
    "for i in mBatch1.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model 1 with batch_size={64} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/938], Loss: 0.8804\n",
      "Epoch [1/15], Accuracy : 42.083333333333336 %\n",
      "120\n",
      "Epoch [1/15], Step [120/938], Loss: 0.4854\n",
      "Epoch [1/15], Accuracy : 58.658854166666664 %\n",
      "180\n",
      "Epoch [1/15], Step [180/938], Loss: 0.3982\n",
      "Epoch [1/15], Accuracy : 67.09201388888889 %\n",
      "240\n",
      "Epoch [1/15], Step [240/938], Loss: 0.3986\n",
      "Epoch [1/15], Accuracy : 72.14192708333333 %\n",
      "300\n",
      "Epoch [1/15], Step [300/938], Loss: 0.2927\n",
      "Epoch [1/15], Accuracy : 75.30208333333333 %\n",
      "360\n",
      "Epoch [1/15], Step [360/938], Loss: 0.3081\n",
      "Epoch [1/15], Accuracy : 77.58680555555556 %\n",
      "420\n",
      "Epoch [1/15], Step [420/938], Loss: 0.3896\n",
      "Epoch [1/15], Accuracy : 79.65029761904762 %\n",
      "480\n",
      "Epoch [1/15], Step [480/938], Loss: 0.2031\n",
      "Epoch [1/15], Accuracy : 81.17838541666667 %\n",
      "540\n",
      "Epoch [1/15], Step [540/938], Loss: 0.2787\n",
      "Epoch [1/15], Accuracy : 82.50289351851852 %\n",
      "600\n",
      "Epoch [1/15], Step [600/938], Loss: 0.2176\n",
      "Epoch [1/15], Accuracy : 83.60416666666667 %\n",
      "660\n",
      "Epoch [1/15], Step [660/938], Loss: 0.3323\n",
      "Epoch [1/15], Accuracy : 84.52414772727273 %\n",
      "720\n",
      "Epoch [1/15], Step [720/938], Loss: 0.2069\n",
      "Epoch [1/15], Accuracy : 85.29730902777777 %\n",
      "780\n",
      "Epoch [1/15], Step [780/938], Loss: 0.0954\n",
      "Epoch [1/15], Accuracy : 85.98157051282051 %\n",
      "840\n",
      "Epoch [1/15], Step [840/938], Loss: 0.2551\n",
      "Epoch [1/15], Accuracy : 86.5718005952381 %\n",
      "900\n",
      "Epoch [1/15], Step [900/938], Loss: 0.1711\n",
      "Epoch [1/15], Accuracy : 87.08506944444444 %\n",
      "60\n",
      "Epoch [2/15], Step [60/938], Loss: 0.2512\n",
      "Epoch [2/15], Accuracy : 95.33854166666667 %\n",
      "120\n",
      "Epoch [2/15], Step [120/938], Loss: 0.1344\n",
      "Epoch [2/15], Accuracy : 95.32552083333333 %\n",
      "180\n",
      "Epoch [2/15], Step [180/938], Loss: 0.1387\n",
      "Epoch [2/15], Accuracy : 95.19965277777777 %\n",
      "240\n",
      "Epoch [2/15], Step [240/938], Loss: 0.3747\n",
      "Epoch [2/15], Accuracy : 95.25390625 %\n",
      "300\n",
      "Epoch [2/15], Step [300/938], Loss: 0.2082\n",
      "Epoch [2/15], Accuracy : 95.41145833333333 %\n",
      "360\n",
      "Epoch [2/15], Step [360/938], Loss: 0.1493\n",
      "Epoch [2/15], Accuracy : 95.44270833333333 %\n",
      "420\n",
      "Epoch [2/15], Step [420/938], Loss: 0.0675\n",
      "Epoch [2/15], Accuracy : 95.33482142857143 %\n",
      "480\n",
      "Epoch [2/15], Step [480/938], Loss: 0.1553\n",
      "Epoch [2/15], Accuracy : 95.41015625 %\n",
      "540\n",
      "Epoch [2/15], Step [540/938], Loss: 0.0880\n",
      "Epoch [2/15], Accuracy : 95.52372685185185 %\n",
      "600\n",
      "Epoch [2/15], Step [600/938], Loss: 0.1772\n",
      "Epoch [2/15], Accuracy : 95.56770833333333 %\n",
      "660\n",
      "Epoch [2/15], Step [660/938], Loss: 0.0824\n",
      "Epoch [2/15], Accuracy : 95.64393939393939 %\n",
      "720\n",
      "Epoch [2/15], Step [720/938], Loss: 0.1321\n",
      "Epoch [2/15], Accuracy : 95.66189236111111 %\n",
      "780\n",
      "Epoch [2/15], Step [780/938], Loss: 0.1086\n",
      "Epoch [2/15], Accuracy : 95.70913461538461 %\n",
      "840\n",
      "Epoch [2/15], Step [840/938], Loss: 0.1138\n",
      "Epoch [2/15], Accuracy : 95.76450892857143 %\n",
      "900\n",
      "Epoch [2/15], Step [900/938], Loss: 0.1211\n",
      "Epoch [2/15], Accuracy : 95.79513888888889 %\n",
      "60\n",
      "Epoch [3/15], Step [60/938], Loss: 0.0933\n",
      "Epoch [3/15], Accuracy : 97.03125 %\n",
      "120\n",
      "Epoch [3/15], Step [120/938], Loss: 0.1297\n",
      "Epoch [3/15], Accuracy : 96.6015625 %\n",
      "180\n",
      "Epoch [3/15], Step [180/938], Loss: 0.0956\n",
      "Epoch [3/15], Accuracy : 96.57986111111111 %\n",
      "240\n",
      "Epoch [3/15], Step [240/938], Loss: 0.2064\n",
      "Epoch [3/15], Accuracy : 96.6015625 %\n",
      "300\n",
      "Epoch [3/15], Step [300/938], Loss: 0.0523\n",
      "Epoch [3/15], Accuracy : 96.5625 %\n",
      "360\n",
      "Epoch [3/15], Step [360/938], Loss: 0.0538\n",
      "Epoch [3/15], Accuracy : 96.66666666666667 %\n",
      "420\n",
      "Epoch [3/15], Step [420/938], Loss: 0.2142\n",
      "Epoch [3/15], Accuracy : 96.67410714285714 %\n",
      "480\n",
      "Epoch [3/15], Step [480/938], Loss: 0.0746\n",
      "Epoch [3/15], Accuracy : 96.69596354166667 %\n",
      "540\n",
      "Epoch [3/15], Step [540/938], Loss: 0.3915\n",
      "Epoch [3/15], Accuracy : 96.74189814814815 %\n",
      "600\n",
      "Epoch [3/15], Step [600/938], Loss: 0.0588\n",
      "Epoch [3/15], Accuracy : 96.72395833333333 %\n",
      "660\n",
      "Epoch [3/15], Step [660/938], Loss: 0.2151\n",
      "Epoch [3/15], Accuracy : 96.78503787878788 %\n",
      "720\n",
      "Epoch [3/15], Step [720/938], Loss: 0.1994\n",
      "Epoch [3/15], Accuracy : 96.83376736111111 %\n",
      "780\n",
      "Epoch [3/15], Step [780/938], Loss: 0.1110\n",
      "Epoch [3/15], Accuracy : 96.8349358974359 %\n",
      "840\n",
      "Epoch [3/15], Step [840/938], Loss: 0.1465\n",
      "Epoch [3/15], Accuracy : 96.8359375 %\n",
      "900\n",
      "Epoch [3/15], Step [900/938], Loss: 0.1635\n",
      "Epoch [3/15], Accuracy : 96.84375 %\n",
      "60\n",
      "Epoch [4/15], Step [60/938], Loss: 0.1135\n",
      "Epoch [4/15], Accuracy : 97.78645833333333 %\n",
      "120\n",
      "Epoch [4/15], Step [120/938], Loss: 0.0972\n",
      "Epoch [4/15], Accuracy : 97.63020833333333 %\n",
      "180\n",
      "Epoch [4/15], Step [180/938], Loss: 0.1619\n",
      "Epoch [4/15], Accuracy : 97.35243055555556 %\n",
      "240\n",
      "Epoch [4/15], Step [240/938], Loss: 0.0813\n",
      "Epoch [4/15], Accuracy : 97.28515625 %\n",
      "300\n",
      "Epoch [4/15], Step [300/938], Loss: 0.0285\n",
      "Epoch [4/15], Accuracy : 97.234375 %\n",
      "360\n",
      "Epoch [4/15], Step [360/938], Loss: 0.0986\n",
      "Epoch [4/15], Accuracy : 97.20920138888889 %\n",
      "420\n",
      "Epoch [4/15], Step [420/938], Loss: 0.3632\n",
      "Epoch [4/15], Accuracy : 97.1875 %\n",
      "480\n",
      "Epoch [4/15], Step [480/938], Loss: 0.0562\n",
      "Epoch [4/15], Accuracy : 97.22005208333333 %\n",
      "540\n",
      "Epoch [4/15], Step [540/938], Loss: 0.1956\n",
      "Epoch [4/15], Accuracy : 97.22222222222223 %\n",
      "600\n",
      "Epoch [4/15], Step [600/938], Loss: 0.0185\n",
      "Epoch [4/15], Accuracy : 97.234375 %\n",
      "660\n",
      "Epoch [4/15], Step [660/938], Loss: 0.0921\n",
      "Epoch [4/15], Accuracy : 97.265625 %\n",
      "720\n",
      "Epoch [4/15], Step [720/938], Loss: 0.0832\n",
      "Epoch [4/15], Accuracy : 97.27647569444444 %\n",
      "780\n",
      "Epoch [4/15], Step [780/938], Loss: 0.0284\n",
      "Epoch [4/15], Accuracy : 97.29767628205128 %\n",
      "840\n",
      "Epoch [4/15], Step [840/938], Loss: 0.0685\n",
      "Epoch [4/15], Accuracy : 97.33258928571429 %\n",
      "900\n",
      "Epoch [4/15], Step [900/938], Loss: 0.0841\n",
      "Epoch [4/15], Accuracy : 97.35243055555556 %\n",
      "60\n",
      "Epoch [5/15], Step [60/938], Loss: 0.0544\n",
      "Epoch [5/15], Accuracy : 97.890625 %\n",
      "120\n",
      "Epoch [5/15], Step [120/938], Loss: 0.0568\n",
      "Epoch [5/15], Accuracy : 97.72135416666667 %\n",
      "180\n",
      "Epoch [5/15], Step [180/938], Loss: 0.0648\n",
      "Epoch [5/15], Accuracy : 97.578125 %\n",
      "240\n",
      "Epoch [5/15], Step [240/938], Loss: 0.0181\n",
      "Epoch [5/15], Accuracy : 97.64973958333333 %\n",
      "300\n",
      "Epoch [5/15], Step [300/938], Loss: 0.0440\n",
      "Epoch [5/15], Accuracy : 97.58854166666667 %\n",
      "360\n",
      "Epoch [5/15], Step [360/938], Loss: 0.0445\n",
      "Epoch [5/15], Accuracy : 97.59982638888889 %\n",
      "420\n",
      "Epoch [5/15], Step [420/938], Loss: 0.0447\n",
      "Epoch [5/15], Accuracy : 97.67113095238095 %\n",
      "480\n",
      "Epoch [5/15], Step [480/938], Loss: 0.1661\n",
      "Epoch [5/15], Accuracy : 97.65950520833333 %\n",
      "540\n",
      "Epoch [5/15], Step [540/938], Loss: 0.1977\n",
      "Epoch [5/15], Accuracy : 97.60995370370371 %\n",
      "600\n",
      "Epoch [5/15], Step [600/938], Loss: 0.0420\n",
      "Epoch [5/15], Accuracy : 97.625 %\n",
      "660\n",
      "Epoch [5/15], Step [660/938], Loss: 0.0321\n",
      "Epoch [5/15], Accuracy : 97.64441287878788 %\n",
      "720\n",
      "Epoch [5/15], Step [720/938], Loss: 0.1398\n",
      "Epoch [5/15], Accuracy : 97.65625 %\n",
      "780\n",
      "Epoch [5/15], Step [780/938], Loss: 0.0507\n",
      "Epoch [5/15], Accuracy : 97.62620192307692 %\n",
      "840\n",
      "Epoch [5/15], Step [840/938], Loss: 0.0297\n",
      "Epoch [5/15], Accuracy : 97.63206845238095 %\n",
      "900\n",
      "Epoch [5/15], Step [900/938], Loss: 0.0590\n",
      "Epoch [5/15], Accuracy : 97.63194444444444 %\n",
      "60\n",
      "Epoch [6/15], Step [60/938], Loss: 0.0138\n",
      "Epoch [6/15], Accuracy : 98.359375 %\n",
      "120\n",
      "Epoch [6/15], Step [120/938], Loss: 0.0083\n",
      "Epoch [6/15], Accuracy : 98.15104166666667 %\n",
      "180\n",
      "Epoch [6/15], Step [180/938], Loss: 0.1345\n",
      "Epoch [6/15], Accuracy : 98.03819444444444 %\n",
      "240\n",
      "Epoch [6/15], Step [240/938], Loss: 0.0548\n",
      "Epoch [6/15], Accuracy : 98.01432291666667 %\n",
      "300\n",
      "Epoch [6/15], Step [300/938], Loss: 0.0065\n",
      "Epoch [6/15], Accuracy : 98.0 %\n",
      "360\n",
      "Epoch [6/15], Step [360/938], Loss: 0.0870\n",
      "Epoch [6/15], Accuracy : 97.99045138888889 %\n",
      "420\n",
      "Epoch [6/15], Step [420/938], Loss: 0.0360\n",
      "Epoch [6/15], Accuracy : 98.02455357142857 %\n",
      "480\n",
      "Epoch [6/15], Step [480/938], Loss: 0.0302\n",
      "Epoch [6/15], Accuracy : 97.99479166666667 %\n",
      "540\n",
      "Epoch [6/15], Step [540/938], Loss: 0.0823\n",
      "Epoch [6/15], Accuracy : 97.9832175925926 %\n",
      "600\n",
      "Epoch [6/15], Step [600/938], Loss: 0.0294\n",
      "Epoch [6/15], Accuracy : 97.97395833333333 %\n",
      "660\n",
      "Epoch [6/15], Step [660/938], Loss: 0.0590\n",
      "Epoch [6/15], Accuracy : 97.95691287878788 %\n",
      "720\n",
      "Epoch [6/15], Step [720/938], Loss: 0.0148\n",
      "Epoch [6/15], Accuracy : 97.94704861111111 %\n",
      "780\n",
      "Epoch [6/15], Step [780/938], Loss: 0.0927\n",
      "Epoch [6/15], Accuracy : 97.93669871794872 %\n",
      "840\n",
      "Epoch [6/15], Step [840/938], Loss: 0.0568\n",
      "Epoch [6/15], Accuracy : 97.9296875 %\n",
      "900\n",
      "Epoch [6/15], Step [900/938], Loss: 0.0511\n",
      "Epoch [6/15], Accuracy : 97.93923611111111 %\n",
      "60\n",
      "Epoch [7/15], Step [60/938], Loss: 0.2853\n",
      "Epoch [7/15], Accuracy : 97.91666666666667 %\n",
      "120\n",
      "Epoch [7/15], Step [120/938], Loss: 0.0243\n",
      "Epoch [7/15], Accuracy : 98.13802083333333 %\n",
      "180\n",
      "Epoch [7/15], Step [180/938], Loss: 0.0064\n",
      "Epoch [7/15], Accuracy : 98.17708333333333 %\n",
      "240\n",
      "Epoch [7/15], Step [240/938], Loss: 0.1259\n",
      "Epoch [7/15], Accuracy : 98.1640625 %\n",
      "300\n",
      "Epoch [7/15], Step [300/938], Loss: 0.0800\n",
      "Epoch [7/15], Accuracy : 98.11458333333333 %\n",
      "360\n",
      "Epoch [7/15], Step [360/938], Loss: 0.0076\n",
      "Epoch [7/15], Accuracy : 98.11631944444444 %\n",
      "420\n",
      "Epoch [7/15], Step [420/938], Loss: 0.0665\n",
      "Epoch [7/15], Accuracy : 98.09895833333333 %\n",
      "480\n",
      "Epoch [7/15], Step [480/938], Loss: 0.2030\n",
      "Epoch [7/15], Accuracy : 98.09244791666667 %\n",
      "540\n",
      "Epoch [7/15], Step [540/938], Loss: 0.0350\n",
      "Epoch [7/15], Accuracy : 98.11921296296296 %\n",
      "600\n",
      "Epoch [7/15], Step [600/938], Loss: 0.0729\n",
      "Epoch [7/15], Accuracy : 98.1171875 %\n",
      "660\n",
      "Epoch [7/15], Step [660/938], Loss: 0.0110\n",
      "Epoch [7/15], Accuracy : 98.15814393939394 %\n",
      "720\n",
      "Epoch [7/15], Step [720/938], Loss: 0.0888\n",
      "Epoch [7/15], Accuracy : 98.14019097222223 %\n",
      "780\n",
      "Epoch [7/15], Step [780/938], Loss: 0.0019\n",
      "Epoch [7/15], Accuracy : 98.15705128205128 %\n",
      "840\n",
      "Epoch [7/15], Step [840/938], Loss: 0.1468\n",
      "Epoch [7/15], Accuracy : 98.1640625 %\n",
      "900\n",
      "Epoch [7/15], Step [900/938], Loss: 0.0389\n",
      "Epoch [7/15], Accuracy : 98.15798611111111 %\n",
      "60\n",
      "Epoch [8/15], Step [60/938], Loss: 0.0149\n",
      "Epoch [8/15], Accuracy : 98.671875 %\n",
      "120\n",
      "Epoch [8/15], Step [120/938], Loss: 0.0031\n",
      "Epoch [8/15], Accuracy : 98.52864583333333 %\n",
      "180\n",
      "Epoch [8/15], Step [180/938], Loss: 0.0677\n",
      "Epoch [8/15], Accuracy : 98.46354166666667 %\n",
      "240\n",
      "Epoch [8/15], Step [240/938], Loss: 0.0668\n",
      "Epoch [8/15], Accuracy : 98.46354166666667 %\n",
      "300\n",
      "Epoch [8/15], Step [300/938], Loss: 0.0137\n",
      "Epoch [8/15], Accuracy : 98.41145833333333 %\n",
      "360\n",
      "Epoch [8/15], Step [360/938], Loss: 0.0287\n",
      "Epoch [8/15], Accuracy : 98.28125 %\n",
      "420\n",
      "Epoch [8/15], Step [420/938], Loss: 0.0289\n",
      "Epoch [8/15], Accuracy : 98.27008928571429 %\n",
      "480\n",
      "Epoch [8/15], Step [480/938], Loss: 0.0270\n",
      "Epoch [8/15], Accuracy : 98.251953125 %\n",
      "540\n",
      "Epoch [8/15], Step [540/938], Loss: 0.0165\n",
      "Epoch [8/15], Accuracy : 98.2667824074074 %\n",
      "600\n",
      "Epoch [8/15], Step [600/938], Loss: 0.2009\n",
      "Epoch [8/15], Accuracy : 98.3046875 %\n",
      "660\n",
      "Epoch [8/15], Step [660/938], Loss: 0.0290\n",
      "Epoch [8/15], Accuracy : 98.26467803030303 %\n",
      "720\n",
      "Epoch [8/15], Step [720/938], Loss: 0.0232\n",
      "Epoch [8/15], Accuracy : 98.27473958333333 %\n",
      "780\n",
      "Epoch [8/15], Step [780/938], Loss: 0.0465\n",
      "Epoch [8/15], Accuracy : 98.26121794871794 %\n",
      "840\n",
      "Epoch [8/15], Step [840/938], Loss: 0.0389\n",
      "Epoch [8/15], Accuracy : 98.2421875 %\n",
      "900\n",
      "Epoch [8/15], Step [900/938], Loss: 0.0997\n",
      "Epoch [8/15], Accuracy : 98.23090277777777 %\n",
      "60\n",
      "Epoch [9/15], Step [60/938], Loss: 0.0277\n",
      "Epoch [9/15], Accuracy : 98.41145833333333 %\n",
      "120\n",
      "Epoch [9/15], Step [120/938], Loss: 0.0078\n",
      "Epoch [9/15], Accuracy : 98.22916666666667 %\n",
      "180\n",
      "Epoch [9/15], Step [180/938], Loss: 0.0143\n",
      "Epoch [9/15], Accuracy : 98.26388888888889 %\n",
      "240\n",
      "Epoch [9/15], Step [240/938], Loss: 0.0228\n",
      "Epoch [9/15], Accuracy : 98.28776041666667 %\n",
      "300\n",
      "Epoch [9/15], Step [300/938], Loss: 0.0413\n",
      "Epoch [9/15], Accuracy : 98.28125 %\n",
      "360\n",
      "Epoch [9/15], Step [360/938], Loss: 0.0098\n",
      "Epoch [9/15], Accuracy : 98.3203125 %\n",
      "420\n",
      "Epoch [9/15], Step [420/938], Loss: 0.0100\n",
      "Epoch [9/15], Accuracy : 98.35565476190476 %\n",
      "480\n",
      "Epoch [9/15], Step [480/938], Loss: 0.0084\n",
      "Epoch [9/15], Accuracy : 98.33333333333333 %\n",
      "540\n",
      "Epoch [9/15], Step [540/938], Loss: 0.0558\n",
      "Epoch [9/15], Accuracy : 98.359375 %\n",
      "600\n",
      "Epoch [9/15], Step [600/938], Loss: 0.1854\n",
      "Epoch [9/15], Accuracy : 98.36197916666667 %\n",
      "660\n",
      "Epoch [9/15], Step [660/938], Loss: 0.2017\n",
      "Epoch [9/15], Accuracy : 98.38068181818181 %\n",
      "720\n",
      "Epoch [9/15], Step [720/938], Loss: 0.0281\n",
      "Epoch [9/15], Accuracy : 98.39409722222223 %\n",
      "780\n",
      "Epoch [9/15], Step [780/938], Loss: 0.0038\n",
      "Epoch [9/15], Accuracy : 98.36939102564102 %\n",
      "840\n",
      "Epoch [9/15], Step [840/938], Loss: 0.0602\n",
      "Epoch [9/15], Accuracy : 98.36495535714286 %\n",
      "900\n",
      "Epoch [9/15], Step [900/938], Loss: 0.1356\n",
      "Epoch [9/15], Accuracy : 98.375 %\n",
      "60\n",
      "Epoch [10/15], Step [60/938], Loss: 0.1850\n",
      "Epoch [10/15], Accuracy : 98.4375 %\n",
      "120\n",
      "Epoch [10/15], Step [120/938], Loss: 0.0255\n",
      "Epoch [10/15], Accuracy : 98.515625 %\n",
      "180\n",
      "Epoch [10/15], Step [180/938], Loss: 0.0425\n",
      "Epoch [10/15], Accuracy : 98.46354166666667 %\n",
      "240\n",
      "Epoch [10/15], Step [240/938], Loss: 0.0170\n",
      "Epoch [10/15], Accuracy : 98.46354166666667 %\n",
      "300\n",
      "Epoch [10/15], Step [300/938], Loss: 0.0044\n",
      "Epoch [10/15], Accuracy : 98.49479166666667 %\n",
      "360\n",
      "Epoch [10/15], Step [360/938], Loss: 0.0480\n",
      "Epoch [10/15], Accuracy : 98.51128472222223 %\n",
      "420\n",
      "Epoch [10/15], Step [420/938], Loss: 0.0190\n",
      "Epoch [10/15], Accuracy : 98.48958333333333 %\n",
      "480\n",
      "Epoch [10/15], Step [480/938], Loss: 0.0245\n",
      "Epoch [10/15], Accuracy : 98.51888020833333 %\n",
      "540\n",
      "Epoch [10/15], Step [540/938], Loss: 0.0650\n",
      "Epoch [10/15], Accuracy : 98.50983796296296 %\n",
      "600\n",
      "Epoch [10/15], Step [600/938], Loss: 0.0532\n",
      "Epoch [10/15], Accuracy : 98.49739583333333 %\n",
      "660\n",
      "Epoch [10/15], Step [660/938], Loss: 0.1065\n",
      "Epoch [10/15], Accuracy : 98.46827651515152 %\n",
      "720\n",
      "Epoch [10/15], Step [720/938], Loss: 0.1364\n",
      "Epoch [10/15], Accuracy : 98.47222222222223 %\n",
      "780\n",
      "Epoch [10/15], Step [780/938], Loss: 0.0068\n",
      "Epoch [10/15], Accuracy : 98.46754807692308 %\n",
      "840\n",
      "Epoch [10/15], Step [840/938], Loss: 0.0352\n",
      "Epoch [10/15], Accuracy : 98.45424107142857 %\n",
      "900\n",
      "Epoch [10/15], Step [900/938], Loss: 0.0330\n",
      "Epoch [10/15], Accuracy : 98.45659722222223 %\n",
      "60\n",
      "Epoch [11/15], Step [60/938], Loss: 0.0409\n",
      "Epoch [11/15], Accuracy : 98.77604166666667 %\n",
      "120\n",
      "Epoch [11/15], Step [120/938], Loss: 0.0957\n",
      "Epoch [11/15], Accuracy : 98.84114583333333 %\n",
      "180\n",
      "Epoch [11/15], Step [180/938], Loss: 0.0184\n",
      "Epoch [11/15], Accuracy : 98.671875 %\n",
      "240\n",
      "Epoch [11/15], Step [240/938], Loss: 0.0055\n",
      "Epoch [11/15], Accuracy : 98.60677083333333 %\n",
      "300\n",
      "Epoch [11/15], Step [300/938], Loss: 0.0053\n",
      "Epoch [11/15], Accuracy : 98.671875 %\n",
      "360\n",
      "Epoch [11/15], Step [360/938], Loss: 0.0045\n",
      "Epoch [11/15], Accuracy : 98.68055555555556 %\n",
      "420\n",
      "Epoch [11/15], Step [420/938], Loss: 0.0508\n",
      "Epoch [11/15], Accuracy : 98.61607142857143 %\n",
      "480\n",
      "Epoch [11/15], Step [480/938], Loss: 0.0121\n",
      "Epoch [11/15], Accuracy : 98.58723958333333 %\n",
      "540\n",
      "Epoch [11/15], Step [540/938], Loss: 0.0500\n",
      "Epoch [11/15], Accuracy : 98.63715277777777 %\n",
      "600\n",
      "Epoch [11/15], Step [600/938], Loss: 0.0959\n",
      "Epoch [11/15], Accuracy : 98.63802083333333 %\n",
      "660\n",
      "Epoch [11/15], Step [660/938], Loss: 0.0049\n",
      "Epoch [11/15], Accuracy : 98.66003787878788 %\n",
      "720\n",
      "Epoch [11/15], Step [720/938], Loss: 0.0509\n",
      "Epoch [11/15], Accuracy : 98.63498263888889 %\n",
      "780\n",
      "Epoch [11/15], Step [780/938], Loss: 0.0240\n",
      "Epoch [11/15], Accuracy : 98.61378205128206 %\n",
      "840\n",
      "Epoch [11/15], Step [840/938], Loss: 0.0083\n",
      "Epoch [11/15], Accuracy : 98.62537202380952 %\n",
      "900\n",
      "Epoch [11/15], Step [900/938], Loss: 0.0763\n",
      "Epoch [11/15], Accuracy : 98.62152777777777 %\n",
      "60\n",
      "Epoch [12/15], Step [60/938], Loss: 0.0724\n",
      "Epoch [12/15], Accuracy : 98.80208333333333 %\n",
      "120\n",
      "Epoch [12/15], Step [120/938], Loss: 0.1703\n",
      "Epoch [12/15], Accuracy : 98.68489583333333 %\n",
      "180\n",
      "Epoch [12/15], Step [180/938], Loss: 0.1339\n",
      "Epoch [12/15], Accuracy : 98.56770833333333 %\n",
      "240\n",
      "Epoch [12/15], Step [240/938], Loss: 0.0098\n",
      "Epoch [12/15], Accuracy : 98.56770833333333 %\n",
      "300\n",
      "Epoch [12/15], Step [300/938], Loss: 0.0178\n",
      "Epoch [12/15], Accuracy : 98.59895833333333 %\n",
      "360\n",
      "Epoch [12/15], Step [360/938], Loss: 0.0736\n",
      "Epoch [12/15], Accuracy : 98.57638888888889 %\n",
      "420\n",
      "Epoch [12/15], Step [420/938], Loss: 0.0149\n",
      "Epoch [12/15], Accuracy : 98.60119047619048 %\n",
      "480\n",
      "Epoch [12/15], Step [480/938], Loss: 0.1521\n",
      "Epoch [12/15], Accuracy : 98.61979166666667 %\n",
      "540\n",
      "Epoch [12/15], Step [540/938], Loss: 0.0111\n",
      "Epoch [12/15], Accuracy : 98.62557870370371 %\n",
      "600\n",
      "Epoch [12/15], Step [600/938], Loss: 0.0085\n",
      "Epoch [12/15], Accuracy : 98.65364583333333 %\n",
      "660\n",
      "Epoch [12/15], Step [660/938], Loss: 0.0769\n",
      "Epoch [12/15], Accuracy : 98.63873106060606 %\n",
      "720\n",
      "Epoch [12/15], Step [720/938], Loss: 0.0928\n",
      "Epoch [12/15], Accuracy : 98.6328125 %\n",
      "780\n",
      "Epoch [12/15], Step [780/938], Loss: 0.0557\n",
      "Epoch [12/15], Accuracy : 98.6338141025641 %\n",
      "840\n",
      "Epoch [12/15], Step [840/938], Loss: 0.0058\n",
      "Epoch [12/15], Accuracy : 98.65885416666667 %\n",
      "900\n",
      "Epoch [12/15], Step [900/938], Loss: 0.0233\n",
      "Epoch [12/15], Accuracy : 98.67708333333333 %\n",
      "60\n",
      "Epoch [13/15], Step [60/938], Loss: 0.0050\n",
      "Epoch [13/15], Accuracy : 98.77604166666667 %\n",
      "120\n",
      "Epoch [13/15], Step [120/938], Loss: 0.0924\n",
      "Epoch [13/15], Accuracy : 98.76302083333333 %\n",
      "180\n",
      "Epoch [13/15], Step [180/938], Loss: 0.0069\n",
      "Epoch [13/15], Accuracy : 98.71527777777777 %\n",
      "240\n",
      "Epoch [13/15], Step [240/938], Loss: 0.1034\n",
      "Epoch [13/15], Accuracy : 98.69791666666667 %\n",
      "300\n",
      "Epoch [13/15], Step [300/938], Loss: 0.0052\n",
      "Epoch [13/15], Accuracy : 98.671875 %\n",
      "360\n",
      "Epoch [13/15], Step [360/938], Loss: 0.0502\n",
      "Epoch [13/15], Accuracy : 98.671875 %\n",
      "420\n",
      "Epoch [13/15], Step [420/938], Loss: 0.0088\n",
      "Epoch [13/15], Accuracy : 98.67931547619048 %\n",
      "480\n",
      "Epoch [13/15], Step [480/938], Loss: 0.0245\n",
      "Epoch [13/15], Accuracy : 98.64908854166667 %\n",
      "540\n",
      "Epoch [13/15], Step [540/938], Loss: 0.0059\n",
      "Epoch [13/15], Accuracy : 98.66608796296296 %\n",
      "600\n",
      "Epoch [13/15], Step [600/938], Loss: 0.0236\n",
      "Epoch [13/15], Accuracy : 98.6796875 %\n",
      "660\n",
      "Epoch [13/15], Step [660/938], Loss: 0.0232\n",
      "Epoch [13/15], Accuracy : 98.66950757575758 %\n",
      "720\n",
      "Epoch [13/15], Step [720/938], Loss: 0.0094\n",
      "Epoch [13/15], Accuracy : 98.68272569444444 %\n",
      "780\n",
      "Epoch [13/15], Step [780/938], Loss: 0.0319\n",
      "Epoch [13/15], Accuracy : 98.6698717948718 %\n",
      "840\n",
      "Epoch [13/15], Step [840/938], Loss: 0.0970\n",
      "Epoch [13/15], Accuracy : 98.671875 %\n",
      "900\n",
      "Epoch [13/15], Step [900/938], Loss: 0.0035\n",
      "Epoch [13/15], Accuracy : 98.68229166666667 %\n",
      "60\n",
      "Epoch [14/15], Step [60/938], Loss: 0.0276\n",
      "Epoch [14/15], Accuracy : 98.80208333333333 %\n",
      "120\n",
      "Epoch [14/15], Step [120/938], Loss: 0.0080\n",
      "Epoch [14/15], Accuracy : 98.91927083333333 %\n",
      "180\n",
      "Epoch [14/15], Step [180/938], Loss: 0.0179\n",
      "Epoch [14/15], Accuracy : 98.89756944444444 %\n",
      "240\n",
      "Epoch [14/15], Step [240/938], Loss: 0.0274\n",
      "Epoch [14/15], Accuracy : 98.84765625 %\n",
      "300\n",
      "Epoch [14/15], Step [300/938], Loss: 0.0073\n",
      "Epoch [14/15], Accuracy : 98.78645833333333 %\n",
      "360\n",
      "Epoch [14/15], Step [360/938], Loss: 0.1193\n",
      "Epoch [14/15], Accuracy : 98.73697916666667 %\n",
      "420\n",
      "Epoch [14/15], Step [420/938], Loss: 0.0716\n",
      "Epoch [14/15], Accuracy : 98.75744047619048 %\n",
      "480\n",
      "Epoch [14/15], Step [480/938], Loss: 0.0396\n",
      "Epoch [14/15], Accuracy : 98.76953125 %\n",
      "540\n",
      "Epoch [14/15], Step [540/938], Loss: 0.0187\n",
      "Epoch [14/15], Accuracy : 98.75578703703704 %\n",
      "600\n",
      "Epoch [14/15], Step [600/938], Loss: 0.0823\n",
      "Epoch [14/15], Accuracy : 98.75520833333333 %\n",
      "660\n",
      "Epoch [14/15], Step [660/938], Loss: 0.0250\n",
      "Epoch [14/15], Accuracy : 98.76893939393939 %\n",
      "720\n",
      "Epoch [14/15], Step [720/938], Loss: 0.0261\n",
      "Epoch [14/15], Accuracy : 98.78038194444444 %\n",
      "780\n",
      "Epoch [14/15], Step [780/938], Loss: 0.0298\n",
      "Epoch [14/15], Accuracy : 98.78205128205128 %\n",
      "840\n",
      "Epoch [14/15], Step [840/938], Loss: 0.0165\n",
      "Epoch [14/15], Accuracy : 98.77790178571429 %\n",
      "900\n",
      "Epoch [14/15], Step [900/938], Loss: 0.0611\n",
      "Epoch [14/15], Accuracy : 98.78819444444444 %\n",
      "60\n",
      "Epoch [15/15], Step [60/938], Loss: 0.0556\n",
      "Epoch [15/15], Accuracy : 98.93229166666667 %\n",
      "Max Epoch Reached\n",
      "120\n",
      "Epoch [15/15], Step [120/938], Loss: 0.0014\n",
      "Epoch [15/15], Accuracy : 98.984375 %\n",
      "Max Epoch Reached\n",
      "180\n",
      "Epoch [15/15], Step [180/938], Loss: 0.0376\n",
      "Epoch [15/15], Accuracy : 98.97569444444444 %\n",
      "Max Epoch Reached\n",
      "240\n",
      "Epoch [15/15], Step [240/938], Loss: 0.0827\n",
      "Epoch [15/15], Accuracy : 98.97135416666667 %\n",
      "Max Epoch Reached\n",
      "300\n",
      "Epoch [15/15], Step [300/938], Loss: 0.0139\n",
      "Epoch [15/15], Accuracy : 98.95833333333333 %\n",
      "Max Epoch Reached\n",
      "360\n",
      "Epoch [15/15], Step [360/938], Loss: 0.0390\n",
      "Epoch [15/15], Accuracy : 98.94965277777777 %\n",
      "Max Epoch Reached\n",
      "420\n",
      "Epoch [15/15], Step [420/938], Loss: 0.0043\n",
      "Epoch [15/15], Accuracy : 98.90625 %\n",
      "Max Epoch Reached\n",
      "480\n",
      "Epoch [15/15], Step [480/938], Loss: 0.0028\n",
      "Epoch [15/15], Accuracy : 98.857421875 %\n",
      "Max Epoch Reached\n",
      "540\n",
      "Epoch [15/15], Step [540/938], Loss: 0.0703\n",
      "Epoch [15/15], Accuracy : 98.85995370370371 %\n",
      "Max Epoch Reached\n",
      "600\n",
      "Epoch [15/15], Step [600/938], Loss: 0.0782\n",
      "Epoch [15/15], Accuracy : 98.84635416666667 %\n",
      "Max Epoch Reached\n",
      "660\n",
      "Epoch [15/15], Step [660/938], Loss: 0.0222\n",
      "Epoch [15/15], Accuracy : 98.8565340909091 %\n",
      "Max Epoch Reached\n",
      "720\n",
      "Epoch [15/15], Step [720/938], Loss: 0.0101\n",
      "Epoch [15/15], Accuracy : 98.84765625 %\n",
      "Max Epoch Reached\n",
      "780\n",
      "Epoch [15/15], Step [780/938], Loss: 0.1063\n",
      "Epoch [15/15], Accuracy : 98.85216346153847 %\n",
      "Max Epoch Reached\n",
      "840\n",
      "Epoch [15/15], Step [840/938], Loss: 0.0310\n",
      "Epoch [15/15], Accuracy : 98.84114583333333 %\n",
      "Max Epoch Reached\n",
      "900\n",
      "Epoch [15/15], Step [900/938], Loss: 0.0039\n",
      "Epoch [15/15], Accuracy : 98.84895833333333 %\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "B1_train_epoch,B1_train_losses,B1_train_acc = trainFunc(mBatch1,max_epochs,m1train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1559,  0.0223, -0.0872,  ...,  0.1141,  0.0110, -0.1040],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 25550\n"
     ]
    }
   ],
   "source": [
    "batch1_param = torch.nn.utils.parameters_to_vector(mBatch1.parameters())\n",
    "print(batch1_param,'\\nlen:',len(batch1_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model 2 with batch_size=1024 is:25550\n"
     ]
    }
   ],
   "source": [
    "mBatch2 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mBatch2.parameters(), lr=learning_rate, weight_decay = weight_decay_val)\n",
    "\n",
    "a=[]\n",
    "for i in mBatch2.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model 2 with batch_size={1024} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.6546\n",
      "Epoch [1/15], Accuracy : 55.07 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.3551\n",
      "Epoch [2/15], Accuracy : 86.2 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.2597\n",
      "Epoch [3/15], Accuracy : 90.75166666666667 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.2117\n",
      "Epoch [4/15], Accuracy : 93.04166666666667 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.2109\n",
      "Epoch [5/15], Accuracy : 94.33 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.1574\n",
      "Epoch [6/15], Accuracy : 95.26 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.1035\n",
      "Epoch [7/15], Accuracy : 95.82833333333333 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.1106\n",
      "Epoch [8/15], Accuracy : 96.19833333333334 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.1013\n",
      "Epoch [9/15], Accuracy : 96.55333333333333 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.1193\n",
      "Epoch [10/15], Accuracy : 96.78166666666667 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.1205\n",
      "Epoch [11/15], Accuracy : 96.89166666666667 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0727\n",
      "Epoch [12/15], Accuracy : 97.14666666666666 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.1032\n",
      "Epoch [13/15], Accuracy : 97.25833333333334 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0759\n",
      "Epoch [14/15], Accuracy : 97.34833333333333 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0817\n",
      "Epoch [15/15], Accuracy : 97.525 %\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "B2_train_epoch,B2_train_losses,B2_train_acc = trainFunc(mBatch2,max_epochs,m2train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting Loss plot for both Batch 1 and Batch 2 models\n",
    "#B2_train_epoch,B2_train_losses,B2_train_acc\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(B1_train_epoch,B1_train_losses,color=\"blue\")\n",
    "plt.plot(B2_train_epoch,B2_train_losses,color=\"orange\")\n",
    "plt.title('Batch1 & Bathc2 Loss VS Epoch')\n",
    "plt.legend(['Batch 1','Batch 2'])\n",
    "plt.xlabel ('Epoch', color=\"white\")\n",
    "plt.ylabel ('loss', color=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8803587555885315,\n",
       " 0.4854353070259094,\n",
       " 0.3982457220554352,\n",
       " 0.3986225128173828,\n",
       " 0.29269224405288696,\n",
       " 0.3080838918685913,\n",
       " 0.38961708545684814,\n",
       " 0.20308415591716766,\n",
       " 0.2787364721298218,\n",
       " 0.21758873760700226,\n",
       " 0.3322761058807373,\n",
       " 0.2069367915391922,\n",
       " 0.0954047292470932,\n",
       " 0.2551240921020508,\n",
       " 0.17106911540031433,\n",
       " 0.2511993646621704,\n",
       " 0.1343841850757599,\n",
       " 0.1387263834476471,\n",
       " 0.3747082054615021,\n",
       " 0.2082180231809616,\n",
       " 0.14928480982780457,\n",
       " 0.0674978718161583,\n",
       " 0.15526922047138214,\n",
       " 0.08801790326833725,\n",
       " 0.17718470096588135,\n",
       " 0.08239443600177765,\n",
       " 0.13214612007141113,\n",
       " 0.10863715410232544,\n",
       " 0.1137847825884819,\n",
       " 0.12110236287117004,\n",
       " 0.09333270788192749,\n",
       " 0.12969091534614563,\n",
       " 0.0956137478351593,\n",
       " 0.20635290443897247,\n",
       " 0.05234460160136223,\n",
       " 0.05376746878027916,\n",
       " 0.2142188847064972,\n",
       " 0.07461961358785629,\n",
       " 0.39147093892097473,\n",
       " 0.058782968670129776,\n",
       " 0.21505206823349,\n",
       " 0.19943754374980927,\n",
       " 0.11097252368927002,\n",
       " 0.146540105342865,\n",
       " 0.16354353725910187,\n",
       " 0.11348669230937958,\n",
       " 0.0972476601600647,\n",
       " 0.16188684105873108,\n",
       " 0.08127311617136002,\n",
       " 0.028460105881094933,\n",
       " 0.0986299142241478,\n",
       " 0.3631535768508911,\n",
       " 0.05618288740515709,\n",
       " 0.19563160836696625,\n",
       " 0.018479345366358757,\n",
       " 0.09214845299720764,\n",
       " 0.08317448198795319,\n",
       " 0.028354257345199585,\n",
       " 0.06847579032182693,\n",
       " 0.08410294353961945,\n",
       " 0.05444774031639099,\n",
       " 0.05677483230829239,\n",
       " 0.06477148085832596,\n",
       " 0.018078157678246498,\n",
       " 0.04402914643287659,\n",
       " 0.044543519616127014,\n",
       " 0.04474524036049843,\n",
       " 0.1661371886730194,\n",
       " 0.19768723845481873,\n",
       " 0.04196130484342575,\n",
       " 0.0321444496512413,\n",
       " 0.13980524241924286,\n",
       " 0.05066803842782974,\n",
       " 0.029713783413171768,\n",
       " 0.059014249593019485,\n",
       " 0.013826063834130764,\n",
       " 0.008283249102532864,\n",
       " 0.13451500236988068,\n",
       " 0.05477268248796463,\n",
       " 0.006497327703982592,\n",
       " 0.08704324811697006,\n",
       " 0.03602134808897972,\n",
       " 0.03024185448884964,\n",
       " 0.08225524425506592,\n",
       " 0.029356298968195915,\n",
       " 0.0589882992208004,\n",
       " 0.014756128191947937,\n",
       " 0.09265129268169403,\n",
       " 0.05675680190324783,\n",
       " 0.05107342451810837,\n",
       " 0.28532013297080994,\n",
       " 0.024265889078378677,\n",
       " 0.006415742449462414,\n",
       " 0.12587732076644897,\n",
       " 0.08000361174345016,\n",
       " 0.007594630122184753,\n",
       " 0.06654802709817886,\n",
       " 0.20295073091983795,\n",
       " 0.034980304539203644,\n",
       " 0.07294673472642899,\n",
       " 0.010968068614602089,\n",
       " 0.08876438438892365,\n",
       " 0.0018974230624735355,\n",
       " 0.14675886929035187,\n",
       " 0.038922592997550964,\n",
       " 0.014852660708129406,\n",
       " 0.0031395740807056427,\n",
       " 0.06773187965154648,\n",
       " 0.0667804479598999,\n",
       " 0.013729763217270374,\n",
       " 0.02872788906097412,\n",
       " 0.02889079414308071,\n",
       " 0.02697151154279709,\n",
       " 0.01647474244236946,\n",
       " 0.20088741183280945,\n",
       " 0.02901655063033104,\n",
       " 0.02317981794476509,\n",
       " 0.04647330939769745,\n",
       " 0.038862377405166626,\n",
       " 0.09970226138830185,\n",
       " 0.027651114389300346,\n",
       " 0.007764213718473911,\n",
       " 0.014278789982199669,\n",
       " 0.02275385521352291,\n",
       " 0.041282836347818375,\n",
       " 0.009755478240549564,\n",
       " 0.010024553164839745,\n",
       " 0.008410472422838211,\n",
       " 0.055848922580480576,\n",
       " 0.18544061481952667,\n",
       " 0.20172327756881714,\n",
       " 0.028145454823970795,\n",
       " 0.0038196661043912172,\n",
       " 0.060152314603328705,\n",
       " 0.13562460243701935,\n",
       " 0.1849575936794281,\n",
       " 0.025549326092004776,\n",
       " 0.0424501933157444,\n",
       " 0.016957962885499,\n",
       " 0.004420239478349686,\n",
       " 0.04802199453115463,\n",
       " 0.018961293622851372,\n",
       " 0.024470975622534752,\n",
       " 0.0650458112359047,\n",
       " 0.053159549832344055,\n",
       " 0.10654892027378082,\n",
       " 0.1364217847585678,\n",
       " 0.006780565716326237,\n",
       " 0.03519931808114052,\n",
       " 0.03296932950615883,\n",
       " 0.040866002440452576,\n",
       " 0.09565211087465286,\n",
       " 0.018372397869825363,\n",
       " 0.00554763013496995,\n",
       " 0.0052626971155405045,\n",
       " 0.004453946836292744,\n",
       " 0.05083855986595154,\n",
       " 0.012096266262233257,\n",
       " 0.050019290298223495,\n",
       " 0.0958828404545784,\n",
       " 0.004907162394374609,\n",
       " 0.050874266773462296,\n",
       " 0.024006055667996407,\n",
       " 0.008259988389909267,\n",
       " 0.07632020115852356,\n",
       " 0.07244814932346344,\n",
       " 0.17027416825294495,\n",
       " 0.13385385274887085,\n",
       " 0.009848058223724365,\n",
       " 0.017839279025793076,\n",
       " 0.07355691492557526,\n",
       " 0.014934401959180832,\n",
       " 0.15210580825805664,\n",
       " 0.011092872358858585,\n",
       " 0.008506644517183304,\n",
       " 0.07687478512525558,\n",
       " 0.09284094721078873,\n",
       " 0.05574864521622658,\n",
       " 0.005803748965263367,\n",
       " 0.023347973823547363,\n",
       " 0.0049739135429263115,\n",
       " 0.09244029223918915,\n",
       " 0.006885493639856577,\n",
       " 0.10338647663593292,\n",
       " 0.0051638176664710045,\n",
       " 0.05019517242908478,\n",
       " 0.008769713342189789,\n",
       " 0.02449355460703373,\n",
       " 0.005882214289158583,\n",
       " 0.023616041988134384,\n",
       " 0.023160206153988838,\n",
       " 0.009371975436806679,\n",
       " 0.031860239803791046,\n",
       " 0.09696841984987259,\n",
       " 0.0034895949065685272,\n",
       " 0.027552787214517593,\n",
       " 0.007988779805600643,\n",
       " 0.017862839624285698,\n",
       " 0.02741347812116146,\n",
       " 0.00726534565910697,\n",
       " 0.11930437386035919,\n",
       " 0.07160641252994537,\n",
       " 0.039615146815776825,\n",
       " 0.01865091733634472,\n",
       " 0.08232971280813217,\n",
       " 0.02500852569937706,\n",
       " 0.026075171306729317,\n",
       " 0.029795972630381584,\n",
       " 0.016521025449037552,\n",
       " 0.06106876581907272,\n",
       " 0.05558127164840698,\n",
       " 0.0014387776609510183,\n",
       " 0.03764592111110687,\n",
       " 0.08273893594741821,\n",
       " 0.013855601660907269,\n",
       " 0.03897633031010628,\n",
       " 0.004273596685379744,\n",
       " 0.002818757202476263,\n",
       " 0.07026694715023041,\n",
       " 0.07821045815944672,\n",
       " 0.02217990905046463,\n",
       " 0.010059169493615627,\n",
       " 0.10631853342056274,\n",
       " 0.03097604028880596,\n",
       " 0.003917556721717119]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B1_train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3929,  0.3047,  0.2116,  ..., -0.0465, -0.0207, -0.0538],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 25550\n"
     ]
    }
   ],
   "source": [
    "batch2_param = torch.nn.utils.parameters_to_vector(mBatch2.parameters())\n",
    "print(batch2_param,'\\nlen:',len(batch2_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.  -1.8 -1.6 -1.4 -1.2 -1.  -0.8 -0.6 -0.4 -0.2  0.   0.2  0.4  0.6\n",
      "  0.8  1.   1.2  1.4  1.6  1.8  2. ]\n"
     ]
    }
   ],
   "source": [
    "alpha = np.linspace(-2.0, 2.0, num=21)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaArr =[]\n",
    "for i in range (len(alpha)):\n",
    "    theta = (1-alpha[i])*batch1_param + alpha[i]*batch2_param\n",
    "    thetaArr.append(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThetaModel(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(ThetaModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size) #1st Convolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)   #pool_size=2, strides=2 \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size) #2nd Convolution\n",
    "        self.fc1 = nn.Linear(320, 50) #((I/P - Filter + 2*Pad)/Stride)+1 \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(50, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 320)            #Flattening \n",
    "        x = F.relu(self.fc1(x))        #Fully Connected NN   \n",
    "        x = self.dropout(x)   \n",
    "        x = F.relu(self.fc2(x))        #Fully Connected NN           \n",
    "        x = self.fc3(x)                #O/P Layer       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[[ 0.1749,  0.1401,  0.0354, -0.0394],\n",
      "          [ 0.0261, -0.0507, -0.0003, -0.1037],\n",
      "          [-0.2304,  0.2126,  0.1984,  0.2119],\n",
      "          [-0.2380, -0.1541,  0.1360, -0.2074]]],\n",
      "\n",
      "\n",
      "        [[[-0.1243, -0.1807,  0.1552,  0.0601],\n",
      "          [ 0.0297, -0.0783, -0.1552,  0.0180],\n",
      "          [-0.1104, -0.0260, -0.1989,  0.0141],\n",
      "          [-0.2059,  0.2340, -0.1889, -0.0818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0332,  0.2159, -0.2215, -0.2327],\n",
      "          [ 0.0986, -0.0780,  0.0812, -0.0790],\n",
      "          [ 0.1439,  0.2488,  0.1737, -0.1069],\n",
      "          [-0.2298,  0.2033,  0.1610, -0.0870]]],\n",
      "\n",
      "\n",
      "        [[[-0.0910, -0.2244, -0.2077, -0.0398],\n",
      "          [ 0.0486,  0.0881,  0.1672, -0.2101],\n",
      "          [ 0.2241, -0.0039,  0.0468, -0.0176],\n",
      "          [-0.1386,  0.2138,  0.0341, -0.1309]]],\n",
      "\n",
      "\n",
      "        [[[-0.0168, -0.1868, -0.0700,  0.0189],\n",
      "          [ 0.0152, -0.0141,  0.1880,  0.1467],\n",
      "          [-0.2329,  0.1140,  0.1462,  0.0217],\n",
      "          [-0.0369, -0.1947,  0.0833, -0.2405]]],\n",
      "\n",
      "\n",
      "        [[[-0.0931,  0.2026,  0.0868,  0.0153],\n",
      "          [-0.1884, -0.2405,  0.1148, -0.1692],\n",
      "          [ 0.2216,  0.1108,  0.1545,  0.2274],\n",
      "          [-0.2174,  0.0545,  0.2300, -0.1809]]],\n",
      "\n",
      "\n",
      "        [[[-0.0057, -0.1960, -0.0473, -0.0943],\n",
      "          [ 0.1605,  0.2090, -0.1826, -0.1907],\n",
      "          [ 0.0067, -0.1707, -0.1856,  0.1052],\n",
      "          [ 0.2031, -0.2186, -0.1528, -0.0088]]],\n",
      "\n",
      "\n",
      "        [[[-0.0634,  0.2303,  0.2113, -0.0423],\n",
      "          [ 0.1778, -0.2061, -0.0525,  0.1429],\n",
      "          [-0.1306, -0.0074, -0.0388, -0.0156],\n",
      "          [-0.1902,  0.1552,  0.1916,  0.2497]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0034, -0.1644, -0.0813,  0.2118],\n",
      "          [ 0.0825,  0.0433,  0.1071,  0.0525],\n",
      "          [ 0.1049,  0.1858,  0.0043,  0.1702],\n",
      "          [ 0.0612, -0.1083,  0.1548,  0.0741]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2194, -0.0939,  0.0378, -0.0580],\n",
      "          [ 0.2228, -0.1408, -0.0540, -0.1961],\n",
      "          [-0.1051,  0.1068,  0.0308,  0.0522],\n",
      "          [-0.0994,  0.2257,  0.0656,  0.2402]]]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1586,  0.0549,  0.0834, -0.2203,  0.0696, -0.0525,  0.1338,  0.1471,\n",
      "         0.1064,  0.1103], requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 6.9220e-02, -6.7766e-02,  6.6617e-02, -6.5522e-02],\n",
      "          [-6.3670e-02, -4.0960e-02,  5.8705e-02, -2.4299e-02],\n",
      "          [-5.1536e-02,  1.4011e-02,  4.0191e-02, -3.5002e-03],\n",
      "          [ 5.2185e-02, -2.9960e-02,  7.9262e-03, -1.9099e-03]],\n",
      "\n",
      "         [[ 7.7933e-02,  1.8047e-02,  5.9856e-02, -1.0444e-02],\n",
      "          [ 2.5705e-02, -7.3055e-02,  7.7710e-02,  5.5797e-02],\n",
      "          [ 6.6594e-02, -2.9834e-02,  1.8622e-02,  1.5992e-03],\n",
      "          [-4.5569e-02,  9.4181e-03,  5.9558e-03,  6.0703e-02]],\n",
      "\n",
      "         [[-2.3011e-02, -6.8994e-02,  4.9331e-02, -7.4090e-02],\n",
      "          [ 6.6275e-02,  5.9598e-02,  3.6049e-03, -5.2476e-02],\n",
      "          [-1.4333e-02,  4.2530e-02,  3.2789e-02,  5.2600e-02],\n",
      "          [-8.4854e-03, -3.5936e-02, -5.4174e-02,  5.2874e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9662e-02, -6.2535e-02,  1.5864e-02,  3.2547e-02],\n",
      "          [ 1.8780e-02,  1.2016e-02, -4.4840e-02,  4.5602e-02],\n",
      "          [ 7.8979e-02, -3.1271e-02,  3.5641e-02, -3.9507e-02],\n",
      "          [ 1.2595e-02,  7.7496e-02, -6.0114e-02, -7.5819e-02]],\n",
      "\n",
      "         [[ 7.8894e-02, -1.0321e-03, -4.6694e-03,  6.4497e-02],\n",
      "          [-5.0278e-02, -2.3977e-02, -1.4251e-02,  4.1581e-02],\n",
      "          [ 8.3115e-03,  2.2394e-02,  5.7523e-02,  1.5570e-02],\n",
      "          [-6.8546e-02,  4.7902e-02,  6.4896e-02, -4.7507e-02]],\n",
      "\n",
      "         [[-6.0235e-04,  7.6144e-03, -7.4201e-02,  5.7591e-02],\n",
      "          [ 6.2386e-02, -2.3385e-02, -1.5999e-02, -5.2436e-02],\n",
      "          [ 5.8652e-02, -5.7980e-03,  1.9661e-02,  1.1832e-02],\n",
      "          [-7.5464e-02,  4.4388e-02,  2.3856e-02,  6.9237e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.6959e-02,  4.5679e-02, -5.4813e-02,  3.4568e-02],\n",
      "          [ 2.3185e-02,  4.9971e-03, -6.8767e-02,  1.7783e-02],\n",
      "          [-5.4185e-03, -1.9823e-02, -1.2090e-02, -1.6016e-02],\n",
      "          [ 3.5867e-02,  4.4019e-02,  4.9128e-02, -7.6666e-02]],\n",
      "\n",
      "         [[ 4.5672e-02,  3.3033e-02, -7.2564e-03, -4.0845e-02],\n",
      "          [-1.9121e-02, -8.6856e-03,  1.2077e-02,  3.3190e-02],\n",
      "          [-6.8759e-02, -6.6263e-03, -6.1959e-02,  4.5887e-02],\n",
      "          [ 4.9294e-02,  1.5838e-02,  7.9011e-03, -5.9934e-02]],\n",
      "\n",
      "         [[ 9.6861e-03, -6.2588e-03,  5.4709e-02, -5.3747e-02],\n",
      "          [ 4.0915e-02, -4.7397e-02,  3.7951e-02,  6.5863e-03],\n",
      "          [-2.1731e-02, -1.9900e-02, -3.8426e-02,  1.7620e-02],\n",
      "          [ 4.4751e-02,  5.0668e-02,  3.8914e-02,  1.9886e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.3977e-02,  5.2558e-02,  7.5942e-02,  1.3872e-02],\n",
      "          [-5.9450e-02, -7.2396e-02, -6.5603e-03,  2.4065e-02],\n",
      "          [ 2.7229e-02, -7.7273e-03,  2.4233e-02,  1.1044e-02],\n",
      "          [-4.2353e-02, -4.8871e-02,  1.2652e-02, -6.7714e-02]],\n",
      "\n",
      "         [[-1.5270e-02, -3.0420e-02,  7.0505e-02, -5.1132e-02],\n",
      "          [-6.8595e-02, -5.4365e-02,  6.2857e-02, -7.2785e-02],\n",
      "          [ 9.1060e-03, -4.7264e-02,  6.3060e-02,  2.4548e-02],\n",
      "          [ 2.5577e-02,  2.2957e-02, -3.4638e-02,  6.6970e-02]],\n",
      "\n",
      "         [[-2.1706e-03, -2.0677e-02, -7.4874e-02, -2.3380e-02],\n",
      "          [ 6.4056e-03,  6.2708e-02, -3.3678e-02,  2.5657e-02],\n",
      "          [-1.6112e-02,  7.1438e-02, -1.6944e-02, -2.7276e-02],\n",
      "          [ 3.9658e-02, -2.5986e-03, -1.1180e-02, -5.8529e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.9414e-02,  6.0438e-02,  4.5485e-02, -7.7643e-03],\n",
      "          [-5.6713e-03, -6.0481e-02,  5.8684e-02,  5.2762e-02],\n",
      "          [-7.4957e-02, -7.8642e-03,  4.0281e-03, -7.6845e-02],\n",
      "          [ 2.2730e-02,  4.6909e-03,  6.7534e-02, -4.4122e-02]],\n",
      "\n",
      "         [[ 1.4400e-02, -2.1254e-02, -3.1376e-02,  2.5234e-02],\n",
      "          [-6.7548e-02,  4.2362e-02,  5.8262e-02,  4.1228e-02],\n",
      "          [-3.6741e-02, -3.4560e-02,  1.5743e-02, -8.3032e-03],\n",
      "          [-6.5219e-02, -2.7455e-02, -1.7423e-02,  8.4888e-03]],\n",
      "\n",
      "         [[-5.5352e-02, -3.6098e-02,  2.5173e-02, -6.9125e-02],\n",
      "          [-3.8142e-02, -2.3970e-02, -6.0006e-02, -2.2723e-02],\n",
      "          [-3.8964e-02,  4.6366e-02,  6.4428e-02,  5.9830e-02],\n",
      "          [ 4.6967e-02,  5.9388e-02,  3.8410e-02,  7.3703e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.8998e-02,  1.8154e-02, -4.9595e-02, -1.1128e-02],\n",
      "          [ 6.2553e-02,  6.9788e-03, -2.9834e-03, -4.1873e-02],\n",
      "          [ 2.6929e-02,  5.0403e-02, -6.7838e-02, -2.9338e-02],\n",
      "          [-1.4590e-02,  1.4528e-02,  4.6825e-02,  3.9715e-02]],\n",
      "\n",
      "         [[ 3.3164e-02, -2.4287e-02, -3.3060e-02,  1.1193e-02],\n",
      "          [-5.8917e-02,  7.7143e-02,  5.0236e-03,  4.5737e-02],\n",
      "          [-3.1122e-02,  1.0603e-02,  4.6201e-02,  5.6463e-02],\n",
      "          [ 4.2166e-03,  2.7715e-02, -4.6629e-02, -1.0956e-03]],\n",
      "\n",
      "         [[ 2.5079e-04, -8.9846e-03, -1.4787e-02, -7.5707e-02],\n",
      "          [-2.3621e-02,  3.1736e-02,  6.2353e-02,  6.7052e-02],\n",
      "          [-2.1257e-02, -6.7208e-02, -7.2844e-02, -2.6164e-02],\n",
      "          [ 5.8875e-02, -5.1818e-02, -7.2920e-02,  7.7863e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.9511e-02, -3.1209e-02,  2.9777e-02,  2.9657e-02],\n",
      "          [-4.7794e-02, -6.4019e-02,  5.5702e-02, -4.1566e-02],\n",
      "          [-8.1391e-03, -2.3482e-02,  7.6795e-02, -1.5654e-02],\n",
      "          [-1.1656e-02,  2.0600e-02,  6.2949e-02,  7.1129e-02]],\n",
      "\n",
      "         [[-1.9974e-02, -2.6604e-02,  9.9902e-03, -2.2713e-03],\n",
      "          [ 2.5660e-02,  5.9689e-02,  7.3345e-02, -6.5402e-02],\n",
      "          [-7.8364e-02, -4.3462e-02, -2.7965e-03,  3.9552e-02],\n",
      "          [-4.3026e-02, -9.0652e-03, -4.9831e-02, -2.6479e-02]],\n",
      "\n",
      "         [[-7.7436e-02, -4.5996e-02, -1.9273e-02, -3.7779e-02],\n",
      "          [-5.9679e-04, -4.2909e-02, -7.4006e-02, -6.7208e-02],\n",
      "          [-6.3101e-02, -1.5225e-02,  7.8540e-02,  4.1202e-02],\n",
      "          [ 3.1411e-02,  4.7951e-02,  4.2152e-02, -3.5657e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5295e-02,  4.7926e-02, -4.4442e-02,  7.0353e-02],\n",
      "          [-1.5802e-02,  4.4835e-02, -3.4829e-02,  5.5331e-02],\n",
      "          [-2.7610e-02,  6.4827e-02,  7.2162e-02,  2.8393e-03],\n",
      "          [ 3.1282e-02, -7.3523e-02, -1.3058e-02, -6.9314e-02]],\n",
      "\n",
      "         [[-6.8377e-02,  7.2817e-02,  2.7023e-02,  5.0595e-02],\n",
      "          [ 6.3034e-02,  5.1928e-02, -4.8670e-02,  5.3571e-03],\n",
      "          [-7.6310e-02,  4.2829e-02, -6.8903e-02,  1.4902e-02],\n",
      "          [-6.4315e-02, -1.6050e-02,  2.0386e-02, -4.6225e-03]],\n",
      "\n",
      "         [[ 6.2883e-02,  7.5300e-02, -3.7375e-02,  4.8502e-02],\n",
      "          [-4.9056e-02, -6.8683e-02,  5.5990e-03,  2.7135e-02],\n",
      "          [ 6.0227e-02, -6.0167e-02, -4.0501e-02, -7.8207e-02],\n",
      "          [-5.8101e-02,  1.3783e-02, -1.0890e-03,  5.4316e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5168e-02, -7.7313e-02,  5.9682e-02,  3.3334e-02],\n",
      "          [-1.3294e-02, -1.3090e-02, -4.7043e-02,  7.1583e-02],\n",
      "          [-3.6388e-02,  7.5987e-02, -2.8464e-02,  2.0851e-02],\n",
      "          [-7.6428e-02, -2.2795e-02,  6.8435e-02, -1.2018e-02]],\n",
      "\n",
      "         [[-3.1571e-02, -1.6346e-02, -2.5097e-02, -6.4183e-02],\n",
      "          [-3.6171e-02, -3.2521e-03, -3.4817e-02, -4.4410e-02],\n",
      "          [-7.1927e-02, -7.7578e-02, -2.9583e-02,  6.7497e-02],\n",
      "          [-1.0990e-02, -3.6097e-02, -2.4317e-02,  7.2738e-02]],\n",
      "\n",
      "         [[ 1.4360e-02, -4.5626e-02,  6.0985e-02,  1.1655e-02],\n",
      "          [ 4.9110e-02,  2.8132e-02,  3.1488e-02, -1.1306e-02],\n",
      "          [-5.5474e-02, -6.8732e-02,  3.3915e-02,  5.3097e-02],\n",
      "          [ 3.7451e-02,  6.8835e-02, -3.3486e-02,  2.0025e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3015e-03,  4.2441e-02, -1.3780e-02,  7.7096e-02],\n",
      "          [-2.9343e-02, -1.9430e-02,  5.1986e-03, -5.9081e-02],\n",
      "          [ 5.0924e-02, -4.9502e-02,  5.1306e-02,  3.2898e-02],\n",
      "          [ 1.0637e-02,  8.9299e-03,  4.9412e-02, -5.3827e-03]],\n",
      "\n",
      "         [[ 5.8190e-02, -7.3828e-03,  5.9339e-02,  1.2105e-02],\n",
      "          [-3.4919e-02, -2.7684e-02,  4.1910e-02,  5.2760e-02],\n",
      "          [ 2.0696e-02, -5.3866e-02, -3.4311e-02,  4.8856e-02],\n",
      "          [ 6.8731e-02, -9.5047e-05,  2.4931e-02,  3.7855e-02]],\n",
      "\n",
      "         [[ 2.8869e-02,  3.4291e-02, -5.2873e-02,  2.6171e-02],\n",
      "          [-2.3622e-03, -5.0289e-02,  6.2259e-02, -4.9659e-02],\n",
      "          [ 3.4687e-02,  3.6955e-02,  3.3420e-02, -5.8122e-02],\n",
      "          [ 1.9616e-02, -6.2665e-04, -3.6858e-02, -7.6000e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.5817e-03, -1.8312e-04,  2.1418e-02,  4.4842e-02],\n",
      "          [ 6.8317e-02,  6.9426e-02, -3.6963e-02, -4.5246e-02],\n",
      "          [ 3.5214e-02,  6.0167e-03,  3.7584e-02,  7.7385e-02],\n",
      "          [-1.3639e-02,  1.7105e-02,  6.0162e-02,  7.4311e-02]],\n",
      "\n",
      "         [[ 2.8304e-02, -1.5317e-02, -3.2011e-02,  7.7305e-02],\n",
      "          [ 4.4893e-02, -1.5235e-02, -4.8115e-02, -6.6289e-03],\n",
      "          [ 3.6742e-02, -9.3230e-03,  7.7919e-02,  1.4141e-02],\n",
      "          [-1.4979e-03,  7.6712e-02,  1.1623e-02,  5.2634e-02]],\n",
      "\n",
      "         [[-1.7648e-02, -5.3219e-02,  3.7217e-02,  6.2800e-02],\n",
      "          [-5.9143e-02,  2.1339e-02, -1.7165e-03,  5.3499e-02],\n",
      "          [ 2.9689e-02, -1.5165e-03, -7.2838e-02,  1.4776e-02],\n",
      "          [-4.7815e-02,  1.6428e-02, -3.9385e-02,  1.1333e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1937e-02,  3.6381e-02,  3.4760e-02,  7.2211e-02],\n",
      "          [ 1.0653e-02,  4.2741e-02,  3.3779e-02, -1.6658e-03],\n",
      "          [-3.0625e-02,  4.9067e-02, -5.6172e-02, -3.1588e-02],\n",
      "          [-3.1641e-03, -6.2146e-02,  2.6928e-02,  2.7598e-02]],\n",
      "\n",
      "         [[ 4.4825e-02,  2.1810e-02, -1.2442e-02, -5.8851e-02],\n",
      "          [-7.6130e-02,  7.8943e-02, -5.5558e-02,  7.6005e-02],\n",
      "          [ 2.1915e-02, -2.0109e-02, -2.4500e-02,  3.0022e-02],\n",
      "          [-3.2573e-03, -7.7049e-02,  1.6991e-02,  2.4670e-02]],\n",
      "\n",
      "         [[-1.2655e-02, -5.4122e-02,  6.3798e-02, -2.1315e-02],\n",
      "          [ 1.3879e-02,  6.4698e-02,  2.8612e-02,  5.6866e-02],\n",
      "          [-4.9169e-02,  5.5001e-02,  5.9527e-02,  1.6328e-03],\n",
      "          [-3.3474e-02, -1.8336e-02, -5.1184e-02, -7.6484e-02]]]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0690,  0.0192,  0.0644, -0.0011, -0.0526,  0.0790, -0.0505,  0.0386,\n",
      "        -0.0312, -0.0589, -0.0119,  0.0268, -0.0539,  0.0517, -0.0338, -0.0561,\n",
      "        -0.0747,  0.0063,  0.0780,  0.0280], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0077,  0.0417, -0.0066,  ..., -0.0522, -0.0277, -0.0223],\n",
      "        [ 0.0271, -0.0124,  0.0282,  ...,  0.0072,  0.0433, -0.0270],\n",
      "        [ 0.0241,  0.0118, -0.0533,  ...,  0.0510,  0.0181, -0.0546],\n",
      "        ...,\n",
      "        [ 0.0368,  0.0364, -0.0395,  ..., -0.0497,  0.0534, -0.0355],\n",
      "        [-0.0089,  0.0485, -0.0213,  ...,  0.0308, -0.0514,  0.0339],\n",
      "        [-0.0056, -0.0514, -0.0147,  ...,  0.0360,  0.0546,  0.0553]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0324, -0.0332,  0.0032, -0.0282, -0.0554, -0.0239, -0.0154, -0.0307,\n",
      "         0.0018, -0.0155, -0.0119,  0.0235, -0.0231,  0.0557, -0.0495,  0.0507,\n",
      "        -0.0165, -0.0003,  0.0547,  0.0417,  0.0317, -0.0179, -0.0234,  0.0155,\n",
      "        -0.0310, -0.0210,  0.0420, -0.0249, -0.0323,  0.0002,  0.0555, -0.0411,\n",
      "        -0.0357,  0.0202, -0.0324, -0.0298, -0.0037, -0.0378, -0.0313,  0.0121,\n",
      "        -0.0320, -0.0472,  0.0234,  0.0020,  0.0004,  0.0377, -0.0361, -0.0529,\n",
      "        -0.0122,  0.0145], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1021,  0.0386, -0.0346,  ..., -0.0635,  0.0031,  0.1195],\n",
      "        [-0.0904, -0.0079,  0.0491,  ..., -0.1084,  0.0199,  0.0337],\n",
      "        [ 0.0989, -0.0385, -0.0820,  ..., -0.0521,  0.1335, -0.0045],\n",
      "        ...,\n",
      "        [ 0.0890,  0.0164, -0.0561,  ..., -0.1296,  0.0545, -0.0145],\n",
      "        [ 0.1343, -0.0141, -0.0750,  ...,  0.0035,  0.0581, -0.0912],\n",
      "        [-0.1070,  0.0391,  0.1300,  ..., -0.0871,  0.0611, -0.1137]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0379,  0.0690, -0.0750, -0.0432,  0.0723, -0.0601, -0.0046, -0.1176,\n",
      "         0.0603,  0.1356, -0.1165, -0.0173, -0.0226, -0.1184,  0.0836,  0.0791,\n",
      "        -0.0978,  0.1178,  0.0288,  0.1356, -0.0380,  0.0675, -0.1353, -0.0658,\n",
      "         0.1337, -0.0571,  0.0624, -0.0450,  0.0437,  0.0018, -0.0163,  0.0843,\n",
      "         0.1014,  0.1237, -0.0120,  0.0002,  0.0049, -0.0317,  0.0812,  0.0276,\n",
      "         0.0082,  0.0605, -0.0083,  0.0072, -0.0393,  0.0341, -0.0979,  0.0167,\n",
      "         0.1053,  0.0048,  0.0023, -0.1383, -0.0801, -0.1152,  0.0096, -0.0359,\n",
      "         0.0083,  0.0798, -0.0350,  0.1161,  0.0068,  0.1061,  0.0181, -0.0882,\n",
      "        -0.1311, -0.0449, -0.1026, -0.0574, -0.1077,  0.0291, -0.0372, -0.1302,\n",
      "        -0.1285, -0.0967,  0.1246,  0.0104,  0.1001,  0.0984, -0.0853, -0.0881,\n",
      "        -0.0611, -0.0624, -0.1044, -0.0598, -0.0361, -0.1118, -0.0136,  0.0408,\n",
      "         0.0586,  0.0588,  0.0129, -0.0599, -0.0198,  0.1361, -0.1257, -0.0890,\n",
      "        -0.1230,  0.0413,  0.0225, -0.0436], requires_grad=True), Parameter containing:\n",
      "tensor([[ 2.4399e-02, -5.5626e-02, -5.3799e-02,  6.1994e-02, -9.9254e-02,\n",
      "          9.8612e-02,  2.6381e-02,  8.6499e-02, -5.2907e-02,  2.4707e-02,\n",
      "         -9.8155e-02, -8.3862e-02,  6.9123e-02, -2.0392e-02, -2.5695e-02,\n",
      "         -1.2563e-02, -6.6640e-02, -2.4823e-02,  4.3418e-02, -5.2230e-02,\n",
      "          6.9518e-02,  3.6064e-02, -8.7340e-02,  8.4034e-02,  1.7293e-02,\n",
      "         -9.3529e-02, -8.4836e-02,  1.0294e-02,  2.9135e-02,  1.5049e-02,\n",
      "         -6.7700e-02, -8.5053e-02,  3.5568e-02, -9.7937e-02,  4.3138e-04,\n",
      "         -4.8659e-02,  3.3024e-03, -6.3145e-02, -7.5050e-02, -4.4916e-02,\n",
      "          1.7704e-02, -5.8337e-02, -9.2859e-02,  5.1788e-02,  5.4473e-02,\n",
      "          1.2186e-02,  3.1479e-02,  8.5077e-02, -9.8438e-02,  9.8672e-02,\n",
      "         -2.1852e-02,  4.0790e-02,  6.1343e-02,  3.0166e-02,  5.2162e-02,\n",
      "          8.8261e-02, -6.9407e-02, -8.5757e-02, -5.5415e-02,  2.2075e-02,\n",
      "          1.1734e-03,  2.6711e-02, -7.0555e-02,  6.5761e-02,  3.8515e-02,\n",
      "         -6.8160e-02,  1.0625e-02, -9.9777e-03, -7.0669e-02, -6.8681e-02,\n",
      "         -7.5510e-02, -5.4236e-02, -6.0988e-02, -7.4199e-02,  8.5286e-03,\n",
      "          3.4064e-02,  2.8522e-02, -6.1897e-02,  6.2910e-02,  3.1293e-02,\n",
      "          3.3541e-02, -9.8496e-02,  5.7666e-02,  3.3224e-03, -1.0322e-03,\n",
      "          7.7512e-02,  4.0321e-02, -3.3565e-02,  6.8522e-02,  1.7965e-02,\n",
      "          5.0967e-02,  4.8736e-02, -8.3207e-04,  9.4957e-02,  9.4708e-02,\n",
      "          1.1573e-02, -8.5228e-02, -8.8930e-02,  3.6477e-02,  8.0958e-02],\n",
      "        [ 4.4406e-03, -3.0567e-02,  4.6448e-03,  6.1275e-02,  3.9269e-02,\n",
      "          3.1296e-02, -3.4347e-02, -3.2779e-02, -9.9260e-02, -2.8903e-02,\n",
      "         -9.8961e-02, -8.3648e-02, -5.1460e-02,  2.9089e-02, -1.9632e-02,\n",
      "          4.2641e-02,  5.3486e-02, -4.1916e-02, -9.2349e-02, -5.6047e-02,\n",
      "         -4.8213e-02, -4.9572e-02, -1.0463e-02, -2.7453e-02, -6.0250e-02,\n",
      "         -7.0205e-03, -9.8272e-02,  4.3974e-02,  6.3797e-02,  9.8307e-02,\n",
      "         -5.0067e-02, -3.1184e-03,  2.4445e-02, -1.7526e-02, -7.1079e-02,\n",
      "         -8.8518e-02, -9.4820e-02, -9.5111e-02,  5.9749e-02, -6.3763e-02,\n",
      "          5.9358e-02,  4.9972e-02,  8.7736e-02,  6.4285e-02,  6.2957e-02,\n",
      "         -1.1277e-02,  4.8897e-03, -6.9372e-02, -2.7076e-02, -8.0737e-02,\n",
      "         -5.4780e-02, -2.9455e-02,  4.0800e-02,  1.0738e-02, -6.0142e-02,\n",
      "         -5.7651e-04, -7.1273e-02, -5.5264e-03,  2.3866e-02,  5.7342e-02,\n",
      "         -3.7544e-02,  4.0959e-02, -1.8952e-02, -3.4954e-03,  3.8427e-02,\n",
      "          9.0380e-02,  8.3997e-02, -6.9578e-02, -9.4116e-02,  5.4575e-02,\n",
      "          3.8213e-02,  1.5330e-02, -6.3567e-02,  7.5087e-02, -7.3679e-02,\n",
      "         -9.4790e-03,  9.2419e-02,  5.4100e-02, -4.2718e-02, -7.0127e-02,\n",
      "          8.8909e-02, -4.4796e-02,  8.6536e-02, -9.4158e-02, -9.8046e-02,\n",
      "          7.8495e-02,  5.8852e-02,  2.9578e-02,  8.4098e-02, -8.0402e-02,\n",
      "         -1.6748e-02,  1.7152e-02,  5.7173e-02,  1.8631e-02,  3.2881e-02,\n",
      "         -3.5271e-03,  3.0580e-02,  3.7907e-02, -6.9162e-02,  5.7579e-02],\n",
      "        [-6.6076e-02, -3.0218e-02, -2.7296e-02, -3.0953e-02, -6.7614e-02,\n",
      "         -1.2378e-02, -2.5841e-02, -2.4126e-02, -7.0728e-02, -2.2439e-02,\n",
      "          3.0619e-02, -1.4664e-03,  2.3172e-02,  3.2837e-02, -6.7139e-02,\n",
      "         -6.1284e-02,  5.8204e-02,  1.1426e-02,  9.9108e-05, -9.3286e-02,\n",
      "          8.6661e-02,  3.8467e-02, -8.1534e-02, -8.1629e-02, -8.9920e-02,\n",
      "         -7.9495e-02,  1.9641e-02,  7.6466e-02,  5.5314e-02,  5.2646e-02,\n",
      "         -1.9198e-02,  7.1736e-02,  6.4515e-02,  9.6658e-03,  5.8511e-02,\n",
      "          8.8789e-02, -1.1637e-02,  1.3981e-02,  9.2447e-02, -9.0851e-02,\n",
      "          5.2384e-02,  6.6121e-02,  3.1756e-02,  3.6737e-02,  3.3279e-02,\n",
      "          1.6509e-02,  4.7640e-02,  7.3562e-02,  1.5841e-02, -4.9570e-02,\n",
      "          6.0270e-02,  2.8088e-02, -8.4384e-02, -4.3794e-02, -7.9280e-02,\n",
      "         -7.2203e-02,  2.7152e-03, -9.4413e-02, -3.7571e-02,  7.8704e-02,\n",
      "         -8.1907e-02, -4.9034e-02, -7.3321e-02, -1.4161e-02, -4.2521e-02,\n",
      "         -3.9745e-02, -4.3842e-02, -1.6821e-02,  1.7712e-02,  1.3615e-02,\n",
      "          8.5394e-02, -7.5819e-02,  4.2216e-02,  5.8921e-02, -3.4761e-02,\n",
      "          2.6828e-02,  7.1390e-02, -3.5864e-02,  6.0737e-02,  2.8346e-02,\n",
      "         -8.2355e-02,  5.8316e-02, -8.9185e-02, -2.2576e-02, -9.3641e-02,\n",
      "          9.6934e-02,  1.0217e-02,  3.4616e-02,  4.1216e-02,  4.5619e-02,\n",
      "         -8.9044e-02, -1.8044e-02,  5.0845e-02,  1.8869e-02,  4.0607e-02,\n",
      "         -2.6714e-02,  9.6585e-02,  2.5161e-02,  2.0555e-02, -8.7485e-02],\n",
      "        [-3.9495e-02, -7.6677e-02, -1.7337e-02, -7.2271e-02,  7.5890e-03,\n",
      "         -4.1753e-03,  8.3920e-02,  4.6352e-02, -4.3216e-02, -2.8203e-02,\n",
      "          4.9884e-02, -3.6788e-02,  4.3665e-02, -8.3628e-02,  6.7887e-02,\n",
      "         -6.8689e-02,  6.8291e-04,  3.3998e-02, -7.8660e-02,  8.5675e-02,\n",
      "         -4.9053e-03, -3.8721e-02,  1.7498e-02,  2.4585e-02,  3.6023e-02,\n",
      "          8.2104e-02, -9.7062e-02,  5.5245e-02, -6.8329e-02, -7.4959e-02,\n",
      "         -4.7526e-02, -2.2249e-02,  7.6632e-02,  7.6062e-02,  8.6150e-02,\n",
      "          7.8621e-03, -8.0455e-02,  9.2916e-02,  1.4383e-02, -7.7811e-02,\n",
      "         -9.1099e-02, -7.3719e-02, -3.0880e-02, -9.1726e-02, -6.0282e-02,\n",
      "         -4.9639e-02,  1.7907e-02,  2.2371e-02,  1.2269e-02, -1.6205e-03,\n",
      "          6.6655e-02,  9.0948e-02,  3.8574e-02, -3.7422e-02, -4.3019e-02,\n",
      "         -6.6761e-02, -3.9705e-02, -4.8062e-02, -2.2544e-02,  8.6443e-02,\n",
      "         -7.4331e-02,  6.5523e-02,  9.2199e-02, -5.5427e-02,  4.7005e-02,\n",
      "         -4.1704e-02, -3.1532e-02, -4.2495e-02, -4.1057e-02, -8.9796e-02,\n",
      "         -7.3501e-02,  1.7502e-02,  2.6880e-02, -9.3953e-02,  1.9299e-03,\n",
      "          1.7074e-02, -3.2322e-02,  1.5085e-02,  8.2629e-02,  2.6623e-02,\n",
      "          6.2437e-02,  8.1204e-02,  3.9349e-02,  5.1190e-02, -4.1461e-02,\n",
      "         -7.2370e-02,  6.6981e-02,  2.1695e-02, -3.7433e-02, -8.0501e-02,\n",
      "         -8.3917e-02, -6.8786e-02, -1.3158e-02, -6.1968e-02,  7.4493e-02,\n",
      "         -5.2782e-02,  4.8741e-02, -1.0077e-02, -9.3274e-02, -1.7747e-02],\n",
      "        [-1.5731e-02,  2.5100e-02,  8.6850e-02, -9.1743e-02,  1.5120e-02,\n",
      "         -2.9010e-02,  5.7886e-02,  7.8245e-02, -8.3421e-02, -6.3369e-02,\n",
      "          6.3734e-02,  1.4445e-02,  9.7422e-03, -3.6383e-02, -1.8258e-03,\n",
      "          5.5054e-02, -3.9506e-03,  9.9808e-02, -4.2310e-02,  8.4330e-02,\n",
      "         -7.8978e-02,  3.5404e-02,  8.5816e-02, -9.7803e-02,  6.3209e-02,\n",
      "          9.6623e-02, -2.3064e-02, -8.1374e-02, -7.4630e-02,  8.3733e-02,\n",
      "          4.3513e-02, -2.9559e-04,  3.9020e-02,  1.2189e-02, -6.8691e-02,\n",
      "          7.6218e-02,  4.8432e-02, -4.2949e-02,  9.4341e-02,  9.4588e-02,\n",
      "         -8.7239e-02,  7.9489e-02, -5.1835e-02, -9.9810e-02,  3.4301e-02,\n",
      "          2.0843e-02, -6.7648e-02,  7.6196e-02,  1.6749e-02,  8.0246e-02,\n",
      "         -8.3294e-02, -3.8279e-02,  5.6948e-02, -7.3416e-03, -6.3781e-02,\n",
      "         -2.4346e-02, -9.4743e-02,  9.2933e-02, -8.3216e-02,  5.7766e-02,\n",
      "          8.0575e-02,  2.7510e-02, -7.9932e-02,  1.1280e-02, -2.2902e-02,\n",
      "         -9.7054e-02, -6.4893e-02, -1.4489e-02, -4.0013e-02,  1.4056e-02,\n",
      "          8.6447e-02,  2.8531e-03, -8.0956e-02,  8.3574e-03,  5.1832e-02,\n",
      "          4.2795e-02, -2.2802e-02,  6.4326e-02, -3.6500e-02, -6.4234e-02,\n",
      "         -5.9862e-02, -9.1008e-02,  3.1642e-02,  6.7585e-03,  9.8776e-02,\n",
      "         -7.6694e-02,  4.5885e-02,  2.0334e-02, -9.1729e-02, -6.4401e-02,\n",
      "          4.1016e-02,  2.0353e-02,  9.4917e-02,  6.1628e-02,  8.3661e-02,\n",
      "          6.1763e-02, -1.2391e-02, -1.7157e-02,  1.7455e-02,  5.9407e-02],\n",
      "        [ 7.8079e-02, -4.5994e-02, -3.2326e-02, -7.5213e-02, -5.9427e-02,\n",
      "          9.6170e-02, -9.7926e-02, -5.7685e-02,  5.5049e-02, -5.5978e-02,\n",
      "         -7.3512e-03,  2.9017e-02, -3.8927e-02, -6.0874e-02, -5.6344e-02,\n",
      "          6.2270e-03, -3.2896e-02, -7.6117e-02,  2.5857e-02,  5.4474e-03,\n",
      "         -5.7328e-02,  1.1949e-02, -2.8076e-02,  3.7444e-02, -2.7999e-02,\n",
      "         -8.4848e-02, -8.4695e-02, -4.9300e-02, -5.8639e-02,  7.8522e-03,\n",
      "         -9.0396e-03, -3.8751e-02,  6.2829e-02,  4.2066e-02,  7.0954e-02,\n",
      "         -1.8514e-02, -4.2455e-02,  2.1505e-02,  4.7796e-03, -5.7205e-02,\n",
      "         -6.7885e-02,  8.5868e-02,  2.0696e-02, -8.1803e-02,  3.3812e-02,\n",
      "          5.8441e-02, -6.2038e-02,  9.3467e-02, -3.8610e-02,  3.2971e-02,\n",
      "         -9.3536e-03,  5.2910e-02,  8.6801e-02,  1.1991e-03,  9.7871e-02,\n",
      "          3.5228e-02, -2.4375e-02, -1.1387e-02,  1.7166e-02,  5.4782e-02,\n",
      "          4.8064e-02, -3.9912e-02,  9.6045e-04,  7.7057e-02,  4.9496e-02,\n",
      "          2.2370e-02,  4.4308e-02,  5.7174e-02,  4.3621e-02,  4.3100e-03,\n",
      "          6.6945e-02, -1.6910e-02,  8.4240e-02,  7.7697e-03, -7.7907e-02,\n",
      "          7.7381e-02,  8.0104e-02, -1.8175e-02,  7.8339e-02,  2.7707e-02,\n",
      "          4.2037e-02, -7.8595e-02,  6.9809e-02, -2.2418e-02, -3.9227e-02,\n",
      "          4.9398e-02,  8.8204e-03,  5.7537e-02, -3.4226e-02,  4.6166e-02,\n",
      "         -3.5174e-02, -2.7852e-02,  2.4051e-02, -4.7759e-02, -3.8692e-02,\n",
      "          3.0142e-02, -4.5840e-02,  2.0596e-02, -2.1785e-02,  9.5018e-02],\n",
      "        [ 3.8830e-02,  8.2857e-02, -2.7000e-02, -8.2686e-02, -2.6420e-02,\n",
      "         -8.4287e-02,  3.7351e-02, -5.7628e-02, -9.5188e-03,  2.2760e-02,\n",
      "         -6.6574e-02,  3.4853e-02, -4.6623e-03, -4.1259e-02, -5.5766e-02,\n",
      "          9.1322e-02, -6.4873e-02, -9.2099e-02, -9.3742e-02, -5.3765e-03,\n",
      "          6.0336e-02, -5.5707e-02, -4.9095e-02,  6.4980e-02, -1.2837e-02,\n",
      "          3.4881e-02,  6.9171e-02,  5.5842e-02,  2.4641e-03,  4.7349e-02,\n",
      "          6.6418e-02,  5.6755e-02, -1.7258e-02, -6.0943e-02,  1.9021e-02,\n",
      "          9.3568e-02, -8.3580e-02,  5.0737e-02,  5.8692e-02,  7.8839e-02,\n",
      "          6.1957e-02, -3.9944e-02,  5.5367e-02, -3.1561e-02,  1.5434e-02,\n",
      "          1.2534e-02,  6.9061e-02, -2.4069e-02,  2.5652e-02, -7.8324e-02,\n",
      "          2.6085e-02, -3.5966e-02,  6.0033e-02, -1.9035e-02,  9.5375e-02,\n",
      "          1.7683e-02, -2.7899e-02, -5.3501e-02, -5.6057e-02, -3.4445e-02,\n",
      "          9.6627e-02, -4.5170e-02, -7.1371e-03,  6.5398e-02,  1.7138e-02,\n",
      "         -8.3889e-03,  1.7973e-03,  2.2345e-02, -7.3448e-05, -3.0755e-02,\n",
      "          8.7376e-02,  9.7442e-02, -4.1924e-02,  8.5917e-02,  4.2684e-02,\n",
      "         -7.8485e-02, -1.5473e-02, -5.9327e-02, -5.2467e-02,  3.8703e-02,\n",
      "          3.1993e-02, -9.1907e-02, -7.3138e-02,  3.6254e-03, -8.0063e-02,\n",
      "          1.9958e-02, -8.0272e-02, -2.9774e-02, -1.4252e-02, -7.1074e-02,\n",
      "         -2.1541e-02,  2.8631e-02, -5.8335e-02,  9.9338e-02, -4.4183e-02,\n",
      "         -3.5926e-03, -8.1976e-02,  9.6735e-02,  6.6363e-02, -2.3236e-02],\n",
      "        [ 7.7146e-02,  4.9509e-02, -8.1888e-02,  6.0299e-02, -4.5815e-02,\n",
      "         -4.6806e-02, -3.6510e-02, -5.7406e-02, -3.8193e-02, -3.6502e-02,\n",
      "          1.6167e-02, -9.8482e-02,  8.5356e-02, -2.4604e-02, -8.1255e-02,\n",
      "          9.7362e-02,  4.5470e-02,  7.7733e-02, -1.3794e-03, -5.4966e-02,\n",
      "          5.3590e-02,  3.5890e-03, -4.1386e-03,  2.6152e-03,  6.5751e-02,\n",
      "          1.7640e-02,  2.4734e-02, -2.6937e-02,  5.4627e-02,  6.5344e-02,\n",
      "          6.5559e-02,  3.1107e-02, -6.1068e-02,  5.9340e-02,  5.6224e-02,\n",
      "         -9.6129e-02,  1.7393e-02, -9.5561e-02,  1.9398e-02, -6.2738e-02,\n",
      "          8.4546e-02, -9.1081e-02, -6.7519e-03,  2.8362e-02,  7.4413e-02,\n",
      "         -3.8209e-03, -7.0538e-02,  7.4104e-02, -7.3558e-02,  1.7664e-02,\n",
      "         -6.5334e-02,  5.5827e-02, -4.1120e-02, -8.4503e-03,  9.3035e-02,\n",
      "         -5.4251e-02,  3.1150e-02, -8.7667e-03,  7.3751e-03, -4.0622e-02,\n",
      "         -7.1087e-02,  8.8826e-02, -6.0277e-02, -3.6873e-03,  7.3837e-02,\n",
      "          5.4097e-02,  6.0093e-02, -5.0806e-03, -9.9910e-02,  8.2122e-02,\n",
      "         -5.9250e-02,  6.5316e-02,  4.4476e-02,  3.5579e-02,  8.2797e-02,\n",
      "          8.2104e-02, -8.7894e-02, -1.4120e-02, -3.4188e-02, -8.4929e-02,\n",
      "         -1.9765e-02, -9.3820e-02,  6.5882e-02, -1.2450e-02, -1.5500e-02,\n",
      "          4.8156e-02, -4.9088e-03, -7.1887e-02,  9.6300e-02,  5.4587e-02,\n",
      "         -2.0181e-02, -5.8076e-02,  6.6187e-03, -9.0945e-02,  6.1882e-02,\n",
      "          8.0136e-03, -5.9585e-02,  9.5094e-02,  3.8260e-03, -1.0984e-02],\n",
      "        [ 8.7213e-02,  7.0496e-02, -2.0576e-02, -4.4563e-02, -9.4359e-02,\n",
      "         -2.6754e-02, -3.9521e-02, -8.3653e-02, -3.6301e-02,  9.7592e-02,\n",
      "         -1.0558e-03,  2.2000e-03,  7.4957e-02, -6.0538e-02, -2.8638e-02,\n",
      "          9.4032e-02, -6.6875e-02,  6.7043e-02, -8.4762e-03, -4.4116e-03,\n",
      "         -3.8789e-02, -4.2974e-02, -1.1214e-02, -6.7161e-02,  5.5507e-02,\n",
      "          6.9884e-02, -8.2658e-02,  2.9751e-02, -1.3750e-02,  8.9241e-02,\n",
      "         -4.6201e-02, -2.4812e-02, -5.9326e-02,  3.3181e-02,  5.4795e-02,\n",
      "          2.1820e-02,  1.7117e-02, -1.4104e-02,  8.3644e-02, -3.4442e-02,\n",
      "         -1.0803e-02,  3.2114e-02,  6.4254e-02,  2.8537e-02, -3.0176e-02,\n",
      "          3.0999e-02,  1.1032e-02, -1.6944e-03, -1.4590e-02,  9.9946e-02,\n",
      "          9.1840e-02,  7.6795e-02, -2.8757e-02,  6.2846e-02, -6.5315e-02,\n",
      "         -6.4721e-02, -7.1018e-02, -2.1167e-02,  5.7236e-02,  8.4178e-03,\n",
      "         -5.4474e-02,  9.4727e-02,  2.2311e-02,  4.6786e-02,  6.7834e-02,\n",
      "         -8.7860e-02,  7.4562e-02, -8.4844e-02, -8.7467e-02,  8.6762e-02,\n",
      "         -3.0437e-02,  9.6563e-02, -2.5683e-03, -1.5583e-02, -8.9397e-02,\n",
      "          5.7484e-03, -4.5235e-02,  8.7932e-02, -2.6732e-02,  1.8635e-02,\n",
      "         -7.8777e-02, -4.5893e-02, -7.3912e-02,  8.1659e-02,  6.3295e-02,\n",
      "          1.4092e-02, -1.1792e-02,  8.9634e-02, -6.1743e-02, -9.7022e-03,\n",
      "         -2.9952e-02, -4.8774e-02, -8.8250e-02,  5.3038e-02,  9.3281e-02,\n",
      "          4.7171e-03, -6.9409e-02, -3.1937e-02, -3.8546e-02, -2.6795e-02],\n",
      "        [-1.2529e-02, -8.5548e-02, -2.6812e-03,  6.0154e-02, -9.3971e-02,\n",
      "          7.9718e-02, -3.9135e-02,  2.5525e-02, -7.1769e-02,  9.6847e-02,\n",
      "         -7.6892e-04,  1.6738e-02,  6.8143e-02,  5.4179e-02,  6.9584e-02,\n",
      "         -1.4999e-02,  9.5434e-02,  4.6332e-03, -8.0399e-02,  5.4547e-03,\n",
      "         -7.8998e-02,  3.7353e-02, -4.9039e-02,  6.2907e-02,  7.3234e-02,\n",
      "          7.7927e-02,  6.3129e-02, -1.0438e-02, -6.0738e-02, -4.0727e-02,\n",
      "          8.6121e-02, -8.4382e-02,  6.9639e-02, -1.1550e-02, -9.2347e-02,\n",
      "          7.1584e-02,  8.8314e-02, -3.0005e-03, -2.4739e-02,  7.1633e-02,\n",
      "         -3.9286e-02,  3.9610e-02, -6.0800e-02, -6.6257e-02,  9.4990e-02,\n",
      "         -5.4517e-02, -7.7266e-02, -6.8037e-02,  6.3979e-02,  1.1950e-02,\n",
      "         -8.7306e-02,  5.9274e-02,  1.0251e-02,  6.5279e-02,  8.8577e-02,\n",
      "         -6.8143e-02,  8.7029e-02,  4.6770e-02,  4.3102e-02,  2.7478e-02,\n",
      "         -3.5873e-02, -9.1337e-02, -3.0943e-02, -2.0793e-02,  6.7650e-03,\n",
      "         -7.6260e-02, -7.0950e-02,  8.8279e-02,  5.8856e-02,  3.0427e-02,\n",
      "          8.4776e-02, -5.4820e-02, -6.8034e-02,  8.5028e-02, -8.8898e-02,\n",
      "          7.0200e-02,  9.6109e-02, -8.0769e-03, -1.9790e-02,  3.1263e-02,\n",
      "          7.8722e-02, -2.4576e-02,  1.3092e-02, -4.0810e-02,  1.9787e-02,\n",
      "         -2.3099e-02, -3.5965e-02, -9.3477e-02,  3.5961e-02, -6.1979e-02,\n",
      "          3.9203e-02,  5.5435e-02,  9.6470e-02, -8.8394e-02,  6.2043e-02,\n",
      "         -4.6618e-03, -2.2813e-02, -5.5837e-03,  7.0225e-02,  1.3270e-02]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0575, -0.0631,  0.0947, -0.0040,  0.0410, -0.0389, -0.0949, -0.0660,\n",
      "         0.0796, -0.0689], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "mT1 = ThetaModel()\n",
    "print(list(mT1.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.utils.vector_to_parameters(theta1,mT1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(mT1.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mT1 = ThetaModel()\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(mT1.parameters(), lr=1e-3, weight_decay = weight_decay_val)\n",
    "\n",
    "# a=[]\n",
    "# for i in mT1.parameters():\n",
    "#     a.append(torch.numel(i))\n",
    "# print(f'Total no of parameters in Model Theta 1 is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T1_train_epoch,T1_train_losses,T1_train_acc = trainFunc(mT1,max_epochs,m2train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model Theta 0 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 17.9412\n",
      "Epoch [1/15], Accuracy : 89.41166666666666 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 6.5040\n",
      "Epoch [2/15], Accuracy : 95.29 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 4.5902\n",
      "Epoch [3/15], Accuracy : 95.87166666666667 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 5.9312\n",
      "Epoch [4/15], Accuracy : 95.885 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 2.3882\n",
      "Epoch [5/15], Accuracy : 95.925 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 1.9596\n",
      "Epoch [6/15], Accuracy : 95.61333333333333 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 2.9291\n",
      "Epoch [7/15], Accuracy : 95.15333333333334 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 1.8386\n",
      "Epoch [8/15], Accuracy : 94.72166666666666 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 1.7516\n",
      "Epoch [9/15], Accuracy : 94.45666666666666 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 1.0272\n",
      "Epoch [10/15], Accuracy : 94.26333333333334 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.5339\n",
      "Epoch [11/15], Accuracy : 94.13166666666666 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 1.0659\n",
      "Epoch [12/15], Accuracy : 93.91 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 1.8108\n",
      "Epoch [13/15], Accuracy : 93.97 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 1.4695\n",
      "Epoch [14/15], Accuracy : 93.57166666666667 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.6878\n",
      "Epoch [15/15], Accuracy : 93.56666666666666 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 1 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 8.9485\n",
      "Epoch [1/15], Accuracy : 91.09333333333333 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 6.0864\n",
      "Epoch [2/15], Accuracy : 95.85 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 2.4841\n",
      "Epoch [3/15], Accuracy : 96.66333333333333 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 2.9093\n",
      "Epoch [4/15], Accuracy : 96.75666666666666 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 1.8987\n",
      "Epoch [5/15], Accuracy : 96.60666666666667 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 1.0423\n",
      "Epoch [6/15], Accuracy : 96.59666666666666 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 1.5605\n",
      "Epoch [7/15], Accuracy : 96.4 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.6043\n",
      "Epoch [8/15], Accuracy : 96.04833333333333 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 1.1617\n",
      "Epoch [9/15], Accuracy : 95.885 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.8650\n",
      "Epoch [10/15], Accuracy : 95.80833333333334 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.8130\n",
      "Epoch [11/15], Accuracy : 95.65 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.4324\n",
      "Epoch [12/15], Accuracy : 95.97166666666666 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.5148\n",
      "Epoch [13/15], Accuracy : 95.73333333333333 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.4326\n",
      "Epoch [14/15], Accuracy : 95.81166666666667 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.9682\n",
      "Epoch [15/15], Accuracy : 96.075 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 2 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 6.9564\n",
      "Epoch [1/15], Accuracy : 92.45 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 3.4612\n",
      "Epoch [2/15], Accuracy : 96.49833333333333 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 1.9734\n",
      "Epoch [3/15], Accuracy : 96.95666666666666 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 1.6624\n",
      "Epoch [4/15], Accuracy : 96.95166666666667 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 1.1604\n",
      "Epoch [5/15], Accuracy : 96.93333333333334 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.4441\n",
      "Epoch [6/15], Accuracy : 96.75833333333334 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.5112\n",
      "Epoch [7/15], Accuracy : 96.52333333333333 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.5698\n",
      "Epoch [8/15], Accuracy : 96.56666666666666 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.9168\n",
      "Epoch [9/15], Accuracy : 96.535 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.6054\n",
      "Epoch [10/15], Accuracy : 96.48 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.4292\n",
      "Epoch [11/15], Accuracy : 96.21 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.6589\n",
      "Epoch [12/15], Accuracy : 96.39 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.3589\n",
      "Epoch [13/15], Accuracy : 96.445 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.5237\n",
      "Epoch [14/15], Accuracy : 96.64833333333333 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.2547\n",
      "Epoch [15/15], Accuracy : 96.66833333333334 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 3 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 2.1912\n",
      "Epoch [1/15], Accuracy : 93.79666666666667 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 1.3357\n",
      "Epoch [2/15], Accuracy : 97.09166666666667 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 2.0908\n",
      "Epoch [3/15], Accuracy : 97.35 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 1.0160\n",
      "Epoch [4/15], Accuracy : 97.53666666666666 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.4883\n",
      "Epoch [5/15], Accuracy : 97.39166666666667 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.2382\n",
      "Epoch [6/15], Accuracy : 97.495 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.3642\n",
      "Epoch [7/15], Accuracy : 97.095 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.2443\n",
      "Epoch [8/15], Accuracy : 97.0 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.2645\n",
      "Epoch [9/15], Accuracy : 97.09833333333333 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.4159\n",
      "Epoch [10/15], Accuracy : 97.015 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.2445\n",
      "Epoch [11/15], Accuracy : 96.98333333333333 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.2513\n",
      "Epoch [12/15], Accuracy : 97.11 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.2572\n",
      "Epoch [13/15], Accuracy : 97.20166666666667 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.2372\n",
      "Epoch [14/15], Accuracy : 97.37666666666667 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.1987\n",
      "Epoch [15/15], Accuracy : 97.33166666666666 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 4 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 1.6364\n",
      "Epoch [1/15], Accuracy : 95.04833333333333 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.8421\n",
      "Epoch [2/15], Accuracy : 97.425 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.5192\n",
      "Epoch [3/15], Accuracy : 97.65166666666667 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.3049\n",
      "Epoch [4/15], Accuracy : 97.62666666666667 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.3494\n",
      "Epoch [5/15], Accuracy : 97.51166666666667 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.3329\n",
      "Epoch [6/15], Accuracy : 97.35166666666667 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.1671\n",
      "Epoch [7/15], Accuracy : 97.225 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.2212\n",
      "Epoch [8/15], Accuracy : 97.48333333333333 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.1151\n",
      "Epoch [9/15], Accuracy : 97.515 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0838\n",
      "Epoch [10/15], Accuracy : 97.55833333333334 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.1132\n",
      "Epoch [11/15], Accuracy : 97.64 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.1438\n",
      "Epoch [12/15], Accuracy : 97.86166666666666 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0668\n",
      "Epoch [13/15], Accuracy : 97.85 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0868\n",
      "Epoch [14/15], Accuracy : 97.94166666666666 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.1165\n",
      "Epoch [15/15], Accuracy : 98.015 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 5 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.5974\n",
      "Epoch [1/15], Accuracy : 96.14833333333333 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.3527\n",
      "Epoch [2/15], Accuracy : 97.945 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.2597\n",
      "Epoch [3/15], Accuracy : 98.175 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.1290\n",
      "Epoch [4/15], Accuracy : 98.15833333333333 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.1458\n",
      "Epoch [5/15], Accuracy : 98.10333333333334 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.1347\n",
      "Epoch [6/15], Accuracy : 98.13833333333334 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.1304\n",
      "Epoch [7/15], Accuracy : 98.06666666666666 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.1061\n",
      "Epoch [8/15], Accuracy : 98.23666666666666 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.1178\n",
      "Epoch [9/15], Accuracy : 98.24333333333334 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0758\n",
      "Epoch [10/15], Accuracy : 98.31166666666667 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0666\n",
      "Epoch [11/15], Accuracy : 98.465 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0741\n",
      "Epoch [12/15], Accuracy : 98.44333333333333 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0896\n",
      "Epoch [13/15], Accuracy : 98.54833333333333 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0315\n",
      "Epoch [14/15], Accuracy : 98.55166666666666 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0855\n",
      "Epoch [15/15], Accuracy : 98.55666666666667 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 6 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.1637\n",
      "Epoch [1/15], Accuracy : 97.235 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.2506\n",
      "Epoch [2/15], Accuracy : 98.37333333333333 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.0261\n",
      "Epoch [3/15], Accuracy : 98.41833333333334 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.1265\n",
      "Epoch [4/15], Accuracy : 98.51166666666667 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0739\n",
      "Epoch [5/15], Accuracy : 98.55166666666666 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0927\n",
      "Epoch [6/15], Accuracy : 98.50666666666666 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0869\n",
      "Epoch [7/15], Accuracy : 98.65333333333334 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0394\n",
      "Epoch [8/15], Accuracy : 98.70166666666667 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.1426\n",
      "Epoch [9/15], Accuracy : 98.76166666666667 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0460\n",
      "Epoch [10/15], Accuracy : 98.84333333333333 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0728\n",
      "Epoch [11/15], Accuracy : 98.91333333333333 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0390\n",
      "Epoch [12/15], Accuracy : 98.93666666666667 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.1136\n",
      "Epoch [13/15], Accuracy : 98.995 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0341\n",
      "Epoch [14/15], Accuracy : 99.04 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0890\n",
      "Epoch [15/15], Accuracy : 99.09666666666666 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 7 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.1586\n",
      "Epoch [1/15], Accuracy : 98.035 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.1030\n",
      "Epoch [2/15], Accuracy : 98.71833333333333 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.0584\n",
      "Epoch [3/15], Accuracy : 98.82833333333333 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0468\n",
      "Epoch [4/15], Accuracy : 98.90666666666667 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0261\n",
      "Epoch [5/15], Accuracy : 98.955 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0172\n",
      "Epoch [6/15], Accuracy : 98.90833333333333 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0397\n",
      "Epoch [7/15], Accuracy : 99.03833333333333 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0351\n",
      "Epoch [8/15], Accuracy : 99.02 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0326\n",
      "Epoch [9/15], Accuracy : 99.16166666666666 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0285\n",
      "Epoch [10/15], Accuracy : 99.21333333333334 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0183\n",
      "Epoch [11/15], Accuracy : 99.20333333333333 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0438\n",
      "Epoch [12/15], Accuracy : 99.25333333333333 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0475\n",
      "Epoch [13/15], Accuracy : 99.23666666666666 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0299\n",
      "Epoch [14/15], Accuracy : 99.28333333333333 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0145\n",
      "Epoch [15/15], Accuracy : 99.33666666666667 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 8 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.0517\n",
      "Epoch [1/15], Accuracy : 98.75833333333334 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.0744\n",
      "Epoch [2/15], Accuracy : 99.09 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.0391\n",
      "Epoch [3/15], Accuracy : 99.14333333333333 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0365\n",
      "Epoch [4/15], Accuracy : 99.20666666666666 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0485\n",
      "Epoch [5/15], Accuracy : 99.28833333333333 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0178\n",
      "Epoch [6/15], Accuracy : 99.31166666666667 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0246\n",
      "Epoch [7/15], Accuracy : 99.37833333333333 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0369\n",
      "Epoch [8/15], Accuracy : 99.36166666666666 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0076\n",
      "Epoch [9/15], Accuracy : 99.41666666666667 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0206\n",
      "Epoch [10/15], Accuracy : 99.37666666666667 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0173\n",
      "Epoch [11/15], Accuracy : 99.455 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0180\n",
      "Epoch [12/15], Accuracy : 99.47666666666667 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0123\n",
      "Epoch [13/15], Accuracy : 99.47333333333333 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0197\n",
      "Epoch [14/15], Accuracy : 99.5 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0206\n",
      "Epoch [15/15], Accuracy : 99.51333333333334 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 9 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.0189\n",
      "Epoch [1/15], Accuracy : 99.03166666666667 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.0166\n",
      "Epoch [2/15], Accuracy : 99.28 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.0239\n",
      "Epoch [3/15], Accuracy : 99.33 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0103\n",
      "Epoch [4/15], Accuracy : 99.36333333333333 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0131\n",
      "Epoch [5/15], Accuracy : 99.41666666666667 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0087\n",
      "Epoch [6/15], Accuracy : 99.415 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0091\n",
      "Epoch [7/15], Accuracy : 99.475 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0101\n",
      "Epoch [8/15], Accuracy : 99.49166666666666 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0226\n",
      "Epoch [9/15], Accuracy : 99.52833333333334 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0141\n",
      "Epoch [10/15], Accuracy : 99.45666666666666 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0165\n",
      "Epoch [11/15], Accuracy : 99.52833333333334 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0151\n",
      "Epoch [12/15], Accuracy : 99.57833333333333 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0264\n",
      "Epoch [13/15], Accuracy : 99.55 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0128\n",
      "Epoch [14/15], Accuracy : 99.50333333333333 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0104\n",
      "Epoch [15/15], Accuracy : 99.545 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 10 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.0165\n",
      "Epoch [1/15], Accuracy : 99.19333333333333 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.0368\n",
      "Epoch [2/15], Accuracy : 99.36166666666666 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.0127\n",
      "Epoch [3/15], Accuracy : 99.38 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0227\n",
      "Epoch [4/15], Accuracy : 99.40333333333334 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0138\n",
      "Epoch [5/15], Accuracy : 99.4 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0173\n",
      "Epoch [6/15], Accuracy : 99.41333333333333 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0335\n",
      "Epoch [7/15], Accuracy : 99.45666666666666 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0171\n",
      "Epoch [8/15], Accuracy : 99.47833333333334 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0368\n",
      "Epoch [9/15], Accuracy : 99.46166666666667 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0193\n",
      "Epoch [10/15], Accuracy : 99.48333333333333 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0138\n",
      "Epoch [11/15], Accuracy : 99.53333333333333 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0208\n",
      "Epoch [12/15], Accuracy : 99.49666666666667 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0150\n",
      "Epoch [13/15], Accuracy : 99.50166666666667 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0241\n",
      "Epoch [14/15], Accuracy : 99.50833333333334 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0070\n",
      "Epoch [15/15], Accuracy : 99.52333333333333 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 11 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.0309\n",
      "Epoch [1/15], Accuracy : 98.81333333333333 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.0213\n",
      "Epoch [2/15], Accuracy : 99.05666666666667 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.0374\n",
      "Epoch [3/15], Accuracy : 99.14833333333333 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0203\n",
      "Epoch [4/15], Accuracy : 99.23333333333333 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0285\n",
      "Epoch [5/15], Accuracy : 99.28333333333333 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0255\n",
      "Epoch [6/15], Accuracy : 99.275 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0140\n",
      "Epoch [7/15], Accuracy : 99.31833333333333 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0218\n",
      "Epoch [8/15], Accuracy : 99.33833333333334 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0238\n",
      "Epoch [9/15], Accuracy : 99.39166666666667 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0103\n",
      "Epoch [10/15], Accuracy : 99.375 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0146\n",
      "Epoch [11/15], Accuracy : 99.43666666666667 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0208\n",
      "Epoch [12/15], Accuracy : 99.435 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0226\n",
      "Epoch [13/15], Accuracy : 99.49833333333333 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0138\n",
      "Epoch [14/15], Accuracy : 99.50333333333333 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0254\n",
      "Epoch [15/15], Accuracy : 99.465 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 12 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.0891\n",
      "Epoch [1/15], Accuracy : 96.16666666666667 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.0662\n",
      "Epoch [2/15], Accuracy : 98.11 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.0545\n",
      "Epoch [3/15], Accuracy : 98.51 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0400\n",
      "Epoch [4/15], Accuracy : 98.70333333333333 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0568\n",
      "Epoch [5/15], Accuracy : 98.80166666666666 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0299\n",
      "Epoch [6/15], Accuracy : 98.94333333333333 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0433\n",
      "Epoch [7/15], Accuracy : 98.98833333333333 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0306\n",
      "Epoch [8/15], Accuracy : 99.03333333333333 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0265\n",
      "Epoch [9/15], Accuracy : 99.11 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0180\n",
      "Epoch [10/15], Accuracy : 99.15666666666667 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0306\n",
      "Epoch [11/15], Accuracy : 99.21666666666667 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0238\n",
      "Epoch [12/15], Accuracy : 99.21833333333333 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0349\n",
      "Epoch [13/15], Accuracy : 99.20666666666666 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0232\n",
      "Epoch [14/15], Accuracy : 99.29833333333333 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0181\n",
      "Epoch [15/15], Accuracy : 99.325 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 13 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.1317\n",
      "Epoch [1/15], Accuracy : 90.54 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.1101\n",
      "Epoch [2/15], Accuracy : 95.96666666666667 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.1261\n",
      "Epoch [3/15], Accuracy : 96.82333333333334 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0855\n",
      "Epoch [4/15], Accuracy : 97.21 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0519\n",
      "Epoch [5/15], Accuracy : 97.50333333333333 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0635\n",
      "Epoch [6/15], Accuracy : 97.71166666666667 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0747\n",
      "Epoch [7/15], Accuracy : 97.82333333333334 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0668\n",
      "Epoch [8/15], Accuracy : 97.88333333333334 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0580\n",
      "Epoch [9/15], Accuracy : 98.10166666666667 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0498\n",
      "Epoch [10/15], Accuracy : 98.14 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0797\n",
      "Epoch [11/15], Accuracy : 98.26666666666667 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0501\n",
      "Epoch [12/15], Accuracy : 98.36833333333334 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0514\n",
      "Epoch [13/15], Accuracy : 98.37166666666667 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0606\n",
      "Epoch [14/15], Accuracy : 98.425 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0474\n",
      "Epoch [15/15], Accuracy : 98.515 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 14 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.0928\n",
      "Epoch [1/15], Accuracy : 96.27333333333333 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.0896\n",
      "Epoch [2/15], Accuracy : 97.43 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.1014\n",
      "Epoch [3/15], Accuracy : 97.50166666666667 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0630\n",
      "Epoch [4/15], Accuracy : 97.73 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0850\n",
      "Epoch [5/15], Accuracy : 97.84166666666667 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0683\n",
      "Epoch [6/15], Accuracy : 98.08333333333333 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0628\n",
      "Epoch [7/15], Accuracy : 98.11166666666666 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0454\n",
      "Epoch [8/15], Accuracy : 98.14666666666666 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0782\n",
      "Epoch [9/15], Accuracy : 98.24 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0539\n",
      "Epoch [10/15], Accuracy : 98.41166666666666 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0550\n",
      "Epoch [11/15], Accuracy : 98.39666666666666 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0613\n",
      "Epoch [12/15], Accuracy : 98.35166666666667 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0576\n",
      "Epoch [13/15], Accuracy : 98.44333333333333 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0413\n",
      "Epoch [14/15], Accuracy : 98.59 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0645\n",
      "Epoch [15/15], Accuracy : 98.59666666666666 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 15 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.0945\n",
      "Epoch [1/15], Accuracy : 97.53166666666667 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.1034\n",
      "Epoch [2/15], Accuracy : 97.66833333333334 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.0643\n",
      "Epoch [3/15], Accuracy : 97.775 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0596\n",
      "Epoch [4/15], Accuracy : 97.91333333333333 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0613\n",
      "Epoch [5/15], Accuracy : 97.91166666666666 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0526\n",
      "Epoch [6/15], Accuracy : 98.02 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0511\n",
      "Epoch [7/15], Accuracy : 98.11833333333334 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0800\n",
      "Epoch [8/15], Accuracy : 98.11666666666666 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0606\n",
      "Epoch [9/15], Accuracy : 98.24166666666666 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0523\n",
      "Epoch [10/15], Accuracy : 98.34 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0586\n",
      "Epoch [11/15], Accuracy : 98.31 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0445\n",
      "Epoch [12/15], Accuracy : 98.33666666666667 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0461\n",
      "Epoch [13/15], Accuracy : 98.325 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0566\n",
      "Epoch [14/15], Accuracy : 98.46166666666667 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0612\n",
      "Epoch [15/15], Accuracy : 98.485 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 16 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.0566\n",
      "Epoch [1/15], Accuracy : 97.04833333333333 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.0975\n",
      "Epoch [2/15], Accuracy : 97.525 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.0740\n",
      "Epoch [3/15], Accuracy : 97.63 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0651\n",
      "Epoch [4/15], Accuracy : 97.79 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0804\n",
      "Epoch [5/15], Accuracy : 97.86666666666666 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0522\n",
      "Epoch [6/15], Accuracy : 97.89 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0631\n",
      "Epoch [7/15], Accuracy : 98.0 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.0612\n",
      "Epoch [8/15], Accuracy : 98.05166666666666 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0714\n",
      "Epoch [9/15], Accuracy : 98.09833333333333 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0596\n",
      "Epoch [10/15], Accuracy : 98.07166666666667 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0538\n",
      "Epoch [11/15], Accuracy : 98.23333333333333 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0584\n",
      "Epoch [12/15], Accuracy : 98.24333333333334 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0869\n",
      "Epoch [13/15], Accuracy : 98.28666666666666 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0514\n",
      "Epoch [14/15], Accuracy : 98.30666666666667 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0547\n",
      "Epoch [15/15], Accuracy : 98.22333333333333 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 17 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.1777\n",
      "Epoch [1/15], Accuracy : 95.37166666666667 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.1264\n",
      "Epoch [2/15], Accuracy : 96.285 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.1076\n",
      "Epoch [3/15], Accuracy : 96.73 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.0750\n",
      "Epoch [4/15], Accuracy : 97.135 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.0563\n",
      "Epoch [5/15], Accuracy : 97.22666666666667 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.0595\n",
      "Epoch [6/15], Accuracy : 97.44 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.0757\n",
      "Epoch [7/15], Accuracy : 97.50833333333334 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.1129\n",
      "Epoch [8/15], Accuracy : 97.625 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.0630\n",
      "Epoch [9/15], Accuracy : 97.695 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.0597\n",
      "Epoch [10/15], Accuracy : 97.85833333333333 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0696\n",
      "Epoch [11/15], Accuracy : 97.92333333333333 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0675\n",
      "Epoch [12/15], Accuracy : 97.94833333333334 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.0917\n",
      "Epoch [13/15], Accuracy : 98.04666666666667 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0854\n",
      "Epoch [14/15], Accuracy : 97.97666666666667 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0560\n",
      "Epoch [15/15], Accuracy : 98.14833333333333 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 18 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 0.2731\n",
      "Epoch [1/15], Accuracy : 93.735 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.2281\n",
      "Epoch [2/15], Accuracy : 94.495 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.1732\n",
      "Epoch [3/15], Accuracy : 94.68333333333334 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.1692\n",
      "Epoch [4/15], Accuracy : 95.355 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.1390\n",
      "Epoch [5/15], Accuracy : 95.66166666666666 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.1228\n",
      "Epoch [6/15], Accuracy : 95.95666666666666 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.1585\n",
      "Epoch [7/15], Accuracy : 96.17166666666667 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.1104\n",
      "Epoch [8/15], Accuracy : 96.31833333333333 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.1034\n",
      "Epoch [9/15], Accuracy : 96.46833333333333 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.1429\n",
      "Epoch [10/15], Accuracy : 96.69 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.0969\n",
      "Epoch [11/15], Accuracy : 96.855 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.0990\n",
      "Epoch [12/15], Accuracy : 96.9 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.1189\n",
      "Epoch [13/15], Accuracy : 97.07166666666667 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.0872\n",
      "Epoch [14/15], Accuracy : 97.05166666666666 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.0903\n",
      "Epoch [15/15], Accuracy : 97.21666666666667 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 19 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 1.2530\n",
      "Epoch [1/15], Accuracy : 91.87833333333333 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 0.6202\n",
      "Epoch [2/15], Accuracy : 93.88166666666666 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 0.3729\n",
      "Epoch [3/15], Accuracy : 93.395 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.3548\n",
      "Epoch [4/15], Accuracy : 93.16166666666666 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.3034\n",
      "Epoch [5/15], Accuracy : 93.345 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.2130\n",
      "Epoch [6/15], Accuracy : 93.725 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.2223\n",
      "Epoch [7/15], Accuracy : 94.03166666666667 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.2039\n",
      "Epoch [8/15], Accuracy : 94.31 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.2109\n",
      "Epoch [9/15], Accuracy : 94.45666666666666 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.2190\n",
      "Epoch [10/15], Accuracy : 94.745 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.2233\n",
      "Epoch [11/15], Accuracy : 94.94 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.2148\n",
      "Epoch [12/15], Accuracy : 95.27666666666667 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.1745\n",
      "Epoch [13/15], Accuracy : 95.52666666666667 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.2383\n",
      "Epoch [14/15], Accuracy : 95.45166666666667 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.1249\n",
      "Epoch [15/15], Accuracy : 95.69166666666666 %\n",
      "Max Epoch Reached\n",
      "Total no of parameters in Model Theta 20 is:25550\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "60\n",
      "Epoch [1/15], Step [60/60], Loss: 2.5606\n",
      "Epoch [1/15], Accuracy : 89.13166666666666 %\n",
      "60\n",
      "Epoch [2/15], Step [60/60], Loss: 1.3738\n",
      "Epoch [2/15], Accuracy : 93.39 %\n",
      "60\n",
      "Epoch [3/15], Step [60/60], Loss: 1.1228\n",
      "Epoch [3/15], Accuracy : 93.16333333333333 %\n",
      "60\n",
      "Epoch [4/15], Step [60/60], Loss: 0.9022\n",
      "Epoch [4/15], Accuracy : 92.46 %\n",
      "60\n",
      "Epoch [5/15], Step [60/60], Loss: 0.6484\n",
      "Epoch [5/15], Accuracy : 91.7 %\n",
      "60\n",
      "Epoch [6/15], Step [60/60], Loss: 0.5364\n",
      "Epoch [6/15], Accuracy : 91.28 %\n",
      "60\n",
      "Epoch [7/15], Step [60/60], Loss: 0.4603\n",
      "Epoch [7/15], Accuracy : 91.475 %\n",
      "60\n",
      "Epoch [8/15], Step [60/60], Loss: 0.5305\n",
      "Epoch [8/15], Accuracy : 91.535 %\n",
      "60\n",
      "Epoch [9/15], Step [60/60], Loss: 0.3611\n",
      "Epoch [9/15], Accuracy : 91.33 %\n",
      "60\n",
      "Epoch [10/15], Step [60/60], Loss: 0.4264\n",
      "Epoch [10/15], Accuracy : 91.87833333333333 %\n",
      "60\n",
      "Epoch [11/15], Step [60/60], Loss: 0.3417\n",
      "Epoch [11/15], Accuracy : 92.2 %\n",
      "60\n",
      "Epoch [12/15], Step [60/60], Loss: 0.3171\n",
      "Epoch [12/15], Accuracy : 92.13333333333334 %\n",
      "60\n",
      "Epoch [13/15], Step [60/60], Loss: 0.2909\n",
      "Epoch [13/15], Accuracy : 92.65833333333333 %\n",
      "60\n",
      "Epoch [14/15], Step [60/60], Loss: 0.2395\n",
      "Epoch [14/15], Accuracy : 92.795 %\n",
      "60\n",
      "Epoch [15/15], Step [60/60], Loss: 0.2015\n",
      "Epoch [15/15], Accuracy : 93.075 %\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "modelsTrainArr = []\n",
    "modelsLossArr = []\n",
    "modelsAccArr = []\n",
    "\n",
    "for i in range (len(thetaArr)):\n",
    "    j=copy.deepcopy(i) \n",
    "    theta = (1-alpha[i])*batch1_param + alpha[i]*batch2_param\n",
    "    j = ThetaModel()\n",
    "    torch.nn.utils.vector_to_parameters(theta,j.parameters())\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(j.parameters(), lr=1e-3, weight_decay = weight_decay_val)\n",
    "\n",
    "    a=[]\n",
    "    for k in j.parameters():\n",
    "        a.append(torch.numel(k))\n",
    "    print(f'Total no of parameters in Model Theta {i} is:{np.sum(a)}')\n",
    "\n",
    "    print(j.parameters)\n",
    "\n",
    "    T_train_epoch,T_train_losses,T_train_acc = trainFunc(j,max_epochs,m2train_loader)\n",
    "    \n",
    "    modelsTrainArr.append(T_train_epoch)\n",
    "    modelsLossArr.append(T_train_losses)\n",
    "    modelsAccArr.append(T_train_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "minModelLossArr = []\n",
    "for i in range (len(modelsLossArr)):\n",
    "    minModel_loss = np.min(modelsLossArr[i])\n",
    "    minModelLossArr.append(minModel_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5339263081550598"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minModelLossArr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1d86b2f3ed665d691ce24c615a98bbc398f66743afc4d4e970e6f8b36fab2b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CPSC-8430-DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
