{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22856c468b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 60000 \n",
      "test_dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset),\"\\ntest_dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader 1\n",
    "m1train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "m1test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader 2\n",
    "m2train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=1000, \n",
    "                                           shuffle=True)\n",
    "\n",
    "m2test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M1(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(M1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size) #1st Convolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)   #pool_size=2, strides=2 \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size) #2nd Convolution\n",
    "        self.fc1 = nn.Linear(320, 50) #((I/P - Filter + 2*Pad)/Stride)+1 \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(50, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 320)            #Flattening \n",
    "        x = F.relu(self.fc1(x))        #Fully Connected NN   \n",
    "        x = self.dropout(x)   \n",
    "        x = F.relu(self.fc2(x))        #Fully Connected NN           \n",
    "        x = self.fc3(x)                #O/P Layer       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def trainFunc(model,num_epochs,train_loader):\n",
    "    print('strated')\n",
    "    n_total_steps = len(train_loader)\n",
    "    train_losses = []\n",
    "    train_epoch = []\n",
    "    train_acc = []\n",
    "    not_converged =True\n",
    "    epoch = 0\n",
    "    while not_converged:\n",
    "        epoch += 1\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Forward pass\n",
    "            prediction = model(images)\n",
    "            loss = loss_func(prediction, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "            if (i+1) % 500 == 0:\n",
    "                print (f'Epoch [{epoch}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "                train_epoch.append(epoch)\n",
    "                train_losses.append(loss.item())\n",
    "                print(f'Epoch [{epoch}/{num_epochs}], Accuracy : {acc} %')\n",
    "                train_acc.append(acc)\n",
    "\n",
    "                if epoch == num_epochs:\n",
    "                        print(\"Max Epoch Reached\")\n",
    "                        not_converged = False\n",
    "                elif (epoch > 5) and  (train_losses[-1] < 0.001):\n",
    "                    if abs(train_losses[-3] - train_losses[-2]) < 1.0e-05 and abs(train_losses[-2] - train_losses[-1]) < 1.0e-05:\n",
    "                        print(\"Convergeance reached for loss:\",loss_arr[-1])\n",
    "                        not_converged = False\n",
    "                        \n",
    "    return train_epoch,train_losses,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "max_epochs = 15\n",
    "learning_rate = 0.001\n",
    "kernel_size = 4\n",
    "weight_decay_val = 1e-4\n",
    "dropout = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model 1 with batch_size=64 is:25550\n"
     ]
    }
   ],
   "source": [
    "mBatch1 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mBatch1.parameters(), lr=learning_rate, weight_decay = weight_decay_val)\n",
    "\n",
    "a=[]\n",
    "for i in mBatch1.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model 1 with batch_size={64} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "Epoch [1/15], Step [500/938], Loss: 0.2480\n",
      "Epoch [1/15], Accuracy : 82.478125 %\n",
      "Epoch [2/15], Step [500/938], Loss: 0.1733\n",
      "Epoch [2/15], Accuracy : 95.85625 %\n",
      "Epoch [3/15], Step [500/938], Loss: 0.1368\n",
      "Epoch [3/15], Accuracy : 97.04375 %\n",
      "Epoch [4/15], Step [500/938], Loss: 0.0355\n",
      "Epoch [4/15], Accuracy : 97.70625 %\n",
      "Epoch [5/15], Step [500/938], Loss: 0.0324\n",
      "Epoch [5/15], Accuracy : 97.978125 %\n",
      "Epoch [6/15], Step [500/938], Loss: 0.0452\n",
      "Epoch [6/15], Accuracy : 98.203125 %\n",
      "Epoch [7/15], Step [500/938], Loss: 0.0082\n",
      "Epoch [7/15], Accuracy : 98.165625 %\n",
      "Epoch [8/15], Step [500/938], Loss: 0.0021\n",
      "Epoch [8/15], Accuracy : 98.428125 %\n",
      "Epoch [9/15], Step [500/938], Loss: 0.0754\n",
      "Epoch [9/15], Accuracy : 98.50625 %\n",
      "Epoch [10/15], Step [500/938], Loss: 0.0593\n",
      "Epoch [10/15], Accuracy : 98.63125 %\n",
      "Epoch [11/15], Step [500/938], Loss: 0.0146\n",
      "Epoch [11/15], Accuracy : 98.75 %\n",
      "Epoch [12/15], Step [500/938], Loss: 0.0341\n",
      "Epoch [12/15], Accuracy : 98.69375 %\n",
      "Epoch [13/15], Step [500/938], Loss: 0.0165\n",
      "Epoch [13/15], Accuracy : 98.93125 %\n",
      "Epoch [14/15], Step [500/938], Loss: 0.0184\n",
      "Epoch [14/15], Accuracy : 98.9625 %\n",
      "Epoch [15/15], Step [500/938], Loss: 0.1426\n",
      "Epoch [15/15], Accuracy : 98.95 %\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "B1_train_epoch,B1_train_losses,B1_train_acc = trainFunc(mBatch1,max_epochs,m1train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4764,  0.4666,  0.3013,  ...,  0.0815, -0.0638,  0.0091],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 25550\n"
     ]
    }
   ],
   "source": [
    "batch1_param = torch.nn.utils.parameters_to_vector(mBatch1.parameters())\n",
    "print(batch1_param,'\\nlen:',len(batch1_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M2(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(M2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size) #1st Convolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)   #pool_size=2, strides=2 \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size) #2nd Convolution\n",
    "        self.fc1 = nn.Linear(320, 50) #((I/P - Filter + 2*Pad)/Stride)+1 \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(50, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 320)            #Flattening \n",
    "        x = F.relu(self.fc1(x))        #Fully Connected NN   \n",
    "        x = self.dropout(x)   \n",
    "        x = F.relu(self.fc2(x))        #Fully Connected NN           \n",
    "        x = self.fc3(x)                #O/P Layer       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model 2 with batch_size=1024 is:25550\n"
     ]
    }
   ],
   "source": [
    "mBatch2 = M2()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mBatch2.parameters(), lr=learning_rate, weight_decay = weight_decay_val)\n",
    "\n",
    "a=[]\n",
    "for i in mBatch2.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model 2 with batch_size={1024} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35600/2014964584.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mB2_train_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB2_train_losses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB2_train_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmBatch2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm2train_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35600/990216189.py\u001b[0m in \u001b[0;36mtrainFunc\u001b[1;34m(model, num_epochs, train_loader)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m# Backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\CPSC-8430-DeepLearning\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "B2_train_epoch,B2_train_losses,B2_train_acc = trainFunc(mBatch2,max_epochs,m2train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch2_param = torch.nn.utils.parameters_to_vector(mBatch2.parameters())\n",
    "print(batch2_param,'\\nlen:',len(batch2_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1d86b2f3ed665d691ce24c615a98bbc398f66743afc4d4e970e6f8b36fab2b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CPSC-8430-DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
