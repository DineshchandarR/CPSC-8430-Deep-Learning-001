{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22795a38990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 60000 \n",
      "test_dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset),\"\\ntest_dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader func\n",
    "def train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def test_loader(batch_size):\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M1(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(M1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.conv2 = nn.Conv2d(3, 13, 5)\n",
    "        self.fc1 = nn.Linear(208, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # flatten as one dimension\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def trainFunc(model,num_epochs,train_batch_size):\n",
    "    model.train()\n",
    "    print('strated')\n",
    "    train_load = train_loader(train_batch_size)\n",
    "    n_total_steps = len(train_load)\n",
    "    train_losses = []\n",
    "    train_epoch = []\n",
    "    train_acc = []\n",
    "    not_converged =True\n",
    "    epoch = 0\n",
    "    while not_converged:\n",
    "        epoch += 1\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i, (images, labels) in enumerate(train_load):  \n",
    "            \n",
    "            images, labels = Variable(images),Variable(labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            prediction = model(images)\n",
    "            loss = loss_func(prediction, labels)\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_acc.append(acc)\n",
    "            train_epoch.append(epoch)\n",
    "\n",
    "            if (i+1) % 60 == 0:\n",
    "                print (f'Train O/P: Epoch [{epoch}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "   \n",
    "                if epoch == num_epochs:\n",
    "                        print(\"Max Epoch Reached\")\n",
    "                        not_converged = False\n",
    "                elif (epoch > 5) and  (train_losses[-1] < 0.001):\n",
    "                    if abs(train_losses[-3] - train_losses[-2]) < 1.0e-05 and abs(train_losses[-2] - train_losses[-1]) < 1.0e-05:\n",
    "                        print(\"Convergeance reached for loss:\",train_losses[-1])\n",
    "                        not_converged = False\n",
    "                        \n",
    "    return train_epoch,train_losses,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model with batch_size=64 is:37160\n"
     ]
    }
   ],
   "source": [
    "# Training Model with batch size=64\n",
    "torch.manual_seed(1)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "mBatch1 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mBatch1.parameters(), lr=learning_rate) #, weight_decay = weight_decay_val)\n",
    "\n",
    "a=[]\n",
    "for i in mBatch1.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model with batch_size={64} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "Train O/P: Epoch [1/15], Step [60/938], Loss: 0.7138\n",
      "Train O/P: Epoch [1/15], Step [120/938], Loss: 0.4053\n",
      "Train O/P: Epoch [1/15], Step [180/938], Loss: 0.4464\n",
      "Train O/P: Epoch [1/15], Step [240/938], Loss: 0.5013\n",
      "Train O/P: Epoch [1/15], Step [300/938], Loss: 0.4077\n",
      "Train O/P: Epoch [1/15], Step [360/938], Loss: 0.1440\n",
      "Train O/P: Epoch [1/15], Step [420/938], Loss: 0.1891\n",
      "Train O/P: Epoch [1/15], Step [480/938], Loss: 0.1546\n",
      "Train O/P: Epoch [1/15], Step [540/938], Loss: 0.1913\n",
      "Train O/P: Epoch [1/15], Step [600/938], Loss: 0.2242\n",
      "Train O/P: Epoch [1/15], Step [660/938], Loss: 0.1742\n",
      "Train O/P: Epoch [1/15], Step [720/938], Loss: 0.1411\n",
      "Train O/P: Epoch [1/15], Step [780/938], Loss: 0.0321\n",
      "Train O/P: Epoch [1/15], Step [840/938], Loss: 0.1450\n",
      "Train O/P: Epoch [1/15], Step [900/938], Loss: 0.1768\n",
      "Train O/P: Epoch [2/15], Step [60/938], Loss: 0.2807\n",
      "Train O/P: Epoch [2/15], Step [120/938], Loss: 0.1112\n",
      "Train O/P: Epoch [2/15], Step [180/938], Loss: 0.1629\n",
      "Train O/P: Epoch [2/15], Step [240/938], Loss: 0.0460\n",
      "Train O/P: Epoch [2/15], Step [300/938], Loss: 0.0329\n",
      "Train O/P: Epoch [2/15], Step [360/938], Loss: 0.0584\n",
      "Train O/P: Epoch [2/15], Step [420/938], Loss: 0.2002\n",
      "Train O/P: Epoch [2/15], Step [480/938], Loss: 0.1005\n",
      "Train O/P: Epoch [2/15], Step [540/938], Loss: 0.1065\n",
      "Train O/P: Epoch [2/15], Step [600/938], Loss: 0.1983\n",
      "Train O/P: Epoch [2/15], Step [660/938], Loss: 0.0634\n",
      "Train O/P: Epoch [2/15], Step [720/938], Loss: 0.0469\n",
      "Train O/P: Epoch [2/15], Step [780/938], Loss: 0.0230\n",
      "Train O/P: Epoch [2/15], Step [840/938], Loss: 0.0474\n",
      "Train O/P: Epoch [2/15], Step [900/938], Loss: 0.0543\n",
      "Train O/P: Epoch [3/15], Step [60/938], Loss: 0.0397\n",
      "Train O/P: Epoch [3/15], Step [120/938], Loss: 0.0382\n",
      "Train O/P: Epoch [3/15], Step [180/938], Loss: 0.0113\n",
      "Train O/P: Epoch [3/15], Step [240/938], Loss: 0.0615\n",
      "Train O/P: Epoch [3/15], Step [300/938], Loss: 0.0515\n",
      "Train O/P: Epoch [3/15], Step [360/938], Loss: 0.1043\n",
      "Train O/P: Epoch [3/15], Step [420/938], Loss: 0.0701\n",
      "Train O/P: Epoch [3/15], Step [480/938], Loss: 0.1199\n",
      "Train O/P: Epoch [3/15], Step [540/938], Loss: 0.0472\n",
      "Train O/P: Epoch [3/15], Step [600/938], Loss: 0.0375\n",
      "Train O/P: Epoch [3/15], Step [660/938], Loss: 0.0230\n",
      "Train O/P: Epoch [3/15], Step [720/938], Loss: 0.1624\n",
      "Train O/P: Epoch [3/15], Step [780/938], Loss: 0.0707\n",
      "Train O/P: Epoch [3/15], Step [840/938], Loss: 0.1046\n",
      "Train O/P: Epoch [3/15], Step [900/938], Loss: 0.1234\n",
      "Train O/P: Epoch [4/15], Step [60/938], Loss: 0.0046\n",
      "Train O/P: Epoch [4/15], Step [120/938], Loss: 0.0469\n",
      "Train O/P: Epoch [4/15], Step [180/938], Loss: 0.0373\n",
      "Train O/P: Epoch [4/15], Step [240/938], Loss: 0.0324\n",
      "Train O/P: Epoch [4/15], Step [300/938], Loss: 0.0044\n",
      "Train O/P: Epoch [4/15], Step [360/938], Loss: 0.0651\n",
      "Train O/P: Epoch [4/15], Step [420/938], Loss: 0.0550\n",
      "Train O/P: Epoch [4/15], Step [480/938], Loss: 0.0382\n",
      "Train O/P: Epoch [4/15], Step [540/938], Loss: 0.0210\n",
      "Train O/P: Epoch [4/15], Step [600/938], Loss: 0.0255\n",
      "Train O/P: Epoch [4/15], Step [660/938], Loss: 0.0221\n",
      "Train O/P: Epoch [4/15], Step [720/938], Loss: 0.1225\n",
      "Train O/P: Epoch [4/15], Step [780/938], Loss: 0.0760\n",
      "Train O/P: Epoch [4/15], Step [840/938], Loss: 0.1102\n",
      "Train O/P: Epoch [4/15], Step [900/938], Loss: 0.0348\n",
      "Train O/P: Epoch [5/15], Step [60/938], Loss: 0.0483\n",
      "Train O/P: Epoch [5/15], Step [120/938], Loss: 0.0413\n",
      "Train O/P: Epoch [5/15], Step [180/938], Loss: 0.0131\n",
      "Train O/P: Epoch [5/15], Step [240/938], Loss: 0.0207\n",
      "Train O/P: Epoch [5/15], Step [300/938], Loss: 0.0634\n",
      "Train O/P: Epoch [5/15], Step [360/938], Loss: 0.0419\n",
      "Train O/P: Epoch [5/15], Step [420/938], Loss: 0.0187\n",
      "Train O/P: Epoch [5/15], Step [480/938], Loss: 0.0142\n",
      "Train O/P: Epoch [5/15], Step [540/938], Loss: 0.0776\n",
      "Train O/P: Epoch [5/15], Step [600/938], Loss: 0.0972\n",
      "Train O/P: Epoch [5/15], Step [660/938], Loss: 0.0670\n",
      "Train O/P: Epoch [5/15], Step [720/938], Loss: 0.0599\n",
      "Train O/P: Epoch [5/15], Step [780/938], Loss: 0.1025\n",
      "Train O/P: Epoch [5/15], Step [840/938], Loss: 0.0192\n",
      "Train O/P: Epoch [5/15], Step [900/938], Loss: 0.0169\n",
      "Train O/P: Epoch [6/15], Step [60/938], Loss: 0.0253\n",
      "Train O/P: Epoch [6/15], Step [120/938], Loss: 0.0150\n",
      "Train O/P: Epoch [6/15], Step [180/938], Loss: 0.0108\n",
      "Train O/P: Epoch [6/15], Step [240/938], Loss: 0.0184\n",
      "Train O/P: Epoch [6/15], Step [300/938], Loss: 0.0132\n",
      "Train O/P: Epoch [6/15], Step [360/938], Loss: 0.0321\n",
      "Train O/P: Epoch [6/15], Step [420/938], Loss: 0.0196\n",
      "Train O/P: Epoch [6/15], Step [480/938], Loss: 0.1241\n",
      "Train O/P: Epoch [6/15], Step [540/938], Loss: 0.1139\n",
      "Train O/P: Epoch [6/15], Step [600/938], Loss: 0.0156\n",
      "Train O/P: Epoch [6/15], Step [660/938], Loss: 0.0091\n",
      "Train O/P: Epoch [6/15], Step [720/938], Loss: 0.0253\n",
      "Train O/P: Epoch [6/15], Step [780/938], Loss: 0.0511\n",
      "Train O/P: Epoch [6/15], Step [840/938], Loss: 0.0043\n",
      "Train O/P: Epoch [6/15], Step [900/938], Loss: 0.0040\n",
      "Train O/P: Epoch [7/15], Step [60/938], Loss: 0.0761\n",
      "Train O/P: Epoch [7/15], Step [120/938], Loss: 0.0070\n",
      "Train O/P: Epoch [7/15], Step [180/938], Loss: 0.0059\n",
      "Train O/P: Epoch [7/15], Step [240/938], Loss: 0.0030\n",
      "Train O/P: Epoch [7/15], Step [300/938], Loss: 0.0275\n",
      "Train O/P: Epoch [7/15], Step [360/938], Loss: 0.0360\n",
      "Train O/P: Epoch [7/15], Step [420/938], Loss: 0.0420\n",
      "Train O/P: Epoch [7/15], Step [480/938], Loss: 0.0026\n",
      "Train O/P: Epoch [7/15], Step [540/938], Loss: 0.0595\n",
      "Train O/P: Epoch [7/15], Step [600/938], Loss: 0.0199\n",
      "Train O/P: Epoch [7/15], Step [660/938], Loss: 0.0330\n",
      "Train O/P: Epoch [7/15], Step [720/938], Loss: 0.0380\n",
      "Train O/P: Epoch [7/15], Step [780/938], Loss: 0.0114\n",
      "Train O/P: Epoch [7/15], Step [840/938], Loss: 0.0351\n",
      "Train O/P: Epoch [7/15], Step [900/938], Loss: 0.1165\n",
      "Train O/P: Epoch [8/15], Step [60/938], Loss: 0.0165\n",
      "Train O/P: Epoch [8/15], Step [120/938], Loss: 0.0588\n",
      "Train O/P: Epoch [8/15], Step [180/938], Loss: 0.0193\n",
      "Train O/P: Epoch [8/15], Step [240/938], Loss: 0.0370\n",
      "Train O/P: Epoch [8/15], Step [300/938], Loss: 0.0308\n",
      "Train O/P: Epoch [8/15], Step [360/938], Loss: 0.0039\n",
      "Train O/P: Epoch [8/15], Step [420/938], Loss: 0.0236\n",
      "Train O/P: Epoch [8/15], Step [480/938], Loss: 0.0893\n",
      "Train O/P: Epoch [8/15], Step [540/938], Loss: 0.0352\n",
      "Train O/P: Epoch [8/15], Step [600/938], Loss: 0.0061\n",
      "Train O/P: Epoch [8/15], Step [660/938], Loss: 0.0809\n",
      "Train O/P: Epoch [8/15], Step [720/938], Loss: 0.0030\n",
      "Train O/P: Epoch [8/15], Step [780/938], Loss: 0.0348\n",
      "Train O/P: Epoch [8/15], Step [840/938], Loss: 0.0172\n",
      "Train O/P: Epoch [8/15], Step [900/938], Loss: 0.0029\n",
      "Train O/P: Epoch [9/15], Step [60/938], Loss: 0.0125\n",
      "Train O/P: Epoch [9/15], Step [120/938], Loss: 0.0157\n",
      "Train O/P: Epoch [9/15], Step [180/938], Loss: 0.0095\n",
      "Train O/P: Epoch [9/15], Step [240/938], Loss: 0.0176\n",
      "Train O/P: Epoch [9/15], Step [300/938], Loss: 0.0257\n",
      "Train O/P: Epoch [9/15], Step [360/938], Loss: 0.0861\n",
      "Train O/P: Epoch [9/15], Step [420/938], Loss: 0.0207\n",
      "Train O/P: Epoch [9/15], Step [480/938], Loss: 0.0104\n",
      "Train O/P: Epoch [9/15], Step [540/938], Loss: 0.0003\n",
      "Train O/P: Epoch [9/15], Step [600/938], Loss: 0.0790\n",
      "Train O/P: Epoch [9/15], Step [660/938], Loss: 0.0812\n",
      "Train O/P: Epoch [9/15], Step [720/938], Loss: 0.0060\n",
      "Train O/P: Epoch [9/15], Step [780/938], Loss: 0.0221\n",
      "Train O/P: Epoch [9/15], Step [840/938], Loss: 0.0168\n",
      "Train O/P: Epoch [9/15], Step [900/938], Loss: 0.0246\n",
      "Train O/P: Epoch [10/15], Step [60/938], Loss: 0.0121\n",
      "Train O/P: Epoch [10/15], Step [120/938], Loss: 0.0253\n",
      "Train O/P: Epoch [10/15], Step [180/938], Loss: 0.0394\n",
      "Train O/P: Epoch [10/15], Step [240/938], Loss: 0.0030\n",
      "Train O/P: Epoch [10/15], Step [300/938], Loss: 0.0018\n",
      "Train O/P: Epoch [10/15], Step [360/938], Loss: 0.0150\n",
      "Train O/P: Epoch [10/15], Step [420/938], Loss: 0.1514\n",
      "Train O/P: Epoch [10/15], Step [480/938], Loss: 0.0096\n",
      "Train O/P: Epoch [10/15], Step [540/938], Loss: 0.0277\n",
      "Train O/P: Epoch [10/15], Step [600/938], Loss: 0.0122\n",
      "Train O/P: Epoch [10/15], Step [660/938], Loss: 0.0058\n",
      "Train O/P: Epoch [10/15], Step [720/938], Loss: 0.0126\n",
      "Train O/P: Epoch [10/15], Step [780/938], Loss: 0.0363\n",
      "Train O/P: Epoch [10/15], Step [840/938], Loss: 0.0030\n",
      "Train O/P: Epoch [10/15], Step [900/938], Loss: 0.0042\n",
      "Train O/P: Epoch [11/15], Step [60/938], Loss: 0.0146\n",
      "Train O/P: Epoch [11/15], Step [120/938], Loss: 0.0353\n",
      "Train O/P: Epoch [11/15], Step [180/938], Loss: 0.0198\n",
      "Train O/P: Epoch [11/15], Step [240/938], Loss: 0.0062\n",
      "Train O/P: Epoch [11/15], Step [300/938], Loss: 0.0335\n",
      "Train O/P: Epoch [11/15], Step [360/938], Loss: 0.0041\n",
      "Train O/P: Epoch [11/15], Step [420/938], Loss: 0.0718\n",
      "Train O/P: Epoch [11/15], Step [480/938], Loss: 0.0085\n",
      "Train O/P: Epoch [11/15], Step [540/938], Loss: 0.0498\n",
      "Train O/P: Epoch [11/15], Step [600/938], Loss: 0.0021\n",
      "Train O/P: Epoch [11/15], Step [660/938], Loss: 0.0044\n",
      "Train O/P: Epoch [11/15], Step [720/938], Loss: 0.0024\n",
      "Train O/P: Epoch [11/15], Step [780/938], Loss: 0.0334\n",
      "Train O/P: Epoch [11/15], Step [840/938], Loss: 0.0247\n",
      "Train O/P: Epoch [11/15], Step [900/938], Loss: 0.0009\n",
      "Train O/P: Epoch [12/15], Step [60/938], Loss: 0.0291\n",
      "Train O/P: Epoch [12/15], Step [120/938], Loss: 0.0102\n",
      "Train O/P: Epoch [12/15], Step [180/938], Loss: 0.0018\n",
      "Train O/P: Epoch [12/15], Step [240/938], Loss: 0.0884\n",
      "Train O/P: Epoch [12/15], Step [300/938], Loss: 0.0110\n",
      "Train O/P: Epoch [12/15], Step [360/938], Loss: 0.0047\n",
      "Train O/P: Epoch [12/15], Step [420/938], Loss: 0.0121\n",
      "Train O/P: Epoch [12/15], Step [480/938], Loss: 0.0007\n",
      "Train O/P: Epoch [12/15], Step [540/938], Loss: 0.0014\n",
      "Train O/P: Epoch [12/15], Step [600/938], Loss: 0.0009\n",
      "Train O/P: Epoch [12/15], Step [660/938], Loss: 0.0082\n",
      "Train O/P: Epoch [12/15], Step [720/938], Loss: 0.0041\n",
      "Train O/P: Epoch [12/15], Step [780/938], Loss: 0.0037\n",
      "Train O/P: Epoch [12/15], Step [840/938], Loss: 0.0042\n",
      "Train O/P: Epoch [12/15], Step [900/938], Loss: 0.0179\n",
      "Train O/P: Epoch [13/15], Step [60/938], Loss: 0.0021\n",
      "Train O/P: Epoch [13/15], Step [120/938], Loss: 0.0523\n",
      "Train O/P: Epoch [13/15], Step [180/938], Loss: 0.0003\n",
      "Train O/P: Epoch [13/15], Step [240/938], Loss: 0.0022\n",
      "Train O/P: Epoch [13/15], Step [300/938], Loss: 0.0004\n",
      "Train O/P: Epoch [13/15], Step [360/938], Loss: 0.0136\n",
      "Train O/P: Epoch [13/15], Step [420/938], Loss: 0.0484\n",
      "Train O/P: Epoch [13/15], Step [480/938], Loss: 0.0241\n",
      "Train O/P: Epoch [13/15], Step [540/938], Loss: 0.0114\n",
      "Train O/P: Epoch [13/15], Step [600/938], Loss: 0.0457\n",
      "Train O/P: Epoch [13/15], Step [660/938], Loss: 0.0066\n",
      "Train O/P: Epoch [13/15], Step [720/938], Loss: 0.0028\n",
      "Train O/P: Epoch [13/15], Step [780/938], Loss: 0.0050\n",
      "Train O/P: Epoch [13/15], Step [840/938], Loss: 0.0103\n",
      "Train O/P: Epoch [13/15], Step [900/938], Loss: 0.0078\n",
      "Train O/P: Epoch [14/15], Step [60/938], Loss: 0.0118\n",
      "Train O/P: Epoch [14/15], Step [120/938], Loss: 0.0250\n",
      "Train O/P: Epoch [14/15], Step [180/938], Loss: 0.0017\n",
      "Train O/P: Epoch [14/15], Step [240/938], Loss: 0.0185\n",
      "Train O/P: Epoch [14/15], Step [300/938], Loss: 0.0095\n",
      "Train O/P: Epoch [14/15], Step [360/938], Loss: 0.0151\n",
      "Train O/P: Epoch [14/15], Step [420/938], Loss: 0.0075\n",
      "Train O/P: Epoch [14/15], Step [480/938], Loss: 0.0198\n",
      "Train O/P: Epoch [14/15], Step [540/938], Loss: 0.0032\n",
      "Train O/P: Epoch [14/15], Step [600/938], Loss: 0.0070\n",
      "Train O/P: Epoch [14/15], Step [660/938], Loss: 0.0027\n",
      "Train O/P: Epoch [14/15], Step [720/938], Loss: 0.0076\n",
      "Train O/P: Epoch [14/15], Step [780/938], Loss: 0.0015\n",
      "Train O/P: Epoch [14/15], Step [840/938], Loss: 0.0003\n",
      "Train O/P: Epoch [14/15], Step [900/938], Loss: 0.0652\n",
      "Train O/P: Epoch [15/15], Step [60/938], Loss: 0.0004\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [120/938], Loss: 0.0150\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [180/938], Loss: 0.0159\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [240/938], Loss: 0.0149\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [300/938], Loss: 0.0374\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [360/938], Loss: 0.0015\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [420/938], Loss: 0.0074\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [480/938], Loss: 0.0037\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [540/938], Loss: 0.0101\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [600/938], Loss: 0.0006\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [660/938], Loss: 0.0709\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [720/938], Loss: 0.0036\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [780/938], Loss: 0.0248\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [840/938], Loss: 0.0024\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [15/15], Step [900/938], Loss: 0.0004\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 15\n",
    "train_batch_size = 64\n",
    "B1_train_epoch,B1_train_losses,B1_train_acc = trainFunc(mBatch1,max_epochs,train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model with batch_size=1000 is:37160\n"
     ]
    }
   ],
   "source": [
    "# Training Model with batch size=1000\n",
    "torch.manual_seed(1)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "mBatch2 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mBatch2.parameters(), lr=learning_rate) \n",
    "\n",
    "a=[]\n",
    "for i in mBatch2.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model with batch_size={1000} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "Train O/P: Epoch [1/15], Step [60/60], Loss: 0.5032\n",
      "Train O/P: Epoch [2/15], Step [60/60], Loss: 0.2616\n",
      "Train O/P: Epoch [3/15], Step [60/60], Loss: 0.2219\n",
      "Train O/P: Epoch [4/15], Step [60/60], Loss: 0.1868\n",
      "Train O/P: Epoch [5/15], Step [60/60], Loss: 0.1737\n",
      "Train O/P: Epoch [6/15], Step [60/60], Loss: 0.1565\n",
      "Train O/P: Epoch [7/15], Step [60/60], Loss: 0.1477\n",
      "Train O/P: Epoch [8/15], Step [60/60], Loss: 0.1193\n",
      "Train O/P: Epoch [9/15], Step [60/60], Loss: 0.1066\n",
      "Train O/P: Epoch [10/15], Step [60/60], Loss: 0.1034\n",
      "Train O/P: Epoch [11/15], Step [60/60], Loss: 0.0805\n",
      "Train O/P: Epoch [12/15], Step [60/60], Loss: 0.0571\n",
      "Train O/P: Epoch [13/15], Step [60/60], Loss: 0.0600\n",
      "Train O/P: Epoch [14/15], Step [60/60], Loss: 0.0868\n",
      "Train O/P: Epoch [15/15], Step [60/60], Loss: 0.0734\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 15\n",
    "train_batch_size = 1000\n",
    "B2_train_epoch,B2_train_losses,B2_train_acc = trainFunc(mBatch2,max_epochs,train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14070"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(B1_train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting Loss plot for both Batch 1 and Batch 2 models\n",
    "#B2_train_epoch,B2_train_losses,B2_train_acc\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(B1_train_epoch,B1_train_losses,color=\"blue\")\n",
    "plt.plot(B2_train_epoch,B2_train_losses,color=\"orange\")\n",
    "plt.title('Batch1 & Bathc2 Loss VS Epoch')\n",
    "plt.legend(['Batch 1','Batch 2'])\n",
    "plt.xlabel ('Epoch')\n",
    "plt.ylabel ('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.113686711410992e-05 0.03891250491142273\n"
     ]
    }
   ],
   "source": [
    "print(np.min(B1_train_losses),np.min(B2_train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1340, -0.0950, -0.1584,  ..., -0.0381,  0.1652, -0.0572],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 37160\n"
     ]
    }
   ],
   "source": [
    "batch1_param = torch.nn.utils.parameters_to_vector(mBatch1.parameters())\n",
    "print(batch1_param,'\\nlen:',len(batch1_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1260, -0.0375,  0.0124,  ...,  0.0111,  0.0043, -0.0403],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 37160\n"
     ]
    }
   ],
   "source": [
    "batch2_param = torch.nn.utils.parameters_to_vector(mBatch2.parameters())\n",
    "print(batch2_param,'\\nlen:',len(batch2_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.  -1.8 -1.6 -1.4 -1.2 -1.  -0.8 -0.6 -0.4 -0.2  0.   0.2  0.4  0.6\n",
      "  0.8  1.   1.2  1.4  1.6  1.8  2. ]\n"
     ]
    }
   ],
   "source": [
    "alpha = np.linspace(-2.0, 2.0, num=21)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaArr =[]\n",
    "for i in range (len(alpha)):\n",
    "    theta = (1-alpha[i])*batch1_param + alpha[i]*batch2_param\n",
    "    thetaArr.append(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThetaModel(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(ThetaModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.conv2 = nn.Conv2d(3, 13, 5)\n",
    "        self.fc1 = nn.Linear(208, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # flatten as one dimension\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunction(model,loss_func,test_batch_size): \n",
    "    test_load = test_loader(test_batch_size)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        testLoss = 0\n",
    "        count =0\n",
    "        for images, labels in test_load:\n",
    "            images, labels = Variable(images),Variable(labels)\n",
    "            \n",
    "            prediction = model(images)\n",
    "            testLoss += loss_func(prediction,labels).item()\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            count +=1\n",
    "    netTest_loss = testLoss/n_samples\n",
    "    netTest_acc1 = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the test images: {netTest_acc1} & Test Loss: {netTest_loss} %')\n",
    "    return netTest_acc1, netTest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model Theta 0 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.8494\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 97.41 & Test Loss: 0.007982954763712897 %\n",
      "Total no of parameters in Model Theta 1 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.5162\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 97.7 & Test Loss: 0.005582327369865919 %\n",
      "Total no of parameters in Model Theta 2 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.2638\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.05 & Test Loss: 0.003825676831367472 %\n",
      "Total no of parameters in Model Theta 3 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.1417\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.3 & Test Loss: 0.002667859084692796 %\n",
      "Total no of parameters in Model Theta 4 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0931\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.49 & Test Loss: 0.0019025371589959405 %\n",
      "Total no of parameters in Model Theta 5 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0553\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.61 & Test Loss: 0.0013272236327271163 %\n",
      "Total no of parameters in Model Theta 6 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0499\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.71 & Test Loss: 0.0010046563834830349 %\n",
      "Total no of parameters in Model Theta 7 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0240\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.76 & Test Loss: 0.0007301674815879462 %\n",
      "Total no of parameters in Model Theta 8 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0118\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.8 & Test Loss: 0.0005111506290525969 %\n",
      "Total no of parameters in Model Theta 9 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0070\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.86 & Test Loss: 0.0003696764381368098 %\n",
      "Total no of parameters in Model Theta 10 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0089\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.94 & Test Loss: 0.000313276861956183 %\n",
      "Total no of parameters in Model Theta 11 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0129\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 99.04 & Test Loss: 0.00028477893380841126 %\n",
      "Total no of parameters in Model Theta 12 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0206\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.95 & Test Loss: 0.00030146290631164445 %\n",
      "Total no of parameters in Model Theta 13 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0387\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.76 & Test Loss: 0.00037538432283181465 %\n",
      "Total no of parameters in Model Theta 14 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0623\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.25 & Test Loss: 0.0005054445686808322 %\n",
      "Total no of parameters in Model Theta 15 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0781\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.01 & Test Loss: 0.0005936057168291882 %\n",
      "Total no of parameters in Model Theta 16 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.0868\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 97.86 & Test Loss: 0.0006750292377313599 %\n",
      "Total no of parameters in Model Theta 17 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.1081\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 97.33 & Test Loss: 0.0008775066933594644 %\n",
      "Total no of parameters in Model Theta 18 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.1992\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 95.82 & Test Loss: 0.0014335149963386356 %\n",
      "Total no of parameters in Model Theta 19 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.4076\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 92.74 & Test Loss: 0.002440277622267604 %\n",
      "Total no of parameters in Model Theta 20 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/1], Step [60/100], Loss: 0.7779\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 87.5 & Test Loss: 0.0041269039921462535 %\n"
     ]
    }
   ],
   "source": [
    "modelsTrainEpochArr = []\n",
    "modelsTrainLossArr = []\n",
    "modelsTrainAccArr = []\n",
    "modelsTestLossArr = []\n",
    "modelsTestAccArr = []\n",
    "\n",
    "for i in range (len(thetaArr)):\n",
    "    torch.manual_seed(1)\n",
    "    j=copy.deepcopy(i) \n",
    "    theta = (1-alpha[i])*batch1_param + alpha[i]*batch2_param\n",
    "    j = ThetaModel()\n",
    "    torch.nn.utils.vector_to_parameters(theta,j.parameters())\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(j.parameters(), lr=1e-3, weight_decay = 1e-3)\n",
    "\n",
    "    a=[]\n",
    "    for k in j.parameters():\n",
    "        a.append(torch.numel(k))\n",
    "    print(f'Total no of parameters in Model Theta {i} is:{np.sum(a)}')\n",
    "\n",
    "    print(j.parameters)\n",
    "\n",
    "    max_epochs = 1\n",
    "    train_batch_size = 600\n",
    "    T_train_epoch,T_train_losses,T_train_acc = trainFunc(j,max_epochs,train_batch_size)\n",
    "    \n",
    "    \n",
    "    modelsTrainEpochArr.append(T_train_epoch)\n",
    "    modelsTrainLossArr.append(T_train_losses)\n",
    "    modelsTrainAccArr.append(T_train_acc)\n",
    "    \n",
    "    test_batch_size=100\n",
    "    T_acc, T_testLoss = testFunction(j,loss_func,test_batch_size)\n",
    "    modelsTestAccArr.append(T_acc)\n",
    "    modelsTestLossArr.append(T_testLoss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(modelsTrainAccArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanScore(dataArr):\n",
    "    meanModelData = []\n",
    "    for i in range (len(dataArr)):\n",
    "        meanScore = np.mean(dataArr[i])\n",
    "        meanModelData.append(meanScore)\n",
    "    return meanModelData\n",
    "\n",
    "def minScore(dataArr):\n",
    "    minModelScore = []\n",
    "    for i in range (len(dataArr)):\n",
    "        minScore = np.mean(dataArr[i])\n",
    "        minModelScore.append(minScore)\n",
    "    return minModelScore\n",
    "\n",
    "def maxScore(dataArr):\n",
    "    maxModelScore = []\n",
    "    for i in range (len(dataArr)):\n",
    "        maxScore = np.max(dataArr[i])\n",
    "        maxModelScore.append(maxScore)\n",
    "    return maxModelScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(alpha,minScore(modelsTrainLossArr),color=\"Blue\",linestyle='dashed', marker=\"o\")\n",
    "ax.plot(alpha,modelsTestLossArr,color=\"Blue\", marker=\"v\")\n",
    "ax.legend(['Train Loss','Test Loss'],loc=\"center left\")\n",
    "ax.set_xlabel(\"Alpha\",color=\"Green\")\n",
    "ax.set_ylabel(\"CrossEntropy Loss\",color = \"blue\")\n",
    "\n",
    "\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(alpha,meanScore(modelsTrainAccArr),color=\"red\",linestyle='dashed', marker=\"o\")\n",
    "ax2.plot(alpha,modelsTestAccArr,color=\"red\", marker=\"v\")\n",
    "ax2.set_xlabel(\"Alpha\",color=\"Green\")\n",
    "ax2.set_ylabel(\"Accuracy\",color = \"red\")\n",
    "ax2.legend(['Train Acc','Test Acc'],loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('D:/Clemson/COURSE/SEM-2/CPSC-8430 Deep Learning - 001/Homework/CPSC-8430-Deep-Learning-001/HW1/Diff Batch Graph HW1_3.1-2.jpg',\n",
    "            format='jpeg',\n",
    "            dpi=100,\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model with batch_size=600 is:37160\n"
     ]
    }
   ],
   "source": [
    "# Training Model with batch size=600 and Lr 1e-3\n",
    "torch.manual_seed(1)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "mLr1 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mLr1.parameters(), lr=learning_rate) \n",
    "\n",
    "a=[]\n",
    "for i in mLr1.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model with batch_size={600} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "Train O/P: Epoch [1/15], Step [60/100], Loss: 0.6050\n",
      "Train O/P: Epoch [2/15], Step [60/100], Loss: 0.3066\n",
      "Train O/P: Epoch [3/15], Step [60/100], Loss: 0.1868\n",
      "Train O/P: Epoch [4/15], Step [60/100], Loss: 0.1115\n",
      "Train O/P: Epoch [5/15], Step [60/100], Loss: 0.1018\n",
      "Train O/P: Epoch [6/15], Step [60/100], Loss: 0.1589\n",
      "Train O/P: Epoch [7/15], Step [60/100], Loss: 0.0668\n",
      "Train O/P: Epoch [8/15], Step [60/100], Loss: 0.0962\n",
      "Train O/P: Epoch [9/15], Step [60/100], Loss: 0.0827\n",
      "Train O/P: Epoch [10/15], Step [60/100], Loss: 0.0940\n",
      "Train O/P: Epoch [11/15], Step [60/100], Loss: 0.0860\n",
      "Train O/P: Epoch [12/15], Step [60/100], Loss: 0.0474\n",
      "Train O/P: Epoch [13/15], Step [60/100], Loss: 0.0756\n",
      "Train O/P: Epoch [14/15], Step [60/100], Loss: 0.0663\n",
      "Train O/P: Epoch [15/15], Step [60/100], Loss: 0.0371\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 15\n",
    "train_batch_size = 600\n",
    "L1_train_epoch,L1_train_losses,L1_train_acc = trainFunc(mLr1,max_epochs,train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1359, -0.0415,  0.0009,  ...,  0.0156,  0.0185, -0.0429],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 37160\n"
     ]
    }
   ],
   "source": [
    "Lr1_param = torch.nn.utils.parameters_to_vector(mLr1.parameters())\n",
    "print(Lr1_param,'\\nlen:',len(Lr1_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model with batch_size=600 is:37160\n"
     ]
    }
   ],
   "source": [
    "# Training Model with batch size=600 and Lr 1e-2\n",
    "torch.manual_seed(1)\n",
    "learning_rate = 1e-2\n",
    "mLr2 = M1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mLr2.parameters(), lr=learning_rate) \n",
    "\n",
    "a=[]\n",
    "for i in mLr2.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print(f'Total no of parameters in Model with batch_size={600} is:{np.sum(a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strated\n",
      "Train O/P: Epoch [1/15], Step [60/100], Loss: 0.1845\n",
      "Train O/P: Epoch [2/15], Step [60/100], Loss: 0.0812\n",
      "Train O/P: Epoch [3/15], Step [60/100], Loss: 0.0607\n",
      "Train O/P: Epoch [4/15], Step [60/100], Loss: 0.0380\n",
      "Train O/P: Epoch [5/15], Step [60/100], Loss: 0.0390\n",
      "Train O/P: Epoch [6/15], Step [60/100], Loss: 0.0608\n",
      "Train O/P: Epoch [7/15], Step [60/100], Loss: 0.0173\n",
      "Train O/P: Epoch [8/15], Step [60/100], Loss: 0.0344\n",
      "Train O/P: Epoch [9/15], Step [60/100], Loss: 0.0255\n",
      "Train O/P: Epoch [10/15], Step [60/100], Loss: 0.0150\n",
      "Train O/P: Epoch [11/15], Step [60/100], Loss: 0.0230\n",
      "Train O/P: Epoch [12/15], Step [60/100], Loss: 0.0162\n",
      "Train O/P: Epoch [13/15], Step [60/100], Loss: 0.0242\n",
      "Train O/P: Epoch [14/15], Step [60/100], Loss: 0.0359\n",
      "Train O/P: Epoch [15/15], Step [60/100], Loss: 0.0235\n",
      "Max Epoch Reached\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 15\n",
    "train_batch_size = 600\n",
    "L2_train_epoch,L2_train_losses,L2_train_acc = trainFunc(mLr2,max_epochs,train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting Loss plot for both Batch 1 and Batch 2 models\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(L1_train_epoch,L1_train_losses,color=\"blue\")\n",
    "plt.plot(L2_train_epoch,L2_train_losses,color=\"orange\")\n",
    "plt.title('Lr1 & Lr2 Loss VS Epoch',color=\"green\")\n",
    "plt.legend(['LR1:1e-3 1','LR2:1e-2'])\n",
    "plt.xlabel ('Epoch')\n",
    "plt.ylabel ('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1326, -0.0430, -0.0602,  ...,  0.0240,  0.1268, -0.0453],\n",
      "       grad_fn=<CatBackward0>) \n",
      "len: 37160\n"
     ]
    }
   ],
   "source": [
    "Lr2_param = torch.nn.utils.parameters_to_vector(mLr2.parameters())\n",
    "print(Lr2_param,'\\nlen:',len(Lr2_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model Theta 0 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 1089.3296\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 360.0997\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 182.9377\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 166.1089\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 119.5325\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 86.4572\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 69.0773\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 54.4664\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 69.8954\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 60.5991\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 48.8338\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 43.3977\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 45.8358\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 29.0469\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 46.3312\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 22.2333\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 44.8665\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 33.0703\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 28.7660\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 20.9117\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 39.6131\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 31.5458\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 14.1364\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 16.0391\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 28.4555\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 19.8543\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 28.1350\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 24.1907\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 18.2828\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 26.6634\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 17.2491\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 17.0274\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 16.4037\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 28.4373\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 15.0682\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 14.5586\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 27.6834\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 16.5779\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 16.6280\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 4.9800\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 12.9221\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 9.7653\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 11.9477\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 14.9816\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 11.0866\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 13.1428\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 15.5901\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 10.6051\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 7.4695\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 17.4399\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 16.7509\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 5.3685\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 7.2633\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 19.4822\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 4.3033\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 6.8868\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 12.5623\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 6.9692\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 13.8561\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 8.1938\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 5.0491\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 6.0175\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 8.4773\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 5.9848\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 11.2200\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 5.1031\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 4.4292\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 8.3596\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 9.3248\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 5.7597\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 3.4229\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 7.5260\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 3.3633\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 6.4584\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 3.9512\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 4.9078\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 3.6700\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 2.4697\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 3.7755\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 8.7145\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 2.4406\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 1.1974\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 3.8614\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 2.5414\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 6.5651\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 6.2479\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 5.4087\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 3.1437\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.1693\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 4.5420\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 4.3538\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 2.0842\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.8512\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 1.2197\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 1.9980\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 3.4967\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 3.1043\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 1.5395\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 2.4832\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 3.7816\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 93.08 & Test Loss: 0.02929633630964007 %\n",
      "Total no of parameters in Model Theta 1 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 523.2776\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 218.8172\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 110.7292\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 97.3674\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 65.4695\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 45.0231\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 42.8397\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 30.2036\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 35.2847\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 31.4991\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 26.7918\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 25.4291\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 24.8545\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 17.6760\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 24.7918\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 12.8422\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 27.3502\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 19.5596\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 17.0351\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 15.9137\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 21.9780\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 17.5336\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 5.9614\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 10.7676\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 15.9248\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 11.0715\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 15.1887\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 14.7569\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 8.3783\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 11.1947\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 13.1738\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 8.2100\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 10.2476\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 21.3385\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 6.6975\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 11.0751\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 13.5529\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 10.2663\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 8.4206\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 2.2146\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 5.3164\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 3.4332\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 10.1740\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 7.9279\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 4.6426\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 9.3365\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 8.6022\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 7.5183\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 3.5233\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 8.6833\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 8.3316\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 4.2979\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 2.8242\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 3.0838\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 3.1121\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 2.8642\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 6.4882\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 4.5660\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 6.6889\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 4.2617\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 3.6434\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 3.1775\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 5.4684\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 2.7772\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 6.4021\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 2.3617\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 2.5517\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 5.8943\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 4.8129\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 2.4377\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 2.7262\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 4.4280\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 2.8267\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 5.5325\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 2.7929\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 3.0088\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 2.0671\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.5677\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 2.1551\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 5.0173\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 1.7751\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 1.4436\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 2.4729\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 1.0374\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 3.5334\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 2.7225\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 3.6485\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 1.5273\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.3851\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 1.8302\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 2.2623\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.9793\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.3153\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.7766\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 1.2551\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 1.2402\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.9754\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 1.0429\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 1.2971\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.9597\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 94.19 & Test Loss: 0.013396357166463707 %\n",
      "Total no of parameters in Model Theta 2 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 307.6625\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 118.7286\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 62.8539\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 49.7364\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 37.4870\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 24.6563\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 23.8835\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 17.3964\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 23.3975\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 19.2065\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 17.7382\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 14.4932\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 15.0879\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 8.9654\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 15.5122\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 7.3555\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 12.5743\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 9.8955\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 8.9220\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 4.9561\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 10.9557\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 9.7083\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 4.5445\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 3.5669\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 10.0686\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 5.1431\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 9.0439\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 9.1884\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 3.9702\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 7.2571\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 4.4797\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 3.4600\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 5.4923\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 10.6480\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 3.0269\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 9.4039\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 5.8320\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 5.5971\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 4.1458\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.9356\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 2.8996\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 1.1001\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 5.4018\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 3.6518\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 1.9872\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 4.6739\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 3.5968\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 3.8827\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 2.6302\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 4.7827\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 4.2757\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 3.2213\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 1.2791\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 1.1140\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 1.5573\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 1.0612\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 2.8480\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 2.1977\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 4.1680\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 2.0648\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 2.6007\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 1.6640\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 2.7966\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 2.1605\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 4.6391\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 1.2543\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 2.0220\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 2.8851\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 3.2466\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 2.2466\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 1.9859\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 2.0683\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 2.3668\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 1.3403\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 1.7475\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 2.7000\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.9698\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.3833\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.9752\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 2.7137\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 1.1735\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.8092\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 1.4141\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.3789\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.9512\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 1.0801\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 1.5641\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.9926\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.5209\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.7542\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 1.1089\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.3365\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0633\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.8730\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.4031\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.5012\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.4416\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.5357\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.5177\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.1616\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 94.3 & Test Loss: 0.007572144470051418 %\n",
      "Total no of parameters in Model Theta 3 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 153.1451\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 60.5232\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 33.3136\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 24.0545\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 17.1431\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 12.4097\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 10.9284\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 10.7413\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 12.1319\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 9.6527\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 10.9624\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 7.6214\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 8.0547\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 5.1876\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 8.2629\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 2.8610\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 7.8115\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 6.4770\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 4.0306\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 2.1127\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 6.7907\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 3.8982\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 3.0887\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 2.3223\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 5.7286\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 2.6085\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 3.8071\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 4.0017\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 1.4858\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 3.9617\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 2.0626\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 1.0343\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 1.7072\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 6.2991\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 1.8235\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 2.3689\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 2.6475\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 3.4141\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 1.8867\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.6219\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 1.7717\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.3902\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 2.5570\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 1.8620\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 1.1130\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 2.5279\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 1.5552\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 1.8271\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 1.6881\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 2.4301\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 2.4844\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 1.2565\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 1.6891\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.7476\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.6932\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.6221\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 1.3364\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 1.3652\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 2.5407\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 1.0994\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.9476\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.4694\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 1.4462\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.7906\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 2.0484\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.5942\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.5202\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 1.4162\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 1.6643\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 1.0187\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.4171\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.9314\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.5655\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.8195\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.5352\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 1.3377\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.2778\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.1232\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.7214\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 1.3198\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.6691\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.5263\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.5938\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.2407\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.5318\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.5185\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.6613\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.4803\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.1533\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.2842\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.5899\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.1626\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0166\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.3985\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0434\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.2711\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0660\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.2712\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.4420\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.1808\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 95.08 & Test Loss: 0.003616108191724959 %\n",
      "Total no of parameters in Model Theta 4 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 68.0893\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 26.3998\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 17.1042\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 10.6512\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 7.4326\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 4.5591\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 4.5133\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 6.5311\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 6.4630\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 4.5539\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 6.2726\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 3.9719\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 3.3047\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 2.4994\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 3.6137\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 1.8700\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 4.0087\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 3.1191\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 1.7146\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.8609\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 2.6300\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 2.4337\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 1.3222\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.8759\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 2.6270\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 1.3115\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 1.7069\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 2.2135\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.8216\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 1.9131\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.8007\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.3068\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 1.1614\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 3.4996\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.4558\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.9822\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 1.5610\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 1.5799\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.9741\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.1832\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 1.1016\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.4062\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.5538\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 1.4033\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.3414\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 1.8391\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.4942\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.8591\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.7520\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.8422\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.8440\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.4719\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.4917\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.2681\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.3160\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.5891\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.6711\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.5965\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 1.2629\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.6794\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.2821\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.3179\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.7126\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.6488\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.8420\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.1979\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.2584\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.8361\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.6453\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.2753\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0863\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.2715\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.3043\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.4181\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.2273\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.4593\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.3206\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0594\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.4474\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.5352\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.3238\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.1920\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.3506\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0455\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.3036\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.3052\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.2274\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0914\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0870\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.1103\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.2021\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0553\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0173\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.2186\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.1041\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0867\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0157\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.1233\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.2830\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0259\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 96.27 & Test Loss: 0.001679167950869305 %\n",
      "Total no of parameters in Model Theta 5 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 29.1250\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 9.7829\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 7.3911\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 4.0992\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 2.5545\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 2.1504\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 1.8851\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 2.1156\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 3.2333\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 1.6882\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 3.2196\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 1.1735\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 1.6659\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 1.0600\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 1.3745\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.8963\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 2.0373\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 1.0728\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.6612\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.4131\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 1.1498\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 1.2203\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.7476\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.3948\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 1.0678\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.6405\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.5028\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 1.5485\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.5213\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.8124\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.3607\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0755\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.6595\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 1.2342\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.3418\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.2435\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.5453\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.9130\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.3919\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.2999\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.4514\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0549\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.4005\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.5477\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.2163\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.6218\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.3839\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.3793\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.2842\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.5297\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.3678\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.3578\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.3321\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0815\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.1022\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.1205\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.3739\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.2899\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.5135\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.3570\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0406\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.2088\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.3358\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.1482\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.2271\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.1179\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.1911\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.4029\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.3501\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.1514\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.1016\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.1813\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0474\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.2465\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.1022\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.2315\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.2264\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0308\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.1890\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.2540\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.1222\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0660\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.1896\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0122\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.1326\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.1625\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.1906\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0631\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0456\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.1269\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0918\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0353\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0352\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0633\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0933\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0505\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0144\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0648\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.1685\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0338\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 96.66 & Test Loss: 0.0010725297429366037 %\n",
      "Total no of parameters in Model Theta 6 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 8.0344\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 2.7540\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 2.4613\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 1.4838\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.7931\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.8571\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.8233\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 1.1085\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 1.2373\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.6413\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 1.1695\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.5445\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.6927\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.3221\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.4449\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.3190\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.6516\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.5662\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.3091\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.3413\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.4545\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.4400\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.2486\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.1791\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.5240\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.2786\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.2435\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.5627\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.2933\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.3385\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.2149\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0048\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.2301\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.4307\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.1875\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.1736\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.2202\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.4149\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.1485\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.2152\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.1380\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0220\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.1660\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.2973\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0557\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.3115\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.1937\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.1394\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.1089\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.2129\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.2210\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0911\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.1560\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0480\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0199\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0481\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.1792\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.1864\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.2528\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.2133\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0254\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0921\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.1820\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0421\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.1654\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0764\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0833\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.2409\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.2035\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0799\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0655\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.1166\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0283\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.1945\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0718\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.1437\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.2052\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0254\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.1191\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.1706\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0878\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0384\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0816\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0137\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.1031\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0931\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.1203\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0440\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0421\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0571\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0545\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0319\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0396\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0344\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0705\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0379\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0129\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0112\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0744\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0747\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 97.69 & Test Loss: 0.0006984521743477671 %\n",
      "Total no of parameters in Model Theta 7 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 2.5331\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.8900\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 1.2050\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.6874\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.4675\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.3342\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.2862\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.4460\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.4689\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.1941\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.6310\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.3080\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.1994\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.1361\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.1131\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.2053\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.2643\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.2096\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.1036\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.1248\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.2565\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.1597\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0901\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0543\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.2293\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.1187\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0652\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.3766\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.1149\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.1049\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0821\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0297\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.1788\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.2479\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.1243\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.1008\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.1597\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.2513\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0738\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0372\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.1078\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0644\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.1006\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.1697\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0289\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.1843\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0946\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.1088\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.1167\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.1555\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.1465\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0846\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.1192\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0340\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0120\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0872\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.1153\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.1324\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.1493\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0844\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0104\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0457\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.1284\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0152\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.1111\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0468\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0375\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.1677\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0842\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0551\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0342\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0788\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0564\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0832\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0452\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0636\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.1366\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0276\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.1008\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.1592\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0413\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0436\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0838\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0424\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0705\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0961\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0822\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0154\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0634\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0796\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0329\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0326\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0381\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0247\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0440\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0412\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0166\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0040\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0537\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0752\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.16 & Test Loss: 0.0005299166733791935 %\n",
      "Total no of parameters in Model Theta 8 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.3821\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.1670\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.4550\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.2057\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.1680\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0955\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0835\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.1492\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.1763\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0513\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.2848\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.1364\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0866\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0625\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0711\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.1080\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0601\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.1589\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0525\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0378\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.1468\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0563\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0414\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0308\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.1430\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0952\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0442\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.2010\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0438\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0776\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0325\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0223\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0743\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.1712\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.1032\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0833\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0876\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.1281\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0478\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0135\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0928\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0615\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0325\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0838\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0113\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0938\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0574\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.1023\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0799\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0916\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.1036\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0698\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0876\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0610\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0111\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.1067\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0820\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0655\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0959\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0386\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0144\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0339\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0784\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0187\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0546\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0481\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0314\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.1233\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0404\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0501\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0157\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0544\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0477\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0548\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0720\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0209\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.1318\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0079\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0693\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.1212\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0168\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0203\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0641\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0406\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0338\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0680\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0545\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0241\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0689\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0217\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0316\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0271\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0238\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0090\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0143\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0668\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0296\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0033\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0202\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.1027\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.45 & Test Loss: 0.0004397354414330039 %\n",
      "Total no of parameters in Model Theta 9 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.1087\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0185\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.1410\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0536\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0625\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0527\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0392\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0711\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0712\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0200\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0984\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0742\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0791\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0241\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0445\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0190\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0444\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.1003\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0255\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0385\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0949\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0648\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0389\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0620\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0832\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0681\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0321\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.1164\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0486\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0515\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0159\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0302\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0747\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0681\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0699\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0680\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0904\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.1113\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0392\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0212\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0856\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0248\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0156\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0245\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0048\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0533\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0636\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.1098\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0478\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0429\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0661\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0178\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0496\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0417\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0071\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0603\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0653\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0549\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0435\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0268\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0147\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0406\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0755\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0247\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0360\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0239\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0083\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.1109\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0474\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0436\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0142\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0179\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0411\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0344\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0368\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0176\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0712\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0110\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0245\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0611\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0196\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0057\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0300\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0215\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0068\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0697\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0283\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0126\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0289\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0109\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0102\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0135\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0365\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0131\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0194\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0688\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0085\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0016\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0045\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0269\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.92 & Test Loss: 0.00034881473014193034 %\n",
      "Total no of parameters in Model Theta 10 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0212\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0409\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.1196\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0457\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0368\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0818\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0394\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0470\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0546\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0905\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.1163\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0492\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0488\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0292\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0385\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0236\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0326\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.1230\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0190\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0325\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0971\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0643\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0403\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0630\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.1180\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0391\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0321\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0795\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0384\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0612\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0066\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0312\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0378\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0432\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0405\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0615\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0738\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0443\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0320\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0057\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0657\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0245\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0098\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0175\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0055\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0588\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0569\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0751\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0324\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0433\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0283\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0286\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0305\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0189\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0052\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0441\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0821\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0332\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0511\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0136\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0091\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0185\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0470\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0225\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0145\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0240\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0063\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.1030\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0205\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0519\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0116\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0238\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0177\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0149\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0218\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0199\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0394\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0156\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0417\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0582\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0211\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0017\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0328\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0276\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0056\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0325\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0362\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0072\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0058\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0256\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0092\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0030\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0093\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0068\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0290\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0353\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0026\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0018\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0008\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0246\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.84 & Test Loss: 0.0003667798863740245 %\n",
      "Total no of parameters in Model Theta 11 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0273\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0367\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0701\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0611\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0227\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0760\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0243\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0431\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0369\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0604\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0541\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0305\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0404\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0153\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0343\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0179\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0112\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0921\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0102\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0328\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0722\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0072\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0554\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0508\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0901\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0141\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0356\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0415\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0506\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0693\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0090\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0060\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0188\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0302\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0185\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0585\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0923\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0485\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0237\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0013\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0300\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0079\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0051\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0386\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0042\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0383\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0144\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0673\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0132\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0122\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0222\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0229\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0206\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0188\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0070\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0095\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0517\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0437\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0122\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0028\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0099\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0090\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.1071\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0039\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0072\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0113\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0106\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0108\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0642\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0357\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0052\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0226\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0029\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0097\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0098\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0072\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0113\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0027\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0307\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0209\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0167\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0035\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0036\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0265\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0030\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0305\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0300\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0047\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0039\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0069\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0053\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0036\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0060\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0006\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0102\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0068\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0027\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0026\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0026\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0015\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.98 & Test Loss: 0.0003596792273197707 %\n",
      "Total no of parameters in Model Theta 12 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0123\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0184\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0659\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0118\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0192\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0446\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0100\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0189\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0177\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0215\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0170\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0222\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0320\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0090\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0169\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0034\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0087\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0198\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0074\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0399\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0484\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0094\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0169\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0258\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0404\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0041\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0378\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0211\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0232\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0205\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0110\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0044\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0066\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0160\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0103\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0102\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0382\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0029\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0034\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0004\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0049\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0007\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0310\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0027\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0069\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0046\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0227\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0060\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0046\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0020\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0014\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0121\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0005\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0007\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0047\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0041\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0751\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0053\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0011\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0045\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0036\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0032\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0010\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0075\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0019\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0247\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0087\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0152\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0005\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0007\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0012\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0011\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0009\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0008\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0028\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0153\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0043\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0173\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0028\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0189\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0042\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0128\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0126\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0007\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0008\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0032\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0005\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0005\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0037\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0009\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0088\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0106\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.92 & Test Loss: 0.00041174454435837903 %\n",
      "Total no of parameters in Model Theta 13 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0072\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0105\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0270\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0150\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0056\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0189\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0050\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0140\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0031\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0086\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0118\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0159\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0253\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0027\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0095\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0004\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0006\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0210\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0086\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0372\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0046\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0021\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0110\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0053\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0301\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0020\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0247\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0020\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0162\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0085\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0039\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0003\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0028\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0028\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0034\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0039\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0550\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0043\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0089\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0006\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0011\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0005\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0004\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0169\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0030\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0009\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0123\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0124\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0013\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0026\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0076\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0550\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0016\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0062\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0135\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0199\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0023\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0005\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0011\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0022\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0011\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0003\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0029\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0073\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0027\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0005\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0010\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0014\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0029\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0002\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0002\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0088\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0033\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0017\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0007\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0007\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0059\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0148\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0015\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0008\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0004\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0004\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0009\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0078\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0217\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0010\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0006\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.85 & Test Loss: 0.00047399860396980846 %\n",
      "Total no of parameters in Model Theta 14 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0014\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0131\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0223\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0024\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0019\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0109\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0016\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0051\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0024\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0145\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0036\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0120\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0183\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0005\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0010\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0005\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0032\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0017\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0379\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0039\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0003\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0007\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0006\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0110\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0017\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0338\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0012\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0102\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0033\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0022\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0003\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0038\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0016\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0037\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0290\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0004\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0014\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0163\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0003\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0007\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0028\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0021\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0009\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0009\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0166\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0047\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0417\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0009\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0013\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0003\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0012\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0220\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0003\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0025\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0027\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0021\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0003\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0025\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0002\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0071\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0066\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0012\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0080\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0028\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0002\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0010\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0189\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0083\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0346\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0014\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0012\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0217\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.59 & Test Loss: 0.0008563340627944698 %\n",
      "Total no of parameters in Model Theta 15 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0003\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0146\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0207\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0147\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0016\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0064\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0001\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0013\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0025\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0144\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0044\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0019\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0092\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0015\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0007\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0007\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0016\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0004\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0097\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0044\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0027\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0017\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0632\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0098\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0021\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0009\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0008\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0007\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0008\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0705\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0003\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0038\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0031\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0017\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0010\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0010\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0130\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0152\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0009\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0034\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0013\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0169\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0011\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0023\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0004\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0010\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0018\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0002\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0128\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0242\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0012\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0004\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0031\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0003\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0722\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0004\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0005\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0018\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.84 & Test Loss: 0.0006829750378081673 %\n",
      "Total no of parameters in Model Theta 16 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0072\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0400\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0082\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0022\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0029\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0083\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0014\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0027\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0041\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0009\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0012\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0012\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0006\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0016\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0018\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0151\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0005\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0001\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0007\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0349\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0039\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0018\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0017\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0264\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0116\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0007\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0091\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0131\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0002\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0559\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0008\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0226\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0073\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0183\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0003\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0012\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0004\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0117\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0030\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0183\n",
      "Train O/P: Epoch [9/10], Step [60/600], Loss: 0.0013\n",
      "Train O/P: Epoch [9/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [9/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [240/600], Loss: 0.0010\n",
      "Train O/P: Epoch [9/10], Step [300/600], Loss: 0.0026\n",
      "Train O/P: Epoch [9/10], Step [360/600], Loss: 0.0045\n",
      "Train O/P: Epoch [9/10], Step [420/600], Loss: 0.0013\n",
      "Train O/P: Epoch [9/10], Step [480/600], Loss: 0.0031\n",
      "Train O/P: Epoch [9/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [9/10], Step [600/600], Loss: 0.0003\n",
      "Train O/P: Epoch [10/10], Step [60/600], Loss: 0.0002\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [120/600], Loss: 0.0007\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [180/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [240/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [300/600], Loss: 0.0001\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [360/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [420/600], Loss: 0.0071\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [480/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [540/600], Loss: 0.0004\n",
      "Max Epoch Reached\n",
      "Train O/P: Epoch [10/10], Step [600/600], Loss: 0.0000\n",
      "Max Epoch Reached\n",
      "Accuracy of the network on the test images: 98.64 & Test Loss: 0.0009632429874012747 %\n",
      "Total no of parameters in Model Theta 17 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0006\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0251\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0199\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0028\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0015\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0014\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0033\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0014\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0186\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0034\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0002\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0002\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0017\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0006\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0847\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0004\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0008\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0016\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0003\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0089\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0005\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0253\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0026\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0020\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0007\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0112\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0014\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0200\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0008\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0008\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0003\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0010\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0018\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0006\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0025\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0013\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0005\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0652\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0014\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0006\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0212\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0007\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0003\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 6.429157110687811e-06\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0091\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0010\n",
      "Accuracy of the network on the test images: 98.83 & Test Loss: 0.0007886266246837219 %\n",
      "Total no of parameters in Model Theta 18 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.0035\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0483\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0027\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0206\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0006\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0016\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0278\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0015\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0757\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0135\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0052\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0002\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0011\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0748\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0023\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0094\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.0447\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0032\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0005\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0005\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0250\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0217\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0015\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0036\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0004\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0012\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0089\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 1.0014719009632245e-05\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0357\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0001\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0020\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Accuracy of the network on the test images: 98.74 & Test Loss: 0.0011467837225579832 %\n",
      "Total no of parameters in Model Theta 19 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.1056\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0222\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.0054\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0224\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0232\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.0201\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.0954\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.0243\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0274\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0079\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0004\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.1515\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0022\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0012\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0008\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0246\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.1240\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0269\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0139\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0281\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0719\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0005\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0004\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0002\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0005\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0554\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0009\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0159\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0051\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0007\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 1.8559196632850217e-06\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 7.223961233648879e-07\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0014\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0006\n",
      "Accuracy of the network on the test images: 98.71 & Test Loss: 0.0014466508991596474 %\n",
      "Total no of parameters in Model Theta 20 is:37160\n",
      "<bound method Module.parameters of ThetaModel(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 13, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=208, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n",
      "strated\n",
      "Train O/P: Epoch [1/10], Step [60/600], Loss: 0.5395\n",
      "Train O/P: Epoch [1/10], Step [120/600], Loss: 0.0288\n",
      "Train O/P: Epoch [1/10], Step [180/600], Loss: 0.0333\n",
      "Train O/P: Epoch [1/10], Step [240/600], Loss: 0.1712\n",
      "Train O/P: Epoch [1/10], Step [300/600], Loss: 0.0032\n",
      "Train O/P: Epoch [1/10], Step [360/600], Loss: 0.0008\n",
      "Train O/P: Epoch [1/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [1/10], Step [540/600], Loss: 0.1512\n",
      "Train O/P: Epoch [1/10], Step [600/600], Loss: 0.1166\n",
      "Train O/P: Epoch [2/10], Step [60/600], Loss: 0.0004\n",
      "Train O/P: Epoch [2/10], Step [120/600], Loss: 0.1034\n",
      "Train O/P: Epoch [2/10], Step [180/600], Loss: 0.0194\n",
      "Train O/P: Epoch [2/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [300/600], Loss: 0.0009\n",
      "Train O/P: Epoch [2/10], Step [360/600], Loss: 0.0008\n",
      "Train O/P: Epoch [2/10], Step [420/600], Loss: 0.0427\n",
      "Train O/P: Epoch [2/10], Step [480/600], Loss: 0.0007\n",
      "Train O/P: Epoch [2/10], Step [540/600], Loss: 0.0000\n",
      "Train O/P: Epoch [2/10], Step [600/600], Loss: 0.0429\n",
      "Train O/P: Epoch [3/10], Step [60/600], Loss: 0.0526\n",
      "Train O/P: Epoch [3/10], Step [120/600], Loss: 0.0002\n",
      "Train O/P: Epoch [3/10], Step [180/600], Loss: 0.0003\n",
      "Train O/P: Epoch [3/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [300/600], Loss: 0.0149\n",
      "Train O/P: Epoch [3/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [3/10], Step [420/600], Loss: 0.2667\n",
      "Train O/P: Epoch [3/10], Step [480/600], Loss: 0.0851\n",
      "Train O/P: Epoch [3/10], Step [540/600], Loss: 0.0111\n",
      "Train O/P: Epoch [3/10], Step [600/600], Loss: 0.0010\n",
      "Train O/P: Epoch [4/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [240/600], Loss: 0.0509\n",
      "Train O/P: Epoch [4/10], Step [300/600], Loss: 0.0010\n",
      "Train O/P: Epoch [4/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [4/10], Step [420/600], Loss: 0.0293\n",
      "Train O/P: Epoch [4/10], Step [480/600], Loss: 0.0033\n",
      "Train O/P: Epoch [4/10], Step [540/600], Loss: 0.0001\n",
      "Train O/P: Epoch [4/10], Step [600/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [60/600], Loss: 0.0006\n",
      "Train O/P: Epoch [5/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [180/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [240/600], Loss: 0.0246\n",
      "Train O/P: Epoch [5/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [360/600], Loss: 0.0001\n",
      "Train O/P: Epoch [5/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [5/10], Step [540/600], Loss: 0.0005\n",
      "Train O/P: Epoch [5/10], Step [600/600], Loss: 0.0009\n",
      "Train O/P: Epoch [6/10], Step [60/600], Loss: 0.0186\n",
      "Train O/P: Epoch [6/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [300/600], Loss: 0.0013\n",
      "Train O/P: Epoch [6/10], Step [360/600], Loss: 0.0002\n",
      "Train O/P: Epoch [6/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [480/600], Loss: 0.0000\n",
      "Train O/P: Epoch [6/10], Step [540/600], Loss: 0.0602\n",
      "Train O/P: Epoch [6/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [60/600], Loss: 0.0001\n",
      "Train O/P: Epoch [7/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [180/600], Loss: 0.0004\n",
      "Train O/P: Epoch [7/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [360/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [7/10], Step [480/600], Loss: 0.0277\n",
      "Train O/P: Epoch [7/10], Step [540/600], Loss: 0.0002\n",
      "Train O/P: Epoch [7/10], Step [600/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [60/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [120/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [180/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [240/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [300/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [360/600], Loss: 0.0000\n",
      "Convergeance reached for loss: 2.38418573772492e-09\n",
      "Train O/P: Epoch [8/10], Step [420/600], Loss: 0.0000\n",
      "Train O/P: Epoch [8/10], Step [480/600], Loss: 0.0011\n",
      "Train O/P: Epoch [8/10], Step [540/600], Loss: 0.0263\n",
      "Train O/P: Epoch [8/10], Step [600/600], Loss: 0.0002\n",
      "Accuracy of the network on the test images: 98.78 & Test Loss: 0.0019091117901165497 %\n"
     ]
    }
   ],
   "source": [
    "modelsTrainEpochArr2 = []\n",
    "modelsTrainLossArr2 = []\n",
    "modelsTrainAccArr2 = []\n",
    "modelsTestLossArr2 = []\n",
    "modelsTestAccArr2 = []\n",
    "\n",
    "for i in range (len(thetaArr)):\n",
    "    torch.manual_seed(1)\n",
    "    j=copy.deepcopy(i) \n",
    "    theta = (1-alpha[i])*Lr1_param + alpha[i]*Lr2_param\n",
    "    j = ThetaModel()\n",
    "    torch.nn.utils.vector_to_parameters(theta,j.parameters())\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(j.parameters(), lr=1e-3) #,weight_decay = 0.025)\n",
    "\n",
    "    a=[]\n",
    "    for k in j.parameters():\n",
    "        a.append(torch.numel(k))\n",
    "    print(f'Total no of parameters in Model Theta {i} is:{np.sum(a)}')\n",
    "\n",
    "    print(j.parameters)\n",
    "\n",
    "    max_epochs = 10\n",
    "    train_batch_size = 100\n",
    "    T2_train_epoch,T2_train_losses,T2_train_acc = trainFunc(j,max_epochs,train_batch_size)\n",
    "    \n",
    "    \n",
    "    modelsTrainEpochArr2.append(T2_train_epoch)\n",
    "    modelsTrainLossArr2.append(T2_train_losses)\n",
    "    modelsTrainAccArr2.append(T2_train_acc)\n",
    "    \n",
    "    test_batch_size=100\n",
    "    T2_acc,T2_testLoss = testFunction(j,loss_func,test_batch_size)\n",
    "    modelsTestAccArr2.append(T2_acc)\n",
    "    modelsTestLossArr2.append(T2_testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89.62884194614863,\n",
       " 51.43520320745694,\n",
       " 27.694102717806896,\n",
       " 14.078875946624894,\n",
       " 6.572698178871884,\n",
       " 2.7852773498211754,\n",
       " 1.0177112113306843,\n",
       " 0.4059771665027365,\n",
       " 0.1413017183856185,\n",
       " 0.05372775089618517,\n",
       " 0.039741171537641395,\n",
       " 0.0275257305751414,\n",
       " 0.012272847957333095,\n",
       " 0.00662830408284439,\n",
       " 0.004287244827290162,\n",
       " 0.0034578299631879425,\n",
       " 0.003609596868031848,\n",
       " 0.005055599534849311,\n",
       " 0.010022570470187434,\n",
       " 0.019066808825665648,\n",
       " 0.030557849257920502]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minScore(modelsTrainLossArr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[81.44456822040547,\n",
       " 82.50044078844297,\n",
       " 83.61030656132422,\n",
       " 84.86601750826082,\n",
       " 86.57398280195477,\n",
       " 88.8769055760797,\n",
       " 91.53130063083748,\n",
       " 93.75643727351256,\n",
       " 96.18584909313564,\n",
       " 98.02543709598783,\n",
       " 98.71199005081381,\n",
       " 99.09878548874556,\n",
       " 99.58960300838024,\n",
       " 99.78020859433953,\n",
       " 99.86365214636047,\n",
       " 99.88946491441646,\n",
       " 99.89073192278288,\n",
       " 99.83773714314749,\n",
       " 99.71720785736764,\n",
       " 99.57900752525299,\n",
       " 99.47716564922071]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanScore(modelsTrainAccArr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(alpha,minScore(modelsTrainLossArr2),color=\"Blue\",linestyle='dashed', marker=\"o\")\n",
    "ax.plot(alpha,modelsTestLossArr2,color=\"Blue\", marker=\"v\")\n",
    "ax.legend(['Train Loss','Test Loss'],loc=\"center left\")\n",
    "ax.set_xlabel(\"Alpha\",color=\"Green\")\n",
    "ax.set_ylabel(\"CrossEntropy Loss\",color = \"blue\")\n",
    "ax.set_title(\"learning rate 1e-3 vs. 1e-2\",color = \"green\")\n",
    "\n",
    "\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(alpha,maxScore(modelsTrainAccArr2),color=\"red\",linestyle='dashed', marker=\"o\")\n",
    "ax2.plot(alpha,modelsTestAccArr2,color=\"red\", marker=\"v\")\n",
    "ax2.set_xlabel(\"Alpha\",color=\"Green\")\n",
    "ax2.set_ylabel(\"Accuracy\",color = \"red\")\n",
    "ax2.legend(['Train Acc','Test Acc'],loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('D:/Clemson/COURSE/SEM-2/CPSC-8430 Deep Learning - 001/Homework/CPSC-8430-Deep-Learning-001/HW1/Diff Batch Graph HW1_3.1Lr.jpg',\n",
    "            format='jpeg',\n",
    "            dpi=100,\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1d86b2f3ed665d691ce24c615a98bbc398f66743afc4d4e970e6f8b36fab2b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CPSC-8430-DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
