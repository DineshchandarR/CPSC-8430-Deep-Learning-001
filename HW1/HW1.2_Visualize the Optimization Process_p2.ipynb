{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cf28a588b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 60000 \n",
      "test_dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset),\"\\ntest_dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader func\n",
    "def train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def test_loader(batch_size):\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        # self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # flatten as one dimension\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def trainFunc(model,num_epochs,train_batch_size,status_interval):\n",
    "    model.train()\n",
    "    print('strated')\n",
    "    train_load = train_loader(train_batch_size)\n",
    "    n_total_steps = len(train_load)\n",
    "    train_losses = []\n",
    "    train_epoch = []\n",
    "    train_acc = []\n",
    "    epoch = 0\n",
    "    modelParamWgt={}\n",
    "    for epoch in range (num_epochs):\n",
    "        epoch += 1\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        lossSum =0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_load):  \n",
    "            #if (i+1)% 60 == 0 : print(i+1)\n",
    "            images, labels = Variable(images),Variable(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            prediction = model(images)\n",
    "\n",
    "            images.requires_grad = True\n",
    "\n",
    "            loss = loss_func(prediction, labels)\n",
    "            lossSum += loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_acc.append(acc)\n",
    "            train_epoch.append(epoch)\n",
    "\n",
    "            #print(epoch,i)\n",
    "            \n",
    "            #Weight Collection\n",
    "            if epoch % 3 == 0:\n",
    "                for name, parameter in model.named_parameters():\n",
    "                    #print(name)\n",
    "                    if'weight' in name:\n",
    "                        modelParamWgt[epoch] = torch.nn.utils.parameters_to_vector(parameter).detach().numpy()\n",
    "                        #print(modelParamWgt)\n",
    "\n",
    "            #Print Status\n",
    "            if (i+1) % status_interval == 0:\n",
    "                print (f'Train O/P: Epoch [{epoch}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}',end= '\\r',flush = True)\n",
    "               \n",
    "                        \n",
    "    trainAvgLoss = lossSum/train_batch_size\n",
    "    print(\"Train Avg loss:\",trainAvgLoss)\n",
    "                        \n",
    "    return train_epoch,train_losses,train_acc,trainAvgLoss, modelParamWgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunction(model,loss_func,test_batch_size): \n",
    "    test_load = test_loader(test_batch_size)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        testLoss = 0\n",
    "        count = 0\n",
    "        for images, labels in test_load:\n",
    "            images, labels = Variable(images),Variable(labels)\n",
    "            \n",
    "            prediction = model(images)\n",
    "            testLoss += loss_func(prediction,labels).item()\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            count += 1\n",
    "    netTest_loss = testLoss/count\n",
    "    netTest_acc1 = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the test images: {netTest_acc1}% & Test Loss: {netTest_loss}',end= '\\r',flush = True)\n",
    "    return netTest_acc1, netTest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaOps(paramDF,itr):\n",
    "    pcaOperation =  PCA(n_components=2)\n",
    "\n",
    "    scale = StandardScaler()\n",
    "\n",
    "    sData = scale.fit_transform(paramDF)\n",
    "\n",
    "    pcaVal = pcaOperation.fit_transform(sData)\n",
    "\n",
    "    itrData = np.full((pcaVal.shape[0],1),itr)\n",
    "\n",
    "    #pcaDf = pd.DataFrame(data = pcaVal, columns = ['x','y'])\n",
    "\n",
    "    pcaDf = pd.DataFrame(np.append(pcaVal,itrData,axis=1),columns=['x','y','Itr No.'])\n",
    "\n",
    "    return pcaDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 0\n",
      "strated\n",
      "Train Avg loss: tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "Iteration No: 1 network on the test images: 96.6% & Test Loss: 0.11637420050799846\n",
      "strated\n",
      "Train Avg loss: tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "Iteration No: 2 network on the test images: 97.15% & Test Loss: 0.09444695001351647\n",
      "strated\n",
      "Train Avg loss: tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "Iteration No: 3 network on the test images: 96.78% & Test Loss: 0.10001725410111248\n",
      "strated\n",
      "Train Avg loss: tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "Iteration No: 4 network on the test images: 96.75% & Test Loss: 0.10693266620626674\n",
      "strated\n",
      "Train Avg loss: tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "Iteration No: 5 network on the test images: 96.81% & Test Loss: 0.09956782429013401\n",
      "strated\n",
      "Train Avg loss: tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "Iteration No: 6 network on the test images: 96.8% & Test Loss: 0.10986123773036524\n",
      "strated\n",
      "Train Avg loss: tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "Iteration No: 7 network on the test images: 96.25% & Test Loss: 0.12490174120990559\n",
      "strated\n",
      "Train Avg loss: tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "Accuracy of the network on the test images: 96.7% & Test Loss: 0.11091293419944122\r"
     ]
    }
   ],
   "source": [
    "#Main Implementation\n",
    "\n",
    "modelParamArr = []\n",
    "pcaDf = pd.DataFrame(columns=['x','y','Itr No.'])\n",
    "\n",
    "\n",
    "for itr in range(8):\n",
    "    print('Iteration No:', itr)\n",
    "    j = copy.deepcopy(itr) \n",
    "    j = Model()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(j.parameters(), lr=0.0015, weight_decay = 1e-4)\n",
    "\n",
    "    max_epochs = 15\n",
    "    train_batch_size = int(6000)\n",
    "    test_batch_size = int(100)\n",
    "    status_interval = 10\n",
    "\n",
    "    train_epoch,train_losses,train_acc,trainAvgLoss, modelParamWgt = trainFunc(j,max_epochs,train_batch_size,status_interval)\n",
    "\n",
    "    testAcc, testLoss = testFunction(j,loss_func,test_batch_size)\n",
    "\n",
    "    #modelParamArr.append(modelParamWgt.values())\n",
    "\n",
    "    paramDF = pd.DataFrame.from_dict(data=modelParamWgt,orient='index')\n",
    "\n",
    "    \n",
    "    #PCA Dim Reduction.\n",
    "    pcaOperation =  PCA(n_components=2)\n",
    "\n",
    "    scale = StandardScaler()\n",
    "\n",
    "    sData = scale.fit_transform(paramDF)\n",
    "\n",
    "    pcaVal = pcaOperation.fit_transform(sData)\n",
    "\n",
    "    itrData = np.full((pcaVal.shape[0],1),itr)\n",
    "\n",
    "    #pcaDf = pd.DataFrame(data = pcaVal, columns = ['x','y'])\n",
    "\n",
    "    tempDf = pd.DataFrame(np.append(pcaVal,itrData,axis=1),columns=['x','y','Itr No.'])\n",
    "\n",
    "    pcaDf = pcaDf.append(tempDf, ignore_index=True)\n",
    "\n",
    "    #pcaDf = pd.DataFrame(np.append(pcaVal,itrData,axis=1),columns=['x','y','Itr No.'])\n",
    "\n",
    "    #resultDf = pcaOps(paramDF,itr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the Optimization Process\n",
    "\n",
    "plt.scatter(pcaDf['x'],pcaDf['y'],color=\"Blue\")\n",
    "plt.title(\"PCA for model\",color=\"g\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Itr No.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.843964</td>\n",
       "      <td>-16.185280</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.213867</td>\n",
       "      <td>27.728584</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.169346</td>\n",
       "      <td>8.804453</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-21.520700</td>\n",
       "      <td>-5.142484</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-32.367771</td>\n",
       "      <td>-15.205253</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.557999</td>\n",
       "      <td>-15.506945</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.430465</td>\n",
       "      <td>23.844307</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-7.058244</td>\n",
       "      <td>12.046673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-21.575172</td>\n",
       "      <td>-3.742217</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-33.355087</td>\n",
       "      <td>-16.641815</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50.945850</td>\n",
       "      <td>-16.157593</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.756237</td>\n",
       "      <td>24.470364</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-6.714079</td>\n",
       "      <td>13.673354</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-21.568399</td>\n",
       "      <td>-3.368571</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-32.419582</td>\n",
       "      <td>-18.617554</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50.497253</td>\n",
       "      <td>-17.235991</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11.191826</td>\n",
       "      <td>24.510576</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-5.916291</td>\n",
       "      <td>13.454924</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-21.500532</td>\n",
       "      <td>-2.724116</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-34.272270</td>\n",
       "      <td>-18.005394</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>49.919334</td>\n",
       "      <td>-16.447626</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.759738</td>\n",
       "      <td>22.033924</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-6.578148</td>\n",
       "      <td>12.376283</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-22.520906</td>\n",
       "      <td>-1.812808</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-34.580036</td>\n",
       "      <td>-16.149797</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>48.657978</td>\n",
       "      <td>-18.493689</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.383255</td>\n",
       "      <td>23.971067</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-6.112807</td>\n",
       "      <td>12.695344</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-22.681273</td>\n",
       "      <td>-2.532330</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-35.247166</td>\n",
       "      <td>-15.640407</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>49.241402</td>\n",
       "      <td>-17.627251</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>12.606387</td>\n",
       "      <td>25.179405</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-6.313646</td>\n",
       "      <td>12.261089</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-22.044737</td>\n",
       "      <td>-3.106369</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-33.489410</td>\n",
       "      <td>-16.706833</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>48.474209</td>\n",
       "      <td>-18.707172</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>13.183190</td>\n",
       "      <td>24.861570</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-5.100209</td>\n",
       "      <td>12.923835</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-21.433380</td>\n",
       "      <td>-1.835221</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-35.123798</td>\n",
       "      <td>-17.243015</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            x          y  Itr No.\n",
       "0   50.843964 -16.185280      0.0\n",
       "1   10.213867  27.728584      0.0\n",
       "2   -7.169346   8.804453      0.0\n",
       "3  -21.520700  -5.142484      0.0\n",
       "4  -32.367771 -15.205253      0.0\n",
       "5   51.557999 -15.506945      1.0\n",
       "6   10.430465  23.844307      1.0\n",
       "7   -7.058244  12.046673      1.0\n",
       "8  -21.575172  -3.742217      1.0\n",
       "9  -33.355087 -16.641815      1.0\n",
       "10  50.945850 -16.157593      2.0\n",
       "11   9.756237  24.470364      2.0\n",
       "12  -6.714079  13.673354      2.0\n",
       "13 -21.568399  -3.368571      2.0\n",
       "14 -32.419582 -18.617554      2.0\n",
       "15  50.497253 -17.235991      3.0\n",
       "16  11.191826  24.510576      3.0\n",
       "17  -5.916291  13.454924      3.0\n",
       "18 -21.500532  -2.724116      3.0\n",
       "19 -34.272270 -18.005394      3.0\n",
       "20  49.919334 -16.447626      4.0\n",
       "21  13.759738  22.033924      4.0\n",
       "22  -6.578148  12.376283      4.0\n",
       "23 -22.520906  -1.812808      4.0\n",
       "24 -34.580036 -16.149797      4.0\n",
       "25  48.657978 -18.493689      5.0\n",
       "26  15.383255  23.971067      5.0\n",
       "27  -6.112807  12.695344      5.0\n",
       "28 -22.681273  -2.532330      5.0\n",
       "29 -35.247166 -15.640407      5.0\n",
       "30  49.241402 -17.627251      6.0\n",
       "31  12.606387  25.179405      6.0\n",
       "32  -6.313646  12.261089      6.0\n",
       "33 -22.044737  -3.106369      6.0\n",
       "34 -33.489410 -16.706833      6.0\n",
       "35  48.474209 -18.707172      7.0\n",
       "36  13.183190  24.861570      7.0\n",
       "37  -5.100209  12.923835      7.0\n",
       "38 -21.433380  -1.835221      7.0\n",
       "39 -35.123798 -17.243015      7.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcaDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1190</th>\n",
       "      <th>1191</th>\n",
       "      <th>1192</th>\n",
       "      <th>1193</th>\n",
       "      <th>1194</th>\n",
       "      <th>1195</th>\n",
       "      <th>1196</th>\n",
       "      <th>1197</th>\n",
       "      <th>1198</th>\n",
       "      <th>1199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.118109</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>-0.084581</td>\n",
       "      <td>0.074787</td>\n",
       "      <td>0.016718</td>\n",
       "      <td>-0.003488</td>\n",
       "      <td>-0.008653</td>\n",
       "      <td>0.030069</td>\n",
       "      <td>0.103357</td>\n",
       "      <td>-0.113688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065332</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>-0.063955</td>\n",
       "      <td>0.058539</td>\n",
       "      <td>-0.088561</td>\n",
       "      <td>0.128292</td>\n",
       "      <td>-0.036612</td>\n",
       "      <td>-0.025034</td>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.084203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.124281</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>-0.088082</td>\n",
       "      <td>0.066617</td>\n",
       "      <td>-0.000939</td>\n",
       "      <td>-0.003010</td>\n",
       "      <td>-0.015071</td>\n",
       "      <td>0.013216</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>-0.114422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079601</td>\n",
       "      <td>-0.003994</td>\n",
       "      <td>-0.064810</td>\n",
       "      <td>0.059908</td>\n",
       "      <td>-0.114571</td>\n",
       "      <td>0.141079</td>\n",
       "      <td>-0.016900</td>\n",
       "      <td>-0.043026</td>\n",
       "      <td>0.031612</td>\n",
       "      <td>0.093656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.121230</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>-0.086581</td>\n",
       "      <td>0.053912</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>-0.002943</td>\n",
       "      <td>-0.007953</td>\n",
       "      <td>0.027960</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>-0.115558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084418</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>-0.059205</td>\n",
       "      <td>0.063091</td>\n",
       "      <td>-0.115257</td>\n",
       "      <td>0.141756</td>\n",
       "      <td>-0.012161</td>\n",
       "      <td>-0.043945</td>\n",
       "      <td>0.030065</td>\n",
       "      <td>0.093179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.118788</td>\n",
       "      <td>0.024208</td>\n",
       "      <td>-0.084046</td>\n",
       "      <td>0.047746</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.002894</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>0.037025</td>\n",
       "      <td>0.105262</td>\n",
       "      <td>-0.116889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088099</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>-0.053806</td>\n",
       "      <td>0.066676</td>\n",
       "      <td>-0.116808</td>\n",
       "      <td>0.142506</td>\n",
       "      <td>-0.010726</td>\n",
       "      <td>-0.043780</td>\n",
       "      <td>0.028342</td>\n",
       "      <td>0.091495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.117277</td>\n",
       "      <td>0.024864</td>\n",
       "      <td>-0.080404</td>\n",
       "      <td>0.042379</td>\n",
       "      <td>-0.018265</td>\n",
       "      <td>-0.002840</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.041063</td>\n",
       "      <td>0.105620</td>\n",
       "      <td>-0.118768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092239</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>-0.049898</td>\n",
       "      <td>0.071013</td>\n",
       "      <td>-0.118144</td>\n",
       "      <td>0.144963</td>\n",
       "      <td>-0.011053</td>\n",
       "      <td>-0.042347</td>\n",
       "      <td>0.026491</td>\n",
       "      <td>0.090288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6     \\\n",
       "3  -0.118109  0.005786 -0.084581  0.074787  0.016718 -0.003488 -0.008653   \n",
       "6  -0.124281  0.013263 -0.088082  0.066617 -0.000939 -0.003010 -0.015071   \n",
       "9  -0.121230  0.021098 -0.086581  0.053912 -0.001117 -0.002943 -0.007953   \n",
       "12 -0.118788  0.024208 -0.084046  0.047746 -0.000536 -0.002894 -0.002317   \n",
       "15 -0.117277  0.024864 -0.080404  0.042379 -0.018265 -0.002840  0.001747   \n",
       "\n",
       "        7         8         9     ...      1190      1191      1192      1193  \\\n",
       "3   0.030069  0.103357 -0.113688  ...  0.065332  0.003742 -0.063955  0.058539   \n",
       "6   0.013216  0.106066 -0.114422  ...  0.079601 -0.003994 -0.064810  0.059908   \n",
       "9   0.027960  0.104072 -0.115558  ...  0.084418  0.005479 -0.059205  0.063091   \n",
       "12  0.037025  0.105262 -0.116889  ...  0.088099  0.011706 -0.053806  0.066676   \n",
       "15  0.041063  0.105620 -0.118768  ...  0.092239  0.017100 -0.049898  0.071013   \n",
       "\n",
       "        1194      1195      1196      1197      1198      1199  \n",
       "3  -0.088561  0.128292 -0.036612 -0.025034  0.033301  0.084203  \n",
       "6  -0.114571  0.141079 -0.016900 -0.043026  0.031612  0.093656  \n",
       "9  -0.115257  0.141756 -0.012161 -0.043945  0.030065  0.093179  \n",
       "12 -0.116808  0.142506 -0.010726 -0.043780  0.028342  0.091495  \n",
       "15 -0.118144  0.144963 -0.011053 -0.042347  0.026491  0.090288  \n",
       "\n",
       "[5 rows x 1200 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1d86b2f3ed665d691ce24c615a98bbc398f66743afc4d4e970e6f8b36fab2b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CPSC-8430-DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
