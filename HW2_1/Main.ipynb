{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from scipy.special import expit\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########DATA PREPROCESSING##############\n",
    "\n",
    "def dictonaryFunc(word_min): \n",
    "#Json Loader#\n",
    "    with open('training_label.json', 'r') as f:\n",
    "        file = json.load(f)\n",
    "\n",
    "    word_count = {}\n",
    "    for d in file:\n",
    "        for s in d['caption']:\n",
    "            word_sentence = re.sub('[.!,;?]]', ' ', s).split()\n",
    "            for word in word_sentence:\n",
    "                word = word.replace('.', '') if '.' in word else word\n",
    "                if word in word_count:\n",
    "                    word_count[word] += 1\n",
    "                else:\n",
    "                    word_count[word] = 1\n",
    "#Dictonary#    \n",
    "    dictonary = {}\n",
    "#     word_count = caption_loader()\n",
    "    for word in word_count:\n",
    "        if word_count[word] > word_min:\n",
    "            dictonary[word] = word_count[word]\n",
    "    useful_tokens = [('<PAD>', 0), ('<SOS>', 1), ('<EOS>', 2), ('<UNK>', 3)]\n",
    "    i2w = {i + len(useful_tokens): w for i, w in enumerate(dictonary)}\n",
    "    w2i = {w: i + len(useful_tokens) for i, w in enumerate(dictonary)}\n",
    "    for token, index in useful_tokens:\n",
    "        i2w[index] = token\n",
    "        w2i[token] = index\n",
    "    return i2w,w2i,dictonary\n",
    "\n",
    "\n",
    "def s_split(sentence, dictonary, w2i):  #sentenceSplit\n",
    "    sentence = re.sub(r'[.!,;?]', ' ', sentence).split()\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] not in dictonary:\n",
    "            sentence[i] = 3\n",
    "        else:\n",
    "            sentence[i] = w2i[sentence[i]]\n",
    "    sentence.insert(0, 1)\n",
    "    sentence.append(2)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def annotate(label_file, dictonary, w2i):\n",
    "    label_json = label_file\n",
    "    annotated_caption = []\n",
    "    with open(label_json, 'r') as f:\n",
    "        label = json.load(f)\n",
    "    for d in label:\n",
    "        for s in d['caption']:\n",
    "            s = s_split(s, dictonary, w2i)\n",
    "            annotated_caption.append((d['id'], s))\n",
    "    return annotated_caption\n",
    "\n",
    "def word2index(w2i, w):\n",
    "        return w2i[w]\n",
    "def index2word(i2w, i):\n",
    "    return i2w[i]\n",
    "def sentence2index(w2i, sentence):\n",
    "    return [w2i[w] for w in sentence]\n",
    "def index2sentence(i2w, index_seq):\n",
    "    return [i2w[int(i)] for i in index_seq]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def avi(files_dir):\n",
    "    avi_data = {}\n",
    "    training_feats = files_dir\n",
    "    files = os.listdir(training_feats)\n",
    "    for file in files:\n",
    "        value = np.load(os.path.join(training_feats, file))\n",
    "        avi_data[file.split('.npy')[0]] = value\n",
    "    return avi_data\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def minibatch(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    avi_data, captions = zip(*data) \n",
    "    avi_data = torch.stack(avi_data, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return avi_data, targets, lengths\n",
    "\n",
    "class Dataprocessor(Dataset):\n",
    "    def __init__(self, label_file, files_dir,dictonary, w2i):\n",
    "    #def __init__(self):\n",
    "        self.label_file = label_file\n",
    "        self.files_dir = files_dir\n",
    "        self.avi = avi(label_file)\n",
    "        self.w2i = w2i\n",
    "        self.dictonary = dictonary\n",
    "        self.data_pair = annotate(files_dir, dictonary, w2i)\n",
    "    def __len__(self):\n",
    "        return len(self.data_pair)\n",
    "    def __getitem__(self, idx):\n",
    "        assert (idx < self.__len__())\n",
    "        avi_file_name, sentence = self.data_pair[idx]\n",
    "        data = torch.Tensor(self.avi[avi_file_name])\n",
    "        data += torch.Tensor(data.size()).random_(0, 2000)/10000\n",
    "        return torch.Tensor(data), torch.Tensor(sentence)\n",
    "\n",
    "class test_dataloader(Dataset):\n",
    "    def __init__(self, test_data_path):\n",
    "        self.avi = []\n",
    "        files = os.listdir(test_data_path)\n",
    "        for file in files:\n",
    "            key = file.split('.npy')[0]\n",
    "            value = np.load(os.path.join(test_data_path, file))\n",
    "            self.avi.append([key, value])\n",
    "    def __len__(self):\n",
    "        return len(self.avi)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.avi[idx]\n",
    "    \n",
    "# In[13]:\n",
    "\n",
    "\n",
    "#####TRAIN & TEST FUNCTIONS###########\n",
    "\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "def calculate_loss(x, y, lengths,loss_fn):\n",
    "    batch_size = len(x)\n",
    "    predict_cat = None\n",
    "    groundT_cat = None\n",
    "    flag = True\n",
    "\n",
    "    for batch in range(batch_size):\n",
    "        predict = x[batch]\n",
    "        ground_truth = y[batch]\n",
    "        seq_len = lengths[batch] -1\n",
    "\n",
    "        predict = predict[:seq_len]\n",
    "        ground_truth = ground_truth[:seq_len]\n",
    "        if flag:\n",
    "            predict_cat = predict\n",
    "            groundT_cat = ground_truth\n",
    "            flag = False\n",
    "        else:\n",
    "            predict_cat = torch.cat((predict_cat, predict), dim=0)\n",
    "            groundT_cat = torch.cat((groundT_cat, ground_truth), dim=0)\n",
    "\n",
    "    loss = loss_fn(predict_cat, groundT_cat)\n",
    "    avg_loss = loss/batch_size\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "def minibatch(data):\n",
    "\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    avi_data, captions = zip(*data) \n",
    "    avi_data = torch.stack(avi_data, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return avi_data, targets, lengths\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "# def train(model, epoch, train_loader = train_dataloader):\n",
    "def train(model, epoch, train_loader, loss_func):\n",
    "    model.train()\n",
    "    print(epoch)\n",
    "    model = model.cuda()\n",
    "    parameters = model.parameters()\n",
    "    optimizer = optim.Adam(parameters, lr=0.001)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        avi_feats, ground_truths, lengths = batch\n",
    "        avi_feats, ground_truths = avi_feats.cuda(), ground_truths.cuda()\n",
    "        avi_feats, ground_truths = Variable(avi_feats), Variable(ground_truths)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        seq_logProb, seq_predictions = model(avi_feats, target_sentences=ground_truths, mode='train', tr_steps=epoch)\n",
    "            \n",
    "        ground_truths = ground_truths[:, 1:]  \n",
    "        loss = calculate_loss(seq_logProb, ground_truths, lengths,loss_func)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss = loss.item()\n",
    "    print('loss:', np.round(loss,3))\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "# def evaluate(test_loader = test_dataloader):\n",
    "def evaluate(test_loader,model):\n",
    "    # set model to evaluation(testing) mode\n",
    "    model.eval()\n",
    "    test_predictions, test_truth = None, None\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        avi_feats, ground_truths, lengths = batch\n",
    "        avi_feats, ground_truths = avi_feats.cuda(), ground_truths.cuda()\n",
    "        avi_feats, ground_truths = Variable(avi_feats), Variable(ground_truths)\n",
    "\n",
    "        seq_logProb, seq_predictions = model(avi_feats, mode='inference')\n",
    "        ground_truths = ground_truths[:, 1:]\n",
    "        test_predictions = seq_predictions[:3]\n",
    "        test_truth = ground_truths[:3]\n",
    "        break\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "def test(test_loader,model,i2w):\n",
    "        \n",
    "        # set model to evaluation(testing) mode\n",
    "        model.eval()\n",
    "        ss = []\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            # prepare data\n",
    "            id, avi_feats = batch\n",
    "            if torch.cuda.is_available():\n",
    "                avi_feats = avi_feats.cuda()\n",
    "            else:\n",
    "                avi_feats=avi_feats\n",
    "                \n",
    "            id, avi_feats = id, Variable(avi_feats).float()\n",
    "\n",
    "            # start inferencing process\n",
    "            seq_logProb, seq_predictions = model(avi_feats, mode='inference')\n",
    "            test_predictions = seq_predictions\n",
    "#             result = [[x if x != '<UNK>' else 'something' for x in s] for s in test_predictions]\n",
    "#             result = [' '.join(s).split('<EOS>')[0] for s in result]\n",
    "\n",
    "            result = [[i2w[x.item()] if i2w[x.item()] != '<UNK>' else 'something' for x in s] for s in test_predictions]\n",
    "            result = [' '.join(s).split('<EOS>')[0] for s in result]\n",
    "        \n",
    "            rr = zip(id, result)\n",
    "            for r in rr:\n",
    "                ss.append(r)\n",
    "        return ss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "loss: 4.126\n",
      "2\n",
      "loss: 3.461\n",
      "3\n",
      "loss: 3.764\n",
      "4\n",
      "loss: 3.433\n",
      "5\n",
      "loss: 3.277\n",
      "6\n",
      "loss: 3.1\n",
      "7\n",
      "loss: 3.803\n",
      "8\n",
      "loss: 3.048\n",
      "9\n",
      "loss: 3.279\n",
      "10\n",
      "loss: 3.031\n",
      "11\n",
      "loss: 3.165\n",
      "12\n",
      "loss: 3.313\n",
      "13\n",
      "loss: 3.145\n",
      "14\n",
      "loss: 3.482\n",
      "15\n",
      "loss: 3.665\n",
      "16\n",
      "loss: 2.978\n",
      "17\n",
      "loss: 3.164\n",
      "18\n",
      "loss: 3.595\n",
      "loss: 3.352\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    label_file = 'training_data/feat'\n",
    "    files_dir = 'training_label.json'\n",
    "    i2w,w2i,dictonary = dictonaryFunc(4)\n",
    "    train_dataset = Dataprocessor(label_file, files_dir,dictonary, w2i)\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size=128, shuffle=True, num_workers=8, collate_fn=minibatch)\n",
    "    \n",
    "    label_file = 'testing_data/feat'\n",
    "    files_dir = 'testing_label.json'\n",
    "    test_dataset = Dataprocessor(label_file,files_dir,dictonary, w2i)\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size=128, shuffle=True, num_workers=8, collate_fn=minibatch)\n",
    "   \n",
    "    epochs_n = 50\n",
    "    ModelSaveLoc = 'SavedModel'\n",
    "    with open('i2wData.pickle', 'wb') as f:\n",
    "         pickle.dump(i2w, f)\n",
    "\n",
    "    \n",
    "    x = len(i2w)+4\n",
    "    if not os.path.exists(ModelSaveLoc):\n",
    "        os.mkdir(ModelSaveLoc)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    encode = models.EncoderNet()\n",
    "    decode = models.DecoderNet(512, x, x, 1024, 0.3)\n",
    "    model = models.ModelMain(encoder = encode,decoder = decode) \n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs_n):\n",
    "        train(model,epoch+1, train_loader=train_dataloader, loss_func=loss_fn)\n",
    "        evaluate(test_dataloader, model)\n",
    "\n",
    "    end = time.time()\n",
    "    torch.save(model, \"{}/{}.h5\".format(ModelSaveLoc, 'model0'))\n",
    "    print(\"Training finished {}  elapsed time: {: .3f} seconds. \\n\".format('test', end-start))\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DBT_pytorch)",
   "language": "python",
   "name": "dbt_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
